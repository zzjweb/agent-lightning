{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Agent Lightning","text":"<p>Agent Lightning is the absolute trainer to light up AI agents.</p> <p>Join our Discord community to connect with other users and contributors.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Turn your agent into an optimizable beast with ZERO CODE CHANGE (almost)! \ud83d\udca4</li> <li>Build with ANY agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework...); or even WITHOUT agent framework (Python OpenAI). You name it! \ud83e\udd16</li> <li>Selectively optimize one or more agents in a multi-agent system. \ud83c\udfaf</li> <li>Embraces Algorithms like Reinforcement Learning, Automatic Prompt Optimization, Supervised Fine-tuning and more. \ud83e\udd17</li> </ul>"},{"location":"#how-to-read-this-documentation","title":"How to Read this Documentation","text":"<p>This documentation is organized into the following parts:</p> <ul> <li>Installation - Get started with Agent Lightning</li> <li>How-to Recipes (e.g., Train SQL Agent with RL) - Practical examples of training agents and customizing algorithms.</li> <li>Learning More (e.g., Debugging) - Guides on specific topics like debugging or parallelization.</li> <li>Algorithm Zoo (e.g., APO) - References for built-in algorithms.</li> <li>Deep Dive (e.g., Bird's Eye View) - For a deeper understanding of what Agent-lightning is doing under the hood.</li> <li>API References (e.g., Agent) - References for the Agent-lightning Python API.</li> </ul>"},{"location":"#resources","title":"Resources","text":"<ul> <li>11/4/2025 Tuning ANY AI agent with Tinker \u2715 Agent-lightning Medium. See also Part 2.</li> <li>10/22/2025 No More Retokenization Drift: Returning Token IDs via the OpenAI Compatible API Matters in Agent RL vLLM blog. See also Zhihu writeup.</li> <li>8/11/2025 Training AI Agents to Write and Self-correct SQL with Reinforcement Learning Medium.</li> <li>8/5/2025 Agent Lightning: Train ANY AI Agents with Reinforcement Learning arXiv paper.</li> <li>7/26/2025 We discovered an approach to train any AI agent with RL, with (almost) zero code changes. Reddit.</li> <li>6/6/2025 Agent Lightning - Microsoft Research Project page.</li> </ul>"},{"location":"#community-projects","title":"Community Projects","text":"<ul> <li>DeepWerewolf \u2014 A case study of agent RL training for the Chinese Werewolf game built with AgentScope and Agent Lightning.</li> <li>AgentFlow \u2014 A modular multi-agent framework that combines planner, executor, verifier, and generator agents with the Flow-GRPO algorithm to tackle long-horizon, sparse-reward tasks.</li> <li>Youtu-Agent \u2014 Youtu-Agent lets you build and train your agent with ease. Built with a modified branch of Agent Lightning, Youtu-Agent has verified up to 128 GPUs RL training on maths/code and search capabilities with steady convergence. Also check the recipe and their blog Stop Wrestling with Your Agent RL: How Youtu-Agent Achieved Stable, 128-GPU Scaling Without Breaking a Sweat.</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you find Agent Lightning useful in your research or projects, please cite our paper:</p> <pre><code>@misc{luo2025agentlightningtrainai,\n      title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning},\n      author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang},\n      year={2025},\n      eprint={2508.03680},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2508.03680},\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>See the LICENSE file for details.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#agent-lightning-v030-12242025","title":"Agent-lightning v0.3.0 (12/24/2025)","text":"<p>Agent-lightning v0.3.0 is a major release that introduces several new features and bug fixes. The release is a collaborative effort between Agent-lightning core teams and the community. Thanks to all the contributors who made this release possible.</p>"},{"location":"changelog/#highlights","title":"Highlights","text":"<ul> <li>Tinker integration: Support Tinker as an alternative backend for Reinforcement Learning (#226 #245 #264 #269 #327). See example code, blog 1 and blog 2.</li> <li>Azure OpenAI integration: Support Azure OpenAI as a backend for LLM inference and supervised fine-tuning (#256 #327). Example code.</li> <li>MongoDB-based Lightning Store is added as an alternative backend for Lightning Store (#323). Documentation.</li> <li>Contrib package: Add contrib package for community projects. Search-R1 is integrated as a contrib recipe. More coming. (#239 #396 #410 #412 #417).</li> <li>RESTful API: Stabilize and document RESTful API for Lightning Store (#241 #275). Documentation.</li> <li>OTel Semantic Conventions that are specifically designed for Agent-optimization areas (#340). Documentation.</li> <li>[Preview] Agent-lightning Dashboard is now available (#288 #289 #291 #296 #371 #375). It's the official web application for inspecting and debugging Agent-lightning experiments. See details here.</li> <li>[Preview] Multi-modality example featuring VERL and a LangGraph agent on ChartQA dataset (#379). Example code.</li> <li>[Preview] Integrate Claude Code as a LitAgent and support training on SWE-Bench (#332 #346 #348). Example code.</li> <li>[Preview] Weave tracer as a substitute for AgentOps tracer (#277 #411 #420 #423). Documentation.</li> <li>[Preview] Trajectory Level Aggregation for more efficient training with VERL. See blog and documentation.</li> </ul>"},{"location":"changelog/#store-benchmark","title":"Store Benchmark","text":"<p>In this release, the Lightning Store core was redesigned for significantly greater efficiency and scalability (#315 #318 #328 #342 #344 #356 #380 #388 #418 #421). The benchmark results below demonstrate the impact: with large numbers of concurrent runners, v0.3.0 delivers up to a 15x increase in throughput compared to v0.2.2.</p> Throughput (#rollout/sec) v0.2.2 v0.3.0 (in-memory) v0.3.0 (Mongo) Minimal (batch, #runner=32, #turns=6) 8.73 9.06 8.71 Medium (batch, #runners=100, #turns=10) 12.03 23.26 32.79 Mid-high (batch, #runners=300, #turns=6) 10.61 24.42 40.24 Large (batch, #runners=1000, #turns=3) 3.36 14.60 50.05 Long queue (queue, #runners=256, #turns=4) 7.42 30.86 57.01 Heavy trace (queue, #runners=512, #turns=20) 5.93 13.28 29.41 <p>Notes:</p> <ol> <li>Benchmarks were run on a single Standard_D32as_v4 Azure VM (Large and heavy trace tests used Standard_D64ads_v5), executed via GitHub Actions.</li> <li>Two algorithm patterns are evaluated: the batch pattern submits a group of rollouts and waits for all to finish before starting the next group, while the queue pattern maintains a set number of in-flight rollouts, submitting new ones as soon as capacity frees up. Configuration details are available here.</li> <li>The number of turns is directly proportional to the number of spans each rollout generates.</li> </ol>"},{"location":"changelog/#maintenance-and-bug-fixes","title":"Maintenance and Bug fixes","text":""},{"location":"changelog/#core-store-interfaces-etc","title":"Core (Store, Interfaces, etc.)","text":"<ul> <li>Add Trainer port option for client-server strategies (#198)</li> <li>Fix store port conflict handling (#227)</li> <li>Unified PythonServerLauncher (#286 #292 #303)</li> <li>Make health timeout configurable (#305)</li> <li>Refactor logging (#306)</li> <li>Support OTLP in LightningStore (#313)</li> <li>Centralized metrics helper (#368)</li> <li>Fix redundant cancel tracebacks on Ctrl+C (#370)</li> </ul>"},{"location":"changelog/#proxy-adapters-and-algorithms","title":"Proxy, Adapters and Algorithms","text":"<ul> <li>Fix training metrics before and after processing in VERL (#145)</li> <li>Forward streaming requests for Anthropic and OpenAI APIs (as non-streaming requests) (#299)</li> <li>Check traces with reward for VERL (#317)</li> <li>Patch LiteLLM root span (#341)</li> <li>Handle ref_in_actor flag for LoRA compatibility (#386)</li> <li>Support <code>with_llm_proxy</code> and <code>with_store</code> in algorithms (#398)</li> <li>Support image URL export in TracerTraceToTriplets (#400)</li> <li>Fix match_rewards assign_to elements in TraceTree (#403)</li> <li>Support customizing trainer and daemon in VERL (#407)</li> </ul>"},{"location":"changelog/#runners-tracers-and-agents","title":"Runners, Tracers and Agents","text":"<ul> <li>Refactor tracer initialization (#321)</li> <li>Fix OpenAI Agents 0.6 compatibility (#322)</li> <li><code>emit_operation</code>, <code>emit_annotation</code>, tags and links (#359)</li> <li>Sunset HTTP tracer (#402)</li> </ul>"},{"location":"changelog/#examples","title":"Examples","text":"<ul> <li>Fix typos in train-first-agent.md (#263)</li> <li>Fix room_selector example which always runs the first task (#270)</li> <li>Fix typo in SQL agent example (#285)</li> <li>Add the README and script files for training SQL agent on NPU (#272)</li> <li>Examples Catalog and Refine Contribution Guide (#331)</li> <li>Upgrade LangChain to 1.x (#364)</li> <li>Update RAG example to Agent-lightning v0.2.x (#349)</li> </ul>"},{"location":"changelog/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>DeepWiki Badge (#263)</li> <li>Add AGENTS.md (#374)</li> </ul>"},{"location":"changelog/#new-contributors","title":"New Contributors","text":"<p>Warm welcome to our first-time contributors: @cptnm3, @TerryChan, @genji970, @zxgx, @xiaochulaoban, @lspinheiro, @Kwanghoon-Choi, @Vasuk12, @totoluo, @jinghuan-Chen \ud83c\udf89</p> <p>Full Changelog: https://github.com/microsoft/agent-lightning/compare/v0.2.0...v0.3.0</p>"},{"location":"changelog/#agent-lightning-v022-11122025","title":"Agent-lightning v0.2.2 (11/12/2025)","text":"<p>Agent-lightning v0.2.2 is a stabilization release for v0.2.1. It introduces several bug fixes.</p> <ul> <li>Fix compatibility issues with VERL 0.6.0.</li> <li>Fix model name for pre-downloaded models in VERL.</li> <li>Fix preparing status transition on rollout when creating attempts.</li> <li>Fix OpenAI Agents SDK compatibility issues.</li> </ul> <p>Full Changelog: https://github.com/microsoft/agent-lightning/compare/v0.2.1...v0.2.2</p>"},{"location":"changelog/#agent-lightning-v021-10302025","title":"Agent-lightning v0.2.1 (10/30/2025)","text":"<p>Agent-lightning v0.2.1 is a stabilization release for v0.2.0. It introduces several bug fixes and new features, plus a number of unlisted CI improvements.</p>"},{"location":"changelog/#bug-fixes","title":"Bug fixes","text":"<ul> <li>Fix LiteLLM issues when restarting the proxy multiple times in the same process (#174 #206)</li> <li>Fix LiteLLM model name selection when multiple servers use the same model (#197)</li> <li>Fix store port conflict handling (#227)</li> </ul>"},{"location":"changelog/#new-features","title":"New Features","text":"<ul> <li>Add trainer port option for client-server strategies (#198)</li> </ul>"},{"location":"changelog/#documentation","title":"Documentation","text":"<ul> <li>Add tutorial for launching workers on separate machines (#213)</li> <li>Add link to VERL framework (#210)</li> <li>Add link to vLLM blog (#215)</li> <li>Fix a couple of typos and avoid emacs backup files (#237)</li> </ul>"},{"location":"changelog/#new-contributors_1","title":"New Contributors","text":"<p>A warm welcome to our first-time contributors: @scott-vsi, @ddsfda99, @jeis4wpi \ud83c\udf89</p> <p>Full Changelog: https://github.com/microsoft/agent-lightning/compare/v0.2.0...v0.2.1</p>"},{"location":"changelog/#agent-lightning-v020-10222025","title":"Agent-lightning v0.2.0 (10/22/2025)","text":"<p>Agent-Lightning v0.2.0 introduces major framework improvements, new execution strategies, expanded documentation, and enhanced reliability across the agent training and deployment workflow. This release includes 78 pull requests since v0.1.2.</p>"},{"location":"changelog/#core-enhancements","title":"Core Enhancements","text":"<ul> <li>Lightning Store: Added unified interface and implementation for Agent-lightning's core storage.</li> <li>Emitter: Emitting any objects as spans to the store.</li> <li>Adapter and Tracer: Adapting to OpenAI-like messages, and OpenTelemetry dummy tracer.</li> <li>LLM Proxy: Added LLM Proxy as the first-class citizen in Agent-lightning.</li> <li>Agent Runner: New version providing a more modular and robust runner design.</li> <li>Embedded Algorithms: Algorithms are now embedded directly into trainers for simplicity.</li> <li>New Execution Strategies: Introduced Client-Server and Shared Memory execution models.</li> <li>Trainer Updates: Integrated v0.2 interfaces and FastAlgorithm validation.</li> </ul>"},{"location":"changelog/#documentation-examples","title":"Documentation &amp; Examples","text":"<ul> <li>Revamped documentation with new guides for agent creation, training, debugging, and store concepts.</li> <li>Improved quickstart tutorials, clarified installation and new deep-dive articles.</li> <li>Added and updated examples: SQL Agent, Calc-X, Local SFT, Search-R1, and APO algorithm.</li> </ul>"},{"location":"changelog/#developer-experience","title":"Developer Experience","text":"<ul> <li>Migrated build and CI pipelines to 1ES, split workflows and aggregate badges for clarity.</li> <li>Adopted uv as the dependency manager.</li> <li>Added GPU-based pytest workflows for full test coverage.</li> <li>Enhanced debugging UX, pre-commit configs, and linting (Pyright fixes, import sorting).</li> </ul>"},{"location":"changelog/#ecosystem-integrations","title":"Ecosystem &amp; Integrations","text":"<ul> <li>Added support for agents built with Agent-framework.</li> <li>Added new community listings: DeepWerewolf and AgentFlow.</li> </ul>"},{"location":"changelog/#new-contributors_2","title":"New Contributors","text":"<p>A warm welcome to our first-time contributors: @hzy46, @lunaqiu, @syeehyn, @linhx1999, @SiyunZhao, and @acured \ud83c\udf89</p> <p>Full changelog: v0.1.2 \u2192 v0.2.0</p>"},{"location":"changelog/#agent-lightning-v012-08122025","title":"Agent-lightning v0.1.2 (08/12/2025)","text":""},{"location":"changelog/#whats-changed","title":"What's Changed","text":"<ul> <li>Add basic documentation in https://github.com/microsoft/agent-lightning/pull/33</li> <li>RAG example by @wizardlancet in https://github.com/microsoft/agent-lightning/pull/21</li> </ul>"},{"location":"changelog/#new-contributors_3","title":"New Contributors","text":"<ul> <li>@wizardlancet made their first contribution in https://github.com/microsoft/agent-lightning/pull/21</li> </ul> <p>Full Changelog: https://github.com/microsoft/agent-lightning/compare/v0.1.1...v0.1.2</p>"},{"location":"changelog/#agent-lightning-v011-08062025","title":"Agent-lightning v0.1.1 (08/06/2025)","text":""},{"location":"changelog/#whats-changed_1","title":"What's Changed","text":"<ul> <li>Disable HTTP tracer tests and bump to 0.1.1 in https://github.com/microsoft/agent-lightning/pull/26</li> <li>Fix trainer bugs in v0.1 in https://github.com/microsoft/agent-lightning/pull/24</li> </ul> <p>Full Changelog: https://github.com/microsoft/agent-lightning/compare/v0.1...v0.1.1</p>"},{"location":"changelog/#agent-lightning-v010-08042025","title":"Agent-lightning v0.1.0 (08/04/2025)","text":"<p>The first release of Agent-lightning!</p> <ul> <li>Turn your agent into an optimizable beast with ZERO CODE CHANGE (almost)! \ud83d\udca4</li> <li>Build with ANY agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, ...); or even WITHOUT agent framework (Python OpenAI). You name it! \ud83e\udd16</li> <li>Selectively optimize one or more agents in a multi-agent system. \ud83c\udfaf</li> <li>Embraces Reinforcement Learning, Automatic Prompt Optimization and more algorithms. \ud83e\udd17</li> </ul> <p>Install via <code>pip install agentlightning</code>.</p>"},{"location":"algorithm-zoo/","title":"Algorithm Zoo","text":"<p>AgentLightning includes several popular and frequently requested algorithms in its built-in library, allowing agent developers to use them directly. These algorithms are designed to be compatible with most agent scenarios.</p> <p>For customizing algorithms, see Algorithm-side References.</p> Algorithm Optimizing Resources Description APO <code>{&lt;initial_prompt_key&gt;: [PromptTemplate][agentlightning.PromptTemplate]}</code> Automatic Prompt Optimization (APO) algorithm using textual gradients and beam search. VERL <code>{\"main_llm\": [LLM][agentlightning.LLM]}</code> Reinforcement Learning with VERL framework."},{"location":"algorithm-zoo/apo/","title":"APO","text":"<p>Shortcut</p> <p>You can use the shortcut <code>agl.APO(...)</code> to create an APO instance.</p> <pre><code>import agentlightning as agl\n\nagl.APO(...)\n</code></pre>"},{"location":"algorithm-zoo/apo/#installation","title":"Installation","text":"<pre><code>pip install agentlightning[apo]\n</code></pre>"},{"location":"algorithm-zoo/apo/#scope-of-current-implementation","title":"Scope of Current Implementation","text":"<p>APO is currently scoped to optimize a single prompt template. Optimizing multiple prompt templates is not supported yet.</p> <p>There is however no restriction on the number of variable placeholders in the prompt template (can range from zero to many). It's possible that invalid prompts are created during the optimization process. It is up to the agent developer to ensure that the prompt template is valid for the agent's task.</p>"},{"location":"algorithm-zoo/apo/#initial-prompt","title":"Initial Prompt","text":"<p>APO expects the initial prompt to be provided in the <code>initial_resources</code> dictionary. This can be done in two approaches:</p> <ol> <li>Pass to the Trainer constructor:</li> </ol> <pre><code>trainer = agl.Trainer(\n    algorithm=agl.APO(...),\n    initial_resources={\"main_prompt\": agl.PromptTemplate(template=\"You are a helpful assistant.\", engine=\"f-string\")},\n)\n</code></pre> <ol> <li>Pass to the <code>[APO][agentlightning.algorithm.apo.APO].set_initial_resources()</code> method:</li> </ol> <pre><code>algo = agl.APO(...)\nalgo.set_initial_resources(\n    {\"this_is_also_valid_key\": agl.PromptTemplate(template=\"You are a helpful assistant.\", engine=\"f-string\")}\n)\n</code></pre> <p>The resource key can be arbitrary, which is used to identify the prompt template in class-based implementations when you have multiple resources. When the key changes, the agent developer needs to update the key in the <code>rollout</code> method.</p>"},{"location":"algorithm-zoo/apo/#tutorials-using-apo","title":"Tutorials Using APO","text":"<ul> <li>Train the First Agent with APO - A step-by-step guide to training your first agent using APO.</li> </ul>"},{"location":"algorithm-zoo/apo/#references","title":"References","text":""},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo","title":"<code>agentlightning.algorithm.apo</code>","text":""},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo.APO","title":"<code>APO</code>","text":"<p>               Bases: <code>Algorithm</code>, <code>Generic[T_task]</code></p> <p>Automatic Prompt Optimization (APO) algorithm using textual gradients and beam search.</p> <p>APO is an iterative prompt optimization algorithm that uses LLM-generated textual gradients to improve prompts through a beam search process. It evaluates prompts on rollouts, computes critiques based on the results, and applies edits to generate improved prompts.</p> <p>The algorithm operates in rounds, where each round:</p> <ol> <li>Samples parent prompts from the current beam</li> <li>Generates new prompts by computing textual gradients and applying edits</li> <li>Evaluates all candidates on a validation set</li> <li>Selects the top-k prompts for the next round</li> </ol> <p>Based on the ideas from:</p> <ul> <li>ProTeGi</li> <li>TextGrad</li> </ul>"},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo.APO.__init__","title":"<code>__init__(async_openai_client, *, gradient_model='gpt-5-mini', apply_edit_model='gpt-4.1-mini', diversity_temperature=1.0, gradient_batch_size=4, val_batch_size=16, beam_width=4, branch_factor=4, beam_rounds=3, rollout_batch_timeout=3600.0, run_initial_validation=True, gradient_prompt_files=None, apply_edit_prompt_files=None, _poml_trace=False)</code>","text":"<p>Initialize the APO algorithm with configuration parameters.</p> <p>Parameters:</p> <ul> <li> <code>async_openai_client</code>               (<code>AsyncOpenAI</code>)           \u2013            <p>AsyncOpenAI client for making LLM API calls.</p> </li> <li> <code>gradient_model</code>               (<code>str</code>, default:                   <code>'gpt-5-mini'</code> )           \u2013            <p>Model name for computing textual gradients (critiques).</p> </li> <li> <code>apply_edit_model</code>               (<code>str</code>, default:                   <code>'gpt-4.1-mini'</code> )           \u2013            <p>Model name for applying edits based on critiques.</p> </li> <li> <code>diversity_temperature</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Temperature parameter for LLM calls to control diversity.</p> </li> <li> <code>gradient_batch_size</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of rollout results to sample for gradient computation.</p> </li> <li> <code>val_batch_size</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>Number of validation examples to use for evaluation.</p> </li> <li> <code>beam_width</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of top-scoring prompts to keep in the beam at each round.</p> </li> <li> <code>branch_factor</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of new prompt candidates to generate from each parent prompt by applying textual gradient edits. This controls the expansion of the search tree.</p> </li> <li> <code>beam_rounds</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Number of beam search rounds to perform.</p> </li> <li> <code>rollout_batch_timeout</code>               (<code>float</code>, default:                   <code>3600.0</code> )           \u2013            <p>Maximum time in seconds to wait for rollout batch completion.</p> </li> <li> <code>run_initial_validation</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, runs validation on the seed prompt before starting optimization to establish a baseline score. Defaults to True.</p> </li> <li> <code>gradient_prompt_files</code>               (<code>Optional[List[Path]]</code>, default:                   <code>None</code> )           \u2013            <p>Prompt templates used to compute textual gradients (critiques).</p> </li> <li> <code>apply_edit_prompt_files</code>               (<code>Optional[List[Path]]</code>, default:                   <code>None</code> )           \u2013            <p>Prompt templates used to apply edits based on critiques.</p> </li> </ul>"},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo.APO.compute_textual_gradient","title":"<code>compute_textual_gradient(current_prompt, rollout_results, *, prefix=None)</code>  <code>async</code>","text":"<p>Compute a textual gradient (critique) for the current prompt based on rollout results.</p> <p>This method samples rollout results, sends them to an LLM along with the current prompt, and generates a critique describing how the prompt could be improved.</p> <p>Parameters:</p> <ul> <li> <code>current_prompt</code>               (<code>VersionedPromptTemplate</code>)           \u2013            <p>The prompt template to critique.</p> </li> <li> <code>rollout_results</code>               (<code>List[RolloutResultForAPO]</code>)           \u2013            <p>List of rollout results containing spans, messages, and rewards.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[str]</code>           \u2013            <p>A textual critique generated by the LLM, or None if generation fails.</p> </li> </ul>"},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo.APO.evaluate_prompt_on_batch","title":"<code>evaluate_prompt_on_batch(prompt, resource_name, dataset, mode, *, prefix=None)</code>  <code>async</code>","text":"<p>Evaluate a prompt on a batch of tasks by running rollouts and computing average reward.</p> <p>This method:</p> <ol> <li>Adds the prompt as a named resource to the store</li> <li>Enqueues rollouts for each task in the dataset</li> <li>Waits for rollouts to complete (with timeout)</li> <li>Computes and returns the average reward</li> </ol> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>VersionedPromptTemplate</code>)           \u2013            <p>The prompt template string to evaluate.</p> </li> <li> <code>resource_name</code>               (<code>str</code>)           \u2013            <p>The name to register the prompt under in the store.</p> </li> <li> <code>dataset</code>               (<code>Sequence[T_task]</code>)           \u2013            <p>Sequence of tasks to evaluate the prompt on.</p> </li> <li> <code>mode</code>               (<code>RolloutMode</code>)           \u2013            <p>Rollout mode (\"train\" or \"val\") for logging/tracking.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[RolloutResultForAPO]</code>           \u2013            <p>A tuple of (rollout_results, average_reward) where rollout_results contains</p> </li> <li> <code>float</code>           \u2013            <p>detailed information for each rollout and average_reward is the mean final reward.</p> </li> </ul>"},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo.APO.get_adapter","title":"<code>get_adapter()</code>","text":"<p>Get the adapter for converting spans to messages.</p> <p>Returns:</p> <ul> <li> <code>TraceToMessages</code>           \u2013            <p>The TraceToMessages instance for this algorithm.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the adapter is not a TraceToMessages.</p> </li> </ul>"},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo.APO.get_best_prompt","title":"<code>get_best_prompt()</code>","text":"<p>Retrieve the best prompt discovered during optimization.</p> <p>Returns:</p> <ul> <li> <code>PromptTemplate</code>           \u2013            <p>The prompt template with the highest validation score found so far.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no best prompt has been found yet (run() not called).</p> </li> </ul>"},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo.APO.get_rollout_results","title":"<code>get_rollout_results(store, rollout, *, prefix=None)</code>  <code>async</code>","text":"<p>Convert completed rollouts to APO-compatible result format.</p> <p>Fetches spans for each rollout, adapts them to messages, and packages them with rewards and status information for gradient computation.</p> <p>Parameters:</p> <ul> <li> <code>rollout</code>               (<code>List[Rollout]</code>)           \u2013            <p>List of completed rollout metadata.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[RolloutResultForAPO]</code>           \u2013            <p>List of rollout results formatted for APO processing.</p> </li> </ul>"},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo.APO.get_seed_prompt_template","title":"<code>get_seed_prompt_template()</code>","text":"<p>Extract the initial prompt template from the algorithm's resources.</p> <p>Returns:</p> <ul> <li> <code>Tuple[str, PromptTemplate]</code>           \u2013            <p>A tuple of (resource_name, prompt_template) representing the seed prompt.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If initial_resources is not set or no PromptTemplate is found.</p> </li> </ul>"},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo.APO.run","title":"<code>run(store, llm_proxy, train_dataset=None, val_dataset=None)</code>  <code>async</code>","text":"<p>Execute the APO algorithm to optimize prompts through beam search with textual gradients.</p> <p>The algorithm performs iterative prompt optimization over multiple rounds:</p> <ul> <li>Each round: samples parent prompts, generates new candidates via textual gradients,   evaluates all candidates on validation data, and keeps the top performers</li> <li>Tracks the historically best prompt across all rounds</li> <li>Uses different training data samples for each gradient computation to ensure diversity</li> </ul> <p>Parameters:</p> <ul> <li> <code>train_dataset</code>               (<code>Optional[Dataset[T_task]]</code>, default:                   <code>None</code> )           \u2013            <p>Dataset of tasks for computing textual gradients. Required.</p> </li> <li> <code>val_dataset</code>               (<code>Optional[Dataset[T_task]]</code>, default:                   <code>None</code> )           \u2013            <p>Dataset of tasks for evaluating and selecting prompts. Required.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If train_dataset or val_dataset is None, or if resources are not set.</p> </li> </ul>"},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo.APO.textual_gradient_and_apply_edit","title":"<code>textual_gradient_and_apply_edit(current_prompt, rollout, *, prefix=None)</code>  <code>async</code>","text":"<p>Generate an improved prompt by computing a textual gradient and applying an edit.</p> <p>This is the main optimization step that:</p> <ol> <li>Computes a critique (textual gradient) based on rollout performance</li> <li>Uses another LLM to apply the critique and generate an improved prompt</li> </ol> <p>Parameters:</p> <ul> <li> <code>current_prompt</code>               (<code>VersionedPromptTemplate</code>)           \u2013            <p>The current prompt template to improve.</p> </li> <li> <code>rollout</code>               (<code>List[RolloutResultForAPO]</code>)           \u2013            <p>List of rollout results to base the critique on.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[str]</code>           \u2013            <p>The improved prompt text, or the original prompt if gradient computation fails.</p> </li> </ul>"},{"location":"algorithm-zoo/verl/","title":"VERL","text":"<p>Shortcut</p> <p>You can use the shortcut <code>agl.VERL(...)</code> to create a VERL instance.</p> <pre><code>import agentlightning as agl\n\nagl.VERL(...)\n</code></pre>"},{"location":"algorithm-zoo/verl/#installation","title":"Installation","text":"<pre><code>pip install agentlightning[verl]\n</code></pre> <p>Warning</p> <p>To avoid various compatibility issues, follow the steps in the installation guide to set up VERL and its dependencies. Installing VERL directly with <code>pip install agentlightning[verl]</code> can cause issues unless you already have a compatible version of PyTorch installed.</p> <p>Notes for Readers</p> <p>VERL in this article refers to a wrapper, provided by Agent-lightning, of the VERL framework. It's a subclass of agentlightning.Algorithm. To differentiate it from the VERL framework, all references to the VERL framework shall use the term \"VERL framework\", and all references to the Agent-lightning wrapper shall be highlighted with a link.</p>"},{"location":"algorithm-zoo/verl/#resources","title":"Resources","text":"<p>VERL expects no initial resources. The first LLM endpoint is directly deployed from the VERL configuration (<code>.actor_rollout_ref.model.path</code>). The resource key is always <code>main_llm</code>.</p> <p>VERL currently does not support optimizing multiple LLMs together.</p> <p>Note</p> <p>The resource type created by VERL is actually a ProxyLLM, a subclass of the LLM type. This object contains a URL template provided by VERL, with placeholders for rollout and attempt IDs. When a rollout begins on the agent side, the framework uses the current <code>rollout_id</code> and <code>attempt_id</code> to format this template, generating a final, unique endpoint URL. This URL points to VERL's internal proxy, allowing it to intercept and log all traffic for that specific attempt, for tracing and load balancing purposes. For agents created with the <code>@rollout</code> decorator, this resolution of the template is handled automatically (\"auto-stripped\"). Class-based agents will need to manually resolve the <code>ProxyLLM</code> using the rollout context.</p> <pre><code>proxy_llm = resources[\"main_llm\"]\nproxy_llm.get_base_url(rollout.rollout_id, rollout.attempt.attempt_id)\n</code></pre>"},{"location":"algorithm-zoo/verl/#customization","title":"Customization","text":"<p>Internally, VERL decomposes each agent execution into prompt\u2013response pairs via the Adapter and associates them with their corresponding reward signals as Triplet objects. The final scalar reward, derived from the last triplet in the trajectory, is propagated to all preceding triplets following the identical assignment strategy. This ensures that each triplet receives an identical reward signal and can be independently optimized as a valid RLHF trajectory within the VERL framework.</p> <p>At present, VERL does not expose fine-grained control over its reward propagation or credit assignment mechanisms. Users requiring customized reward shaping or trajectory decomposition are advised to clone and modify the VERL source implementation directly.</p>"},{"location":"algorithm-zoo/verl/#tutorials-using-verl","title":"Tutorials Using VERL","text":"<ul> <li>Train SQL Agent with RL - A practical example of training a SQL agent using VERL.</li> </ul>"},{"location":"algorithm-zoo/verl/#references-entrypoint","title":"References - Entrypoint","text":""},{"location":"algorithm-zoo/verl/#agentlightning.algorithm.verl","title":"<code>agentlightning.algorithm.verl</code>","text":""},{"location":"algorithm-zoo/verl/#agentlightning.algorithm.verl.VERL","title":"<code>VERL</code>","text":"<p>               Bases: <code>Algorithm</code></p> <p>VERL-powered algorithm that delegates training to the VERL PPO runner.</p> <p>Warning</p> <p>Advanced customisation currently requires copying the VERL source and modifying it directly. Native hooks for overriding training behaviour will land in a future release.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Dictionary mirroring the overrides passed to the VERL CLI. The overrides are merged with VERL's packaged defaults via Hydra before launching training.</p> </li> <li> <code>trainer_cls</code>               (<code>Optional[Type[AgentLightningTrainer]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional override for the trainer class. Experimental.</p> </li> <li> <code>daemon_cls</code>               (<code>Optional[Type[AgentModeDaemon]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional override for the daemon class. Experimental.</p> </li> </ul> <p>Trajectory aggregation (experimental)</p> <p>Trajectory-level aggregation merges an entire multi-turn rollout into a single, masked training sample so GPU time is spent once per trajectory rather than N times per turn. Enable it via:</p> <pre><code>config[\"agentlightning\"][\"trace_aggregator\"] = {\n    \"level\": \"trajectory\",\n    \"trajectory_max_prompt_length\": 4096,\n    \"trajectory_max_response_length\": 34384,\n}\n</code></pre> <p>Keep conversations structured (message lists rather than manual string concatenation) so prefix matching can stitch traces. <code>trajectory_max_prompt_length</code> should be set to the maximum length of the prompt for the first turn, and <code>trajectory_max_response_length</code> should be set to the maximum cumulative length of agent responses in the full trajectory. Toggle <code>debug=True</code> plus <code>mismatch_log_dir</code> when you need to inspect retokenization or chat-template mismatches. See this blog post for more details.</p> <p>Examples:</p> <pre><code>from agentlightning.algorithm.verl import VERL\n\nalgorithm = VERL(\n    config={\n        \"algorithm\": {\n            \"adv_estimator\": \"grpo\",\n            \"use_kl_in_reward\": False,\n        },\n        \"data\": {\n            \"train_batch_size\": 32,\n            \"max_prompt_length\": 4096,\n            \"max_response_length\": 2048,\n        },\n        \"actor_rollout_ref\": {\n            \"rollout\": {\n                \"tensor_model_parallel_size\": 1,\n                \"n\": 4,\n                \"log_prob_micro_batch_size_per_gpu\": 4,\n                \"multi_turn\": {\"format\": \"hermes\"},\n                \"name\": \"vllm\",\n                \"gpu_memory_utilization\": 0.6,\n            },\n            \"actor\": {\n                \"ppo_mini_batch_size\": 32,\n                \"ppo_micro_batch_size_per_gpu\": 4,\n                \"optim\": {\"lr\": 1e-6},\n                \"use_kl_loss\": False,\n                \"kl_loss_coef\": 0.0,\n                \"entropy_coeff\": 0,\n                \"clip_ratio_low\": 0.2,\n                \"clip_ratio_high\": 0.3,\n                \"fsdp_config\": {\n                    \"param_offload\": True,\n                    \"optimizer_offload\": True,\n                },\n            },\n            \"ref\": {\n                \"log_prob_micro_batch_size_per_gpu\": 8,\n                \"fsdp_config\": {\"param_offload\": True},\n            },\n            \"model\": {\n                \"path\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n                \"use_remove_padding\": True,\n                \"enable_gradient_checkpointing\": True,\n            },\n        },\n        \"trainer\": {\n            \"n_gpus_per_node\": 1,\n            \"val_before_train\": True,\n            \"critic_warmup\": 0,\n            \"logger\": [\"console\", \"wandb\"],\n            \"project_name\": \"AgentLightning\",\n            \"experiment_name\": \"calc_x\",\n            \"nnodes\": 1,\n            \"save_freq\": 64,\n            \"test_freq\": 32,\n            \"total_epochs\": 2,\n        },\n    }\n)\ntrainer.fit(algorithm, train_dataset=my_train_dataset)\n</code></pre>"},{"location":"algorithm-zoo/verl/#agentlightning.algorithm.verl.VERL.get_client","title":"<code>get_client()</code>","text":"<p>Create a client bound to the VERL-managed Agent Lightning server.</p> Deprecated <p>Since v0.2.</p>"},{"location":"algorithm-zoo/verl/#agentlightning.algorithm.verl.VERL.run","title":"<code>run(train_dataset=None, val_dataset=None)</code>","text":"<p>Launch the VERL PPO entrypoint with the configured runtime context.</p> <p>Parameters:</p> <ul> <li> <code>train_dataset</code>               (<code>Optional[Dataset[Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dataset forwarded to VERL for training.</p> </li> <li> <code>val_dataset</code>               (<code>Optional[Dataset[Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dataset forwarded to VERL for evaluation.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If required dependencies such as the store, LLM proxy, or adapter have been garbage-collected when using the V1 execution mode.</p> </li> </ul>"},{"location":"algorithm-zoo/verl/#references-implementation","title":"References - Implementation","text":""},{"location":"algorithm-zoo/verl/#agentlightning.verl","title":"<code>agentlightning.verl</code>","text":"<p>This package contains a hacky integration of VERL with Agent Lightning.</p>"},{"location":"algorithm-zoo/verl/#agentlightning.verl.AgentLightningTrainer","title":"<code>AgentLightningTrainer</code>","text":"<p>               Bases: <code>RayPPOTrainer</code></p> <p>Specialized PPO trainer for agent-based reinforcement learning.</p> <p>This trainer is designed specifically for scenarios where the model interacts with external environments, tools, or APIs through an AgentLightningServer. It simplifies the training loop by removing the complex conditional logic present in the original RayPPOTrainer and focusing on the agent mode workflow.</p> <p>Key differences from RayPPOTrainer:</p> <ol> <li>Uses AgentModeDaemon for server communication</li> <li>Simplified data flow without pop/union operations</li> <li>Direct batch processing through agent daemon</li> <li>Streamlined validation using agent_mode validation</li> </ol>"},{"location":"algorithm-zoo/verl/#agentlightning.verl.AgentModeDaemon","title":"<code>AgentModeDaemon</code>","text":"<p>AgentModeDaemon using the AgentLightningServer SDK.</p> <p>This class manages the server lifecycle, task queueing, and results retrieval, while also running a proxy server for LLM requests. It maintains the original interface for compatibility with the RayPPOTrainer.</p>"},{"location":"algorithm-zoo/verl/#agentlightning.verl.AgentModeDaemon.clear_data_and_server","title":"<code>clear_data_and_server()</code>","text":"<p>Resets the internal state of the daemon for the next run.</p>"},{"location":"algorithm-zoo/verl/#agentlightning.verl.AgentModeDaemon.get_test_metrics","title":"<code>get_test_metrics()</code>","text":"<p>Calculates and returns metrics for a validation run.</p>"},{"location":"algorithm-zoo/verl/#agentlightning.verl.AgentModeDaemon.get_train_data_batch","title":"<code>get_train_data_batch(max_prompt_length, max_response_length, device, global_steps)</code>","text":"<p>Processes completed rollouts to generate a training data batch.</p> <p>This function reconstructs the logic from the original AgentModeDaemon, using data retrieved from the new server architecture. It handles padding, truncation, and tensor creation for the PPO training loop.</p>"},{"location":"algorithm-zoo/verl/#agentlightning.verl.AgentModeDaemon.run_until_all_finished","title":"<code>run_until_all_finished(verbose=True)</code>","text":"<p>Synchronously waits for all queued tasks to be completed and reported.</p>"},{"location":"algorithm-zoo/verl/#agentlightning.verl.AgentModeDaemon.set_up_data_and_server","title":"<code>set_up_data_and_server(data, server_addresses, is_train=True)</code>","text":"<p>Synchronous wrapper for setting up data and server resources.</p>"},{"location":"algorithm-zoo/verl/#agentlightning.verl.AgentModeDaemon.start","title":"<code>start()</code>","text":"<p>Starts the main AgentLightningServer and the proxy server.</p>"},{"location":"algorithm-zoo/verl/#agentlightning.verl.get_left_padded_ids_and_attention_mask","title":"<code>get_left_padded_ids_and_attention_mask(ids, max_length, pad_token_id)</code>","text":"<p>Left-pad (or truncate) a sequence of token IDs to a fixed length, and build the corresponding attention mask.</p> <p>Parameters:</p> <ul> <li> <code>ids</code>               (<code>List[int]</code>)           \u2013            <p>the original list of token IDs.</p> </li> <li> <code>max_length</code>               (<code>int</code>)           \u2013            <p>desired total length after padding/truncation.</p> </li> <li> <code>pad_token_id</code>               (<code>int</code>)           \u2013            <p>ID to use for padding.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>padded_ids</code> (              <code>any</code> )          \u2013            <p>list of length == max_length.</p> </li> <li> <code>attention_mask</code> (              <code>any</code> )          \u2013            <p>list of same length: 1 for non-pad tokens, 0 for pads.</p> </li> </ul>"},{"location":"algorithm-zoo/verl/#agentlightning.verl.get_right_padded_ids_and_attention_mask","title":"<code>get_right_padded_ids_and_attention_mask(ids, max_length, pad_token_id)</code>","text":"<p>Right-pad (or truncate) a sequence of token IDs to a fixed length, and build the corresponding attention mask.</p> <p>Parameters:</p> <ul> <li> <code>ids</code>               (<code>List[int]</code>)           \u2013            <p>the original list of token IDs.</p> </li> <li> <code>max_length</code>               (<code>int</code>)           \u2013            <p>desired total length after padding/truncation.</p> </li> <li> <code>pad_token_id</code>               (<code>int</code>)           \u2013            <p>ID to use for padding.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>padded_ids</code> (              <code>any</code> )          \u2013            <p>list of length == max_length.</p> </li> <li> <code>attention_mask</code> (              <code>any</code> )          \u2013            <p>list of same length: 1 for non-pad tokens, 0 for pads.</p> </li> </ul>"},{"location":"community/contributing/","title":"Contributing Guide","text":"<p>Agent Lightning gets better every time someone files a clear bug, polishes docs, improves tests, or lands a new feature. This guide collects the expectations, checklists, and tips that help you go from \u201cI have an idea\u201d to \u201cmy pull request just merged.\u201d</p>"},{"location":"community/contributing/#before-you-start","title":"Before You Start","text":"<p>Agent-lightning is built by a small Microsoft Research team with limited reviewer hours and GPU budget. For any sizeable change (new algorithm, example, or API surface) please first discuss scope with us in Discord. Early alignment keeps your effort from being blocked late in the process.</p>"},{"location":"community/contributing/#where-you-can-help","title":"Where You Can Help","text":"<p>Pick a lane, or combine several. Just keep the discussion-first principle in mind for anything non-trivial.</p>"},{"location":"community/contributing/#documentation-improvements","title":"Documentation Improvements","text":"<p>Documentation improvements are the easiest way to get started. You can find more about how to write good documentations and organize documentations in the following sections. Here are some general contribution points we can think of:</p> <ul> <li>Tighten language, fix typos, clarify confusing sections, or add missing links. Fresh eyes catch docs gaps best.</li> <li>Organize content using the directories listed below so readers can actually find it.</li> <li>Avoid duplicate prose, unrelated \u201chow-to\u201d guides, or translations (we cannot maintain them today).</li> </ul> <p>Changes that are usually rejected</p> <ul> <li>Copy/pasting existing docs with shallow edits.</li> <li>Adding a <code>how-to</code> guide that is not tied to a new example.</li> <li>Adding doc translations to other languages (no capacity to review/maintain yet).</li> </ul>"},{"location":"community/contributing/#bug-fixes","title":"Bug Fixes","text":"<p>Bug fixes are the fastest way to get familiar with the codebase. To get started, you can:</p> <ul> <li>Browse the \"help wanted\" and \"bug\" labels; drop a comment before you start so we can mark it as taken.</li> <li>For fresh bugs, open an issue with reproduction steps, logs, and expected behavior before submitting a fix.</li> <li>Keep each pull request focused, ideally avoiding breaking API changes. Larger refactors should be discussed via RFC or maintainer sync.</li> </ul>"},{"location":"community/contributing/#new-examples","title":"New Examples","text":"<p>Examples must be curated so that we can maintain them. We generally merge only those that meet at least one (ideally several) of these criteria:</p> <ul> <li>Demonstrates an agent framework or workflow that is materially different from what already exists. (LangChain vs. LlamaIndex is not different enough; LangChain vs. n8n or Vercel AI SDK is, because they either have different orchestration paradigms or differ in programming languages.)</li> <li>Shows measurable performance gains on a real-world problem with a real-world dataset, such as tuning a search agent with Google Search API or improving a coding agent\u2019s (e.g., Claude Code) SWE-Bench score.</li> <li>Integrates a new algorithm, training backend, or serving stack (see \u201cNew Algorithms\u201d below).</li> <li>Validates scenarios that are rarely tested, such as multi-modality agents or long-lived memory/workflow agents.</li> </ul> <p>Bonus points for examples that:</p> <ul> <li>Ship CI or self-test coverage so we know they still work as the core evolves.  Otherwise, we would have to mark the example as unmaintained because we won't be able to test the examples manually before each release.</li> <li>Include a <code>docs/how-to/</code> guide (or a detailed README if no how-to exists) without duplicating content in multiple places.</li> <li>Favor simple, dependency-light code over heavy abstractions.</li> <li>Ship a README that documents smoke-test instructions and includes an \"Included Files\" section summarizing every file and its role; keep the runnable module self-contained with a module-level docstring explaining CLI usage, plus targeted docstrings or inline comments for educational functions/classes.</li> </ul> <p>Please discuss first</p> <p>Examples tend to be the most time-consuming contributions for both you and reviewers. Sync with us on Discord or through an issue before diving into a new one.</p>"},{"location":"community/contributing/#fresh-implementations-of-core-modules","title":"Fresh Implementations of Core Modules","text":"<p>If you are looking to extend <code>Runner</code>, <code>Tracer</code>, <code>Adapter</code>, <code>LightningStore</code>, or another core interface, here are the steps:</p> <ol> <li>File an issue or proposal first.</li> <li>Explain which interface you are extending, why existing implementations are insufficient, and how you intend to test compatibility with the rest of the stack (unit tests, documentation updates, example refreshes, etc.).</li> <li>Any API changes must be reviewed up front. DO NOT begin coding large changes before the discussion lands!</li> </ol>"},{"location":"community/contributing/#new-algorithms","title":"New Algorithms","text":"<p>If you are integrating a new training/serving backend, check whether it already lives in the Algorithm Zoo or is covered in the Examples Catalog. We especially welcome:</p> <ul> <li>Currently unsupported or under-tested algorithms such as Supervised Fine-tuning (SFT), Direct Policy Optimization (DPO), or Monte Carlo Tree Search (MCTS).</li> <li>Tuning Resources that are not supported yet, such as workflows or memory.</li> <li>Expansions of supported stacks, e.g., adding multi-modality to APO or multi-agent prompt tuning.</li> <li>Reinforcement-learning integrations beyond our current stack of VERL, vLLM, Azure OpenAI, and Tinker. Contributions using SGLang, TRL, SkyRL, RLinf, litgpt, or similar are welcome.</li> </ul> <p>Most brand-new algorithms ultimately land as \u201cnew examples,\u201d so read that section too. Post an issue or design doc to scope the work, reuse existing utilities, and avoid duplicating efforts. Mature, battle-tested examples graduate into the Algorithm Zoo.</p>"},{"location":"community/contributing/#ecosystem-projects","title":"Ecosystem Projects","text":"<p>Have a project that builds on Agent-lightning but does not belong in the main repo? Fork it or depend on it externally, then let us know. We can showcase notable projects in Community Projects and the main README.</p>"},{"location":"community/contributing/#agent-lightning-contrib","title":"Agent-lightning Contrib","text":"<p><code>contrib/</code> is where work-in-progress or third-party integrations, and curated recipes live before they are hardened enough for the core runtime tree. Think of it as an incubator: additions should remain easy to consume, clearly owned, and scoped so downstream users can vendor them with minimal risk.</p> <p>The following types of contributions are welcome in the contrib area:</p> <ul> <li>Recipes that assemble multiple Agent Lightning components for a narrow task (<code>contrib/recipes/&lt;topic&gt;/</code>). Each recipe must be self-contained, include running instructions and result reports.</li> <li>Runtime extensions that would bloat the primary <code>agentlightning/</code> namespace (<code>contrib/agentlightning/contrib/&lt;feature&gt;/</code>). These should mirror the published wheel layout so that <code>import agentlightning.contrib.&lt;feature&gt;</code> works out of the box.</li> <li>Supporting scripts and assets (<code>contrib/scripts/</code>) that automate dataset downloads, environment preparation, or benchmarks required by contrib modules.</li> </ul> <p>If you are unsure where a contribution should live, start a thread in Discord or open an issue before writing code. The contrib README also lists the directory expectations.</p> <p>A quick checklist for contributions to be accepted:</p> <ol> <li>Document everything. Include configuration steps, environment variables, and sample commands so contributors can reproduce the results without guesswork. Pin to a specific version of Agent-lightning and other dependencies to avoid unexpected changes if you don't want to update the recipe frequently.</li> <li>Keep quality predictable. Match the repo\u2019s style guide, apply exhaustive type hints, and run <code>uv run --no-sync pyright</code> plus targeted <code>pytest</code> suites for any Python module you touch.</li> <li>Ship reproducibility artifacts. Store only scripts or instructions for downloading datasets, weights, or binaries. Never upload large artifacts or credentials directly.</li> <li>Update ownership. Add <code>CODEOWNERS</code> entries when new directories appear so maintainers know who can review follow-up fixes.</li> </ol> <p>Contrib entries do not need the same maturity level as core code, but they must still meet the baseline above. Submissions that lack documentation, hide ownership, or depend on untracked assets are typically rejected until those gaps are resolved.</p>"},{"location":"community/contributing/#other-contribution-ideas","title":"Other Contribution Ideas","text":"<ul> <li>Tests. Add or improve cases in <code>tests/</code> (unit, integration, or end-to-end).</li> <li>Benchmarks. Expand <code>tests/benchmark</code> to stress large-scale training or rollouts.</li> <li>Issue triage. Reproduce bugs, confirm whether they reproduce on <code>main</code>, or suggest short-term mitigations so maintainers can prioritize.</li> </ul>"},{"location":"community/contributing/#contribution-workflow","title":"Contribution Workflow","text":"<p>The steps below keep changes reviewable and CI-friendly. Follow them in order; rerun the relevant pieces if you revisit a branch later.</p>"},{"location":"community/contributing/#1-prepare-your-environment","title":"1. Prepare Your Environment","text":"<p>Minimum tooling:</p> <ul> <li>Python 3.10+ (3.12 recommended).</li> <li>uv for dependency and virtual-environment management. Install it using the official uv docs.</li> <li>Git configured with your GitHub credentials.</li> </ul> <p>Clone your fork and point <code>upstream</code> at the official repo:</p> <pre><code>git clone git@github.com:&lt;your-username&gt;/agent-lightning.git\ncd agent-lightning\ngit remote add upstream https://github.com/microsoft/agent-lightning.git\n</code></pre> <p>Install the default development stack:</p> <pre><code>uv sync --group dev\n</code></pre> <p>Need GPU extras or specific optional dependencies? Lock them in with one command:</p> <pre><code>uv sync --frozen \\\n    --extra apo \\\n    --extra verl \\\n    --group dev \\\n    --group torch-cpu \\\n    --group torch-stable \\\n    --group agents \\\n    --no-default-groups\n</code></pre> <p>After <code>uv sync</code>, run commands via <code>uv run ...</code> (add <code>--no-sync</code> once the environment is locked) or activate <code>.venv/</code>.</p>"},{"location":"community/contributing/#2-install-and-run-pre-commit","title":"2. Install and Run Pre-commit","text":"<p>Formatting and linting are enforced through pre-commit. Install once, then run before each push:</p> <pre><code>uv run --no-sync pre-commit install\nuv run --no-sync pre-commit run --all-files --show-diff-on-failure --color=always\n</code></pre> <p>Once installed, the hooks run automatically on every <code>git commit</code>. Running the pre-commit hooks locally keeps CI green and diffs manageable.</p>"},{"location":"community/contributing/#3-branch-from-fresh-main-and-code","title":"3. Branch from Fresh <code>main</code> and Code","text":"<p>Start all work from the latest upstream state:</p> <pre><code>git fetch upstream\ngit checkout main\ngit merge upstream/main\n</code></pre> <p>Branch naming convention:</p> <ul> <li><code>feature/&lt;short-description&gt;</code> for new features.</li> <li><code>fix/&lt;short-description&gt;</code> for bug fixes.</li> <li><code>docs/&lt;short-description&gt;</code> for documentation-only updates.</li> <li><code>chore/&lt;short-description&gt;</code> for tooling or maintenance.</li> </ul> <p>Use lowercase with hyphens, e.g., <code>feature/async-runner-hooks</code>.</p> <p>Where should docs or examples live?</p> <p>Many new contributors get confused about what to put in the <code>docs/how-to/</code> directory and what to put in the <code>examples/</code> directory (particularly README files). Here is a quick reference you can refer to:</p> Location Description <code>docs/algorithm-zoo/</code> Documentation for built-in algorithms shipped with Agent-lightning. <code>docs/how-to/</code> Step-by-step how-to guides, usually tied to an example in <code>examples/</code>. <code>docs/tutorials/</code> Conceptual walkthroughs for components or workflows. See debugging or parallelization for examples. <code>docs/deep-dive/</code> Advanced explanations and in-depth concepts. <code>examples/&lt;name&gt;/README.md</code> Example-specific README. If any related how-to if that exists, link to it avoid duplicating the same instructions twice; write only brief instructions on how to install and run the example. Otherwise, you can make the README more detailed and self-explanatory. <p>Remember to register new docs in <code>mkdocs.yml</code>, add examples to examples/README, and update the Examples Catalog.</p> <p>Before you start coding, bring the shared coding conventions with you:</p> <ul> <li>Target <code>requires-python &gt;= 3.10</code>, four-space indentation, ~120-character lines (docstrings may run longer), and formatter-owned diffs (Black + isort with the <code>black</code> profile).</li> <li>Use <code>snake_case</code> for modules, functions, and variables; <code>PascalCase</code> for classes and React components; lowercase hyphenation for CLI flags, branch names, and TypeScript filenames.</li> <li>Maintain exhaustive type hints (pyright enforces them), write succinct Google-style docstrings (with <code>[][]</code> cross-references).</li> <li>Prefer dataclasses or Pydantic models from <code>agentlightning.types</code>.</li> <li>Log via <code>logging.getLogger(__name__)</code> with targeted DEBUG/INFO/WARNING/ERROR calls\u2014especially for long multi-step functions or broad <code>try/except</code> blocks.</li> </ul>"},{"location":"community/contributing/#4-test-and-validate","title":"4. Test and Validate","text":"<p>Most contributions require automated checks. Once <code>uv sync</code> locks dependencies, prefix commands with <code>uv run --no-sync ...</code> so they share the same environment as CI.</p> <p>Full test suite</p> <pre><code>uv run --no-sync pytest -v\n</code></pre> <p>Targeted tests</p> <pre><code>uv run --no-sync pytest tests/path/to/test_file.py -k test_name\n</code></pre> <p>Optional/gated tests: GPU-specific suites or API-dependent tests run automatically when the required hardware or environment variables (such as <code>OPENAI_API_KEY</code>) are present.</p> <p>Static analysis:</p> <pre><code>uv run --no-sync pyright\n</code></pre> <p>If you have touched code under <code>examples/</code>, you should run the example-specific smoke tests. Each directory includes a README with example-specific smoke tests\u2014run those too.</p> <p>Build documentation when needed</p> <p>Keep API references under docs/reference up to date. Doc-only changes should still build cleanly:</p> <pre><code>uv run --no-sync mkdocs serve --strict   # live reload\nuv run --no-sync mkdocs build --strict   # CI-equivalent\n</code></pre> <p><code>--strict</code> elevates warnings to errors so you catch issues before CI.</p> <p>Before opening a PR, double-check the basics:</p> <ul> <li>Run <code>uv lock</code> if you changed dependencies.</li> <li>Run <code>uv run --no-sync pre-commit run --all-files --show-diff-on-failure</code> (hooks installed via <code>pre-commit install</code> run automatically on <code>git commit</code>, but rerun them if you amended history).</li> <li>Execute the relevant commands from the test list above.</li> <li>Validate each affected example via its README instructions.</li> </ul>"},{"location":"community/contributing/#5-open-a-pull-request","title":"5. Open a Pull Request","text":"<ol> <li>Push your branch:    <pre><code>git push origin &lt;branch-name&gt;\n</code></pre></li> <li>Open a PR against <code>microsoft/agent-lightning:main</code>.</li> <li>Fill out the template with a concise summary, the commands/tests you ran, and linked issues (use <code>Fixes #123</code> syntax to auto-close).</li> <li>Include screenshots or logs if they clarify behavior.</li> <li>Address review feedback promptly. Follow-up tweaks work best as focused commits; <code>git commit --fixup</code> is handy for reviewer-suggested edits.</li> </ol> <p>Thanks for contributing! every improvement strengthens the Agent Lightning community!</p>"},{"location":"community/maintainers/","title":"Maintainer Guide","text":"<p>This guide describes the day-to-day responsibilities for Agent Lightning maintainers\u2014how to bump versions, run release ceremonies, interact with CI, and backport fixes safely.</p>"},{"location":"community/maintainers/#release-workflow","title":"Release Workflow","text":"<p>Follow this checklist throughout each release cycle.</p>"},{"location":"community/maintainers/#immediately-after-shipping","title":"Immediately After Shipping","text":"<p>Agent Lightning uses a bump-first strategy. As soon as a release is published:</p> <ol> <li>Update version metadata:<ul> <li><code>pyproject.toml</code>: bump the <code>version</code>.</li> <li><code>agentlightning/__init__.py</code>: update <code>__version__</code> if it exists.</li> <li><code>uv.lock</code>: refresh the lock file after the bump.</li> </ul> </li> <li> <p>Refresh dependency pins as needed:     <pre><code>uv lock --upgrade\n</code></pre></p> </li> <li> <p>For a new minor or major release, create a stable branch from <code>main</code>:     <pre><code>git checkout main\ngit pull upstream main\ngit checkout -b stable/v2.0.x  # adjust to the new series\ngit push upstream stable/v2.0.x\n</code></pre></p> <p>All future changes to the stable branch must land via pull requests.</p> </li> </ol>"},{"location":"community/maintainers/#preparing-the-next-release","title":"Preparing the Next Release","text":"<p>When it is time to publish the next version:</p> <ol> <li>Draft release notes in <code>docs/changelog.md</code>, collecting every notable change since the previous tag.</li> <li>Open a release PR targeting <code>main</code> (for minor/major) or the relevant stable branch (for patch releases). Use the title <code>[Release] vX.Y.Z</code>.</li> <li>Run extended CI by labeling the PR with <code>ci-all</code> and commenting <code>/ci</code>. Investigate and resolve any failures.</li> <li>Merge the release PR once notes are final and CI is green.</li> <li> <p>Tag the release from the branch you just merged into:</p> <pre><code>git checkout main          # minor/major releases\ngit checkout stable/vX.Y.Z # patch releases\n\ngit pull\ngit tag vX.Y.Z -m \"Release vX.Y.Z\"\ngit push upstream vX.Y.Z\n</code></pre> <p>Pushing the tag publishes to PyPI and deploys the documentation.</p> </li> <li> <p>Publish the GitHub release using the drafted notes, and confirm the docs site and PyPI listing reflect the new version.</p> </li> </ol>"},{"location":"community/maintainers/#working-with-ci-labels-and-ci","title":"Working with CI Labels and <code>/ci</code>","text":"<p>GPU suites and example end-to-end runs are opt-in. To trigger them on a pull request:</p> <ol> <li>Apply the appropriate labels before issuing the command:<ul> <li><code>ci-all</code> for every repository-dispatch workflow.</li> <li><code>ci-gpu</code> for GPU integration tests (<code>tests-full.yml</code>).</li> <li><code>ci-apo</code>, <code>ci-calc-x</code>, <code>ci-spider</code>, <code>ci-unsloth</code>, <code>ci-compat</code> for the individual example pipelines.</li> </ul> </li> <li>Comment <code>/ci</code> on the PR. The <code>issue-comment</code> workflow acknowledges the request and tracks job results inline.</li> <li>Remove the labels once you have the signal to avoid accidental re-runs.</li> </ol> <p>Use <code>/ci</code> whenever changes touch shared infrastructure, dependencies, or training loops that require coverage beyond the default PR checks.</p> <p>Note</p> <p><code>/ci</code> always executes the workflow definitions on the current <code>main</code> branch, then checks out the PR diff. If you need to test workflow modifications, push the changes to a branch in the upstream repo and run:</p> <pre><code>gh workflow run examples-xxx.yml --ref your-branch-name\n</code></pre>"},{"location":"community/maintainers/#backporting-pull-requests","title":"Backporting Pull Requests","text":"<p>Supported stable branches rely on automated backports:</p> <ol> <li>Identify the target branch (for example <code>stable/v0.2.x</code>).</li> <li>Before merging the original PR into <code>main</code>, add the matching <code>stable/&lt;series&gt;</code> label (e.g. <code>stable/v0.2.x</code>).</li> <li>The <code>backport.yml</code> workflow opens a follow-up PR named <code>backport/&lt;original-number&gt;/&lt;target-branch&gt;</code> authored by <code>agent-lightning-bot</code>.</li> <li>Review the generated PR, ensure CI is green, and merge into the stable branch.</li> <li>Resolve conflicts by pushing manual fixes to the backport branch and re-running <code>/ci</code> if required.</li> </ol> <p>Keep stable branches healthy by cherry-picking only critical fixes and ensuring documentation and example metadata stay aligned with each release line.</p>"},{"location":"deep-dive/birds-eye-view/","title":"The Bird's Eye View of Agent-lightning","text":"<p>High Volume of Information Ahead</p> <p>This article provides an in-depth exploration of the Agent-lightning architecture. It is not intended as a beginner\u2019s guide or usage tutorial.</p> <p>This article summarizes how Agent-lightning (as of v0.2) wires the Algorithm, Runner, and LightningStore loop together and shows where auxiliary components (the Tracer, Adapter, and LLM Proxy) plug into the loop. Each section provides a diagram for a different perspective of the system.</p>"},{"location":"deep-dive/birds-eye-view/#algorithm-runner-store-data-flow","title":"Algorithm \u2194 Runner \u2194 Store data flow","text":"<p>At its heart, Agent-lightning is built on three main components that work in a coordinated loop:</p> <ul> <li>Algorithm: The \"brain\" of the system. It decides what tasks to run, learns from the results, and updates resources (like AI models or prompts).</li> <li>Runner: The \"worker\" of the system. It executes tasks assigned by the algorithm, runs the agent, and records the results.</li> <li>LightningStore: The central \"database\" and message queue. It acts as the single source of truth, storing tasks, results, and resources, and enabling communication between the Algorithm and Runner.</li> </ul> <p>The typical data flow in a training loop is as follows: The Algorithm enqueues tasks (called Rollouts) into the LightningStore. A Runner then dequeues a task, executes it, and streams the results (called Spans) back to the store. Once the task is complete, the algorithm can query the new data from the store to learn and update its resources.</p> <p>The diagram below shows this fundamental interaction in a simple, non-parallel setup.</p> <pre><code>sequenceDiagram\n    autonumber\n    participant Algo as Algorithm\n    participant Store as LightningStore\n    participant Runner\n    participant Agent\n\n    loop Over the dataset\n        Algo--&gt;&gt;Store: add_resources + enqueue_rollout\n        Store--&gt;&gt;Runner: dequeue_rollout \u2192 AttemptedRollout\n        Store--&gt;&gt;Runner: get_latest_resources\n        Runner--&gt;&gt;Store: update_attempt(\"running\", worker_id)\n        Runner-&gt;&gt;Agent: rollout + resources\n        Agent-&gt;&gt;Runner: reward / spans\n        Runner--&gt;&gt;Store: add_span or add_otel_span\n        Runner--&gt;&gt;Store: update_attempt(\"finished\", status)\n        Store--&gt;&gt;Algo: query_rollouts + spans\n        Algo--&gt;&gt;Algo: Update resources (optional)\n    end</code></pre> <p>Solid lines represent direct calls, while dashed lines are asynchronous or long-running operations.</p>"},{"location":"deep-dive/birds-eye-view/#key-terminology","title":"Key Terminology","text":"<p>We define the following terms, which may be helpful for understanding the diagram above.</p> <ul> <li>Resources: A collection of assets to be tuned or trained. Agents perform rollouts against resources and collect span data. Algorithms use those data to update the resources. In RL training, the resources are a tunable model. In prompt tuning, the resources are prompt templates.</li> <li>Rollout: A unit of work that an agent performs against a resource. A rollout (noun) can be incomplete, in which case it is also known as a task, sample, or job (these terms are used interchangeably). The agent executes its own defined workflow against the rollout \u2014 the process is also called \"to rollout\" (verb). After execution, the rollout (noun) is considered complete.</li> <li>Attempt: A single execution of a rollout. One rollout can have multiple attempts in case of failures or timeouts.</li> <li>Span: During the rollout, the agent can generate multiple spans (also known as \"traces\" or \"events\"). The recorded spans are collected in the store, which is crucial for understanding agent behavior and optimizing agents.</li> <li>Reward: A special span that is defined as a number judging the quality of the rollout during some period of the rollout.</li> <li>Dataset: A collection of incomplete rollouts (i.e., tasks) for the agent to process. The dual datasets (train, val) serve as the initial input for the algorithm to enqueue the first batch of rollouts.</li> </ul>"},{"location":"deep-dive/birds-eye-view/#store","title":"Store","text":"<p>As discussed previously, the LightningStore is the central hub for all data in Agent-lightning. The store exposes a set of APIs for algorithms and runners to interact with the data; the most important ones are:</p> <pre><code>from agentlightning.types import AttemptedRollout, ResourcesUpdate, Span, TaskInput\n\nclass LightningStore:\n\n    async def enqueue_rollout(self, input: TaskInput, ...) -&gt; Rollout: ...\n\n    async def dequeue_rollout(self) -&gt; AttemptedRollout | None: ...\n\n    async def add_span(self, span: Span) -&gt; Span: ...\n\n    async def get_latest_resources(self) -&gt; Optional[ResourcesUpdate]: ...\n\n    async def wait_for_rollouts(self, rollout_ids: List[str], ...): ...\n\n    async def query_spans(self, rollout_id: str, ...): ...\n\n    async def update_attempt(self, rollout_id: str, attempt_id: str, status: str, ...): ...\n\n    ...\n</code></pre> <p>These interfaces operate on <code>AttemptedRollout</code>, <code>ResourcesUpdate</code>, <code>Span</code>, and <code>TaskInput</code> instances from <code>agentlightning.types</code>.</p> <p>As the APIs show, the store essentially provides a queue for rollouts and storage for resources, spans, and attempts. Developers should implement the store carefully to ensure data integrity and consistency, especially when multiple runners work in parallel across multiple attempts.</p> <p>The store is designed to be extensible. Users can implement their own store by inheriting from <code>LightningStore</code> and overriding methods. Agent-lightning provides a few reference implementations, such as <code>InMemoryLightningStore</code> (default) and <code>SqliteLightningStore</code> (under construction). When parallelized, the store may need special wrappers to ensure thread/process safety or delegate computation to a store in another process or machine.</p>"},{"location":"deep-dive/birds-eye-view/#supporting-components-in-the-loop","title":"Supporting Components in the Loop","text":"<p>While the core loop is simple, Agent-lightning provides several components to make development easier and more powerful.</p>"},{"location":"deep-dive/birds-eye-view/#tracer","title":"Tracer","text":"<p>The <code>Tracer</code> is a component within the <code>Runner</code> that records detailed spans (events) during an agent's execution and sends them to the <code>LightningStore</code>. Instead of requiring the agent to manually log every span, the tracer automatically instruments key methods (e.g., LLM calls) and captures their inputs, outputs, and metadata. This provides a detailed log of the agent's behavior with minimal effort.</p> <pre><code>sequenceDiagram\n    autonumber\n    participant Store\n    participant Runner\n    participant Tracer\n    participant Agent\n\n    Note over Runner,Tracer: Runner manages tracer as member\n\n    Tracer-&gt;&gt;Agent: Apply instrumentation\n    loop Until no more rollouts\n        Store--&gt;&gt;Runner: dequeue_rollout \u2192 AttemptedRollout\n        Store--&gt;&gt;Runner: get_latest_resources\n        Runner-&gt;&gt;Agent: training_rollout / validation_rollout\n        loop For each finished span\n            Agent--&gt;&gt;Tracer: openai.chat.completion invoked&lt;br&gt;agent.execute invoked&lt;br&gt;...\n            Agent-&gt;&gt;Tracer: emit intermediate reward\n            Tracer--&gt;&gt;Store: add_otel_span(rollout_id, attempt_id, span)\n        end\n        Agent-&gt;&gt;Runner: final reward + extra spans (if any)\n        Runner--&gt;&gt;Store: add_span(rollout_id, attempt_id, span)\n        Runner--&gt;&gt;Store: update_attempt(status)\n    end\n    Tracer-&gt;&gt;Agent: Unapply instrumentation</code></pre> <p>The above diagram shows the overall data flow between store, tracer and agent. In realistic, it's a bit more complicated than that. Spans are not emitted actively by the agent; they are intercepted by the tracer by hooking and instrumenting key methods used in the agents.  The tracer uses a callback (called exporter) to monitor events and log to the store. Before a rollout starts, the runner enters a <code>trace_context</code> before invoking the agent, wiring store identifiers into the tracer. Each span completion streams back to the store through <code>LightningSpanProcessor</code>, so the agent\u2019s instrumentation lands in <code>add_otel_span</code>. If the agent\u2019s rollout method returns a numeric reward, the runner emits one more OpenTelemetry span before finalizing the attempt.</p>"},{"location":"deep-dive/birds-eye-view/#hooks","title":"Hooks","text":"<p><code>Hook</code> implementations are user-defined callback functions that allow you to augment a <code>Runner</code>'s behavior at specific points in its lifecycle. You can use hooks to add custom logging, set up resources before a rollout begins, or tear them down after it ends. Hooks can be triggered at four key moments: <code>on_rollout_start</code>, <code>on_trace_start</code>, <code>on_trace_end</code>, and <code>on_rollout_end</code>.</p> <p>Users should pay special attention to the difference between <code>on_trace_end</code> and <code>on_rollout_end</code>. The former is called right before the tracer exits the trace context, while the latter is called after the runner processes the final leftover rewards and spans, and finalizes the attempt in the store.</p> <pre><code>sequenceDiagram\n    autonumber\n    participant Store\n    participant Hooks\n    participant Runner\n    participant Tracer\n    participant Agent\n\n    Note over Runner,Hooks: Runner manages hooks as member\n\n    loop Until no more rollouts\n        Store--&gt;&gt;Runner: dequeue_rollout \u2192 AttemptedRollout\n        Store--&gt;&gt;Runner: get_latest_resources\n\n        Runner-&gt;&gt;Hooks: on_rollout_start(agent, runner, rollout)\n        Runner-&gt;&gt;Agent: training_rollout / validation_rollout\n        Tracer-&gt;&gt;Agent: enter_trace_context\n        activate Tracer\n        Runner-&gt;&gt;Hooks: on_trace_start(agent, runner, tracer, rollout)\n        Note over Runner,Agent: Agent rollout omitted\n        Runner-&gt;&gt;Hooks: on_trace_end(agent, runner, tracer, rollout)\n        Tracer-&gt;&gt;Agent: exit_trace_context\n        deactivate Tracer\n        Agent-&gt;&gt;Runner: final reward + extra spans (if any)\n        Runner--&gt;&gt;Store: add_span(rollout_id, attempt_id, span)\n        Runner-&gt;&gt;Hooks: on_rollout_end(agent, runner, rollout, status)\n    end</code></pre>"},{"location":"deep-dive/birds-eye-view/#adapter","title":"Adapter","text":"<p>The <code>Adapter</code> is a component used by the <code>Algorithm</code> to transform raw data from the <code>LightningStore</code> into a format suitable for learning. Runners stream raw spans into the store during execution. Later, the algorithm queries these spans and uses an adapter to convert them into structured data, like training examples for a reinforcement learning model.</p> <p>For instance, the <code>TracerTraceToTriplet</code> processes OpenTelemetry spans to create <code>(prompt, response, reward)</code> triplets, which are the fundamental data structure for many RL fine-tuning algorithms.</p> <pre><code>flowchart LR\n    Runner -- (1) add_otel_span --&gt; Store\n    Store -- (2) query_spans --&gt; Algorithm\n    Algorithm -- (3) spans --&gt; Adapter\n    Adapter -- (4) transformed data --&gt; Algorithm</code></pre>"},{"location":"deep-dive/birds-eye-view/#llm-proxy","title":"LLM Proxy","text":"<p>The <code>LLMProxy</code> is an optional bridge component that sits between an agent and the algorithms' resources. It acts as a centralized endpoint for all LLM calls. Usually the proxy URL is added to the store as a special resource, so that the <code>Runner</code> can fetch it along with other resources when dequeuing a rollout. During rollouts, the runner invokes the proxy's HTTP endpoint instead of calling a model backend directly.</p> <p>This design offers several benefits:</p> <ol> <li>Instrumentation: It automatically captures detailed traces of LLM interactions (prompts, responses, metadata) and sends them to the store, complementing the tracer, especially when the agent's code is hard to instrument directly.</li> <li>Backend Abstraction: It provides a unified interface for various LLM backends (OpenAI, Anthropic, local models) and can add features like retry logic, rate limiting, and caching.</li> <li>Resource Management: The algorithm can dynamically update which LLM the agent uses (e.g., swapping to a newly fine-tuned model) by simply swapping the backend model the proxy is using, without interrupting the agent's code.</li> </ol> <p>The benefits above seem to be all discussed within the context of model fine-tuning. As a matter of fact, the proxy can be useful for prompt tuning as well. The algorithm can register one of the following two types of endpoints into the proxy:</p> <ol> <li>Endpoint served by the algorithm: If the algorithm is internally updating the LLM weights (e.g., RL), it can launch an LLM inference engine (i.e., a model server) and register the endpoint URL with the proxy. The proxy then forwards all LLM calls to that endpoint.</li> <li>Third-party LLM endpoint: If the algorithm is not updating the LLM weights (e.g., prompt tuning), it can register a third-party LLM endpoint into the proxy.</li> </ol> <p>We show a diagram below that illustrates how the proxy fits into the overall data flow.</p> <pre><code>sequenceDiagram\n    autonumber\n    participant Algo as Algorithm\n    participant LLMProxy as LLM Proxy\n    participant Store\n    participant Runner\n    participant Agent\n\n    Note over Algo,LLMProxy: Algorithm manages LLMProxy as member\n\n    loop Over the Dataset\n        Algo-&gt;&gt;Algo: Launch LLM Inference Engine&lt;br&gt;(optional)\n        Algo-&gt;&gt;LLMProxy: Register Inference Engine&lt;br&gt;(optional)\n        Algo--&gt;&gt;Store: enqueue_rollout\n        LLMProxy-&gt;&gt;Store: Proxy URL added as Resource\n        Store--&gt;&gt;Runner: dequeue_rollout \u2192 AttemptedRollout\n        Store--&gt;&gt;Runner: get_latest_resources\n        Runner-&gt;&gt;Agent: rollout + resources&lt;br&gt;(LLM Proxy URL as resource)\n        loop Defined by Agent\n            Agent--&gt;&gt;LLMProxy: LLM calls\n            activate LLMProxy\n            LLMProxy--&gt;&gt;Store: add_span or add_otel_span\n            LLMProxy--&gt;&gt;Agent: LLM responses\n            deactivate LLMProxy\n            Agent--&gt;&gt;Runner: rewards\n            Runner--&gt;&gt;Store: add_span or add_otel_span\n        end\n        Runner--&gt;&gt;Store: update_attempt(\"finished\", status)\n        Store--&gt;&gt;Algo: query_rollouts + spans\n        Algo--&gt;&gt;Algo: Update LLM Weights&lt;br&gt;(optional)\n    end</code></pre> <p>In this diagram, the store receives spans from both the proxy and the runner. We will see a problem later with parallelism where the proxy and runner are in different machines, and spans need to obtain a special counter from the store to ensure the ordering of spans.</p>"},{"location":"deep-dive/birds-eye-view/#trainer","title":"Trainer","text":"<p>The Trainer is the high-level orchestrator that initializes and connects all major components -- Algorithm, Runner, LightningStore, Tracer, Adapter, LLM Proxy, and Hook. The components can have a lifecycle as long as the trainer. The trainer manages their lifecycles and handles dependency injection, ensuring that every part of the system operates within a consistent and shared environment.</p> <p>Below, we demonstrate how the components relate to each other and their roles. We first clarify the roles and relationships shown in the diagram:</p> <ol> <li>Owns: components that the trainer constructs and manages directly (e.g., runner, tracer).</li> <li>Injects: components passed into others as dependencies.</li> <li>References: weak links for coordination without ownership.</li> <li>Uses: components that are temporarily interacted with.</li> </ol> <p>For example, the LightningStore is injected into the Algorithm and Runner. The Tracer and LitAgent are injected into the runner. The Adapter and LLM Proxy are injected into the algorithm. The store is further injected into the tracer, adapter and LLM proxy by the runner and algorithm respectively.</p> <pre><code>flowchart TD\n    %% === Left side: Algorithm domain ===\n    subgraph L[\"Algorithm Side\"]\n        Algorithm[\"Algorithm&lt;br&gt;(no default)\"]\n        Adapter[\"Adapter&lt;br&gt;(TracerTraceToTriplet*)\"]\n        LLMProxy[\"LLM Proxy&lt;br&gt;(no default)\"]\n        Algorithm -.injects.-&gt; Adapter\n        Algorithm -.injects.-&gt; LLMProxy\n    end\n    linkStyle 0,1 stroke:#896978,stroke-width:2px;\n\n    %% === Middle: Core trainer and store ===\n    subgraph M[\"Core\"]\n        Trainer[\"Trainer\"]\n        Store[\"LightningStore&lt;br&gt;(InMemory* default)\"]\n        Trainer --has--&gt; Algorithm\n        Trainer --has--&gt; Store\n        Trainer --has--&gt; Adapter\n        Trainer --has--&gt; LLMProxy\n    end\n    linkStyle 2,3,4,5 stroke:#839791,stroke-width:2px;\n\n    %% === Right side: Runner side ===\n    subgraph R[\"Runner Side\"]\n        Runner[\"Runner&lt;br&gt;(LitAgentRunner* default)\"]\n        Tracer[\"Tracer&lt;br&gt;(AgentOpsTracer*)\"]\n        Hooks[\"Hooks (empty default)\"]\n        Agent[\"Agent&lt;br&gt;(LitAgent*)\"]\n        Runner -.injects.-&gt; Tracer\n        Runner -.injects.-&gt; Store\n        Runner -.injects.-&gt; Agent\n        Runner -.injects.-&gt; Hooks\n        Tracer -.injects.-&gt; Store\n        Hooks -.uses.-&gt; Runner\n        Hooks -.uses.-&gt; Agent\n        Hooks -.uses.-&gt; Tracer\n    end\n    linkStyle 6,7,8,9,10 stroke:#896978,stroke-width:2px;\n    linkStyle 11,12,13 stroke:#7a89c2,stroke-width:2px;\n\n    %% === Cross-section connections ===\n    Trainer --has--&gt; Runner\n    Trainer --has--&gt; Tracer\n    Trainer --has--&gt; Hooks\n    Trainer --uses--&gt; Agent\n    Algorithm -.injects.-&gt; Store\n    LLMProxy -.injects.-&gt; Store\n    Agent -.references.-&gt; Trainer\n    Runner -.references.-&gt; Trainer\n    Algorithm -.references.-&gt; Trainer\n    linkStyle 14,15,16 stroke:#839791,stroke-width:2px;\n    linkStyle 17,20,21,22 stroke:#7a89c2,stroke-width:2px;\n    linkStyle 18,19 stroke:#896978,stroke-width:2px;\n\n    style L fill:none;\n    style M fill:none;\n    style R fill:none;</code></pre>"},{"location":"deep-dive/birds-eye-view/#putting-it-all-together-a-reinforcement-learning-example-verl","title":"Putting It All Together: A Reinforcement Learning Example (VERL)","text":"<p>VERL shows how an algorithm consumes the shared infrastructure. For historical reasons, code lives in <code>agentlightning.algorithm.verl</code> and <code>agentlightning.verl</code>. The latter is legacy and reuses terms like <code>Trainer</code> in confusing ways. The former is a thin wrapper that conforms to the new algorithm interface. Future versions will merge the two.</p> <p>Reinforcement learning aims to learn a policy that takes actions in states to maximize expected reward. For agents, the policy is usually a language model. Inputs are prompts (state). Outputs are generated text (action). A numeric score judges quality (reward). The <code>(state, action, reward)</code> triplet is the basic learning unit.</p> <p>In Agent-lightning, the environment is implicit in the agent\u2019s workflow, which orchestrates one or more LLM calls and often self-judges using rules or additional model calls. During a rollout, the agent emits spans that contain everything needed for RL training, including LLM call traces and numeric judge/reward signals. The \"algorithm\", on the other hand, have more responsibilities.</p> <ol> <li>Providing a language model deployment that is currently learning and improving for the agent to interact with;</li> <li>Preparing the tasks that the agents will perform;</li> <li>Querying the spans generated, extracting triplets, and converting them into a format that the underlying RL library can consume;</li> <li>Updating the language model based on the learning signals.</li> </ol> <p>In the VERL integration, the algorithm launches a chat completion endpoint using <code>vLLM</code> and wraps training with <code>FSDP</code> for distributed optimization. It enqueues tasks from the dataset. After rollouts finish, it queries spans and converts them to triplets with <code>TracerTraceToTriplet</code>. VERL\u2019s native training loop then consumes these triplets to update model weights. The workflow can be summarized in the following diagram.</p> <pre><code>sequenceDiagram\n    autonumber\n    participant vLLM as vLLM Chat&lt;br&gt;Completion Endpoint\n    participant FSDP as FSDP / Megatron&lt;br&gt;Weights Optimizer\n    participant Algo as Algorithm&lt;br&gt;Main Controller&lt;br&gt;(Main Process)\n    participant Adapter as TracerTraceToTriplet\n    participant LLMProxy as LLM Proxy\n    participant Store as LightningStore\n    participant Runner as Runner + Agent\n\n    Note over Algo,LLMProxy: LLMProxy and Adapter are injected by Trainer as member\n    Note over vLLM,Algo: Algorithm creates and owns vLLM and FSDP\n\n    loop Over the Dataset in Batches\n        Algo-&gt;&gt;vLLM: Create Chat Completion Endpoint\n        activate vLLM\n        vLLM-&gt;&gt;LLMProxy: Registered as Backend Endpoint\n        LLMProxy-&gt;&gt;Store: Proxy URL added as Resource\n        par Over data samples in the batch\n            Algo--&gt;&gt;Store: enqueue_rollout\n            Store--&gt;&gt;Runner: Dequeue Rollout +&lt;br&gt;Resources (i.e., URL)\n            loop One Rollout Attempt\n                Runner--&gt;&gt;LLMProxy: LLM calls\n                LLMProxy--&gt;&gt;vLLM: Forwarded LLM calls\n                vLLM--&gt;&gt;LLMProxy: LLM responses\n                LLMProxy--&gt;&gt;Store: add_span / add_otel_span\n                LLMProxy--&gt;&gt;Runner: Forwarded LLM responses\n                Runner--&gt;&gt;Store: add_span / add_otel_span &lt;br&gt; (by tracer, including rewards)\n            end\n            Runner--&gt;&gt;Store: update_attempt(\"finished\", status)\n        end\n        Algo--&gt;&gt;Store: Poll for completed rollouts + spans\n        Algo-&gt;&gt;vLLM: Chat Completion Endpoint Sleeps\n        deactivate vLLM\n        Algo-&gt;&gt;Adapter: adapt(spans)\n        Adapter-&gt;&gt;FSDP: Triplets (state, action, reward)\n        activate FSDP\n        FSDP--&gt;&gt;Algo: Updated LLM weights\n        deactivate FSDP\n    end</code></pre> <p>Notes:</p> <ol> <li> <p>There are interactions between different components injected into or owned by algorithms in the diagram, such as the output of the adapter feeding into the FSDP optimizer. This is for simplicity of illustration and slightly different from the actual implementation, where it's the algorithm main controller that orchestrates the data flow between components.</p> </li> <li> <p>On mapping to VERL. VERL uses a classic RLHF setup where each action is a single token, the state is the full conversation history up to that token, and reward is given at the end. This is very different from our setup where each action is actually  a chunk of text, although they are both called RL! Therefore, after the adapter produces triplets, the algorithm converts each <code>(state, action, reward)</code> into a VERL trajectory (<code>DataProto</code>) with keys like <code>input_ids</code>, <code>position_ids</code>, <code>attention_mask</code>, and <code>token_level_scores</code>. That conversion happens after triplet generation and is not shown in the diagram.</p> </li> </ol>"},{"location":"deep-dive/birds-eye-view/#execution-strategies-and-parallelism","title":"Execution Strategies and Parallelism","text":"<p>Readers might have observed from the diagram above that there is absolutely no communication between (1) runner and agents and (2) algorithm. The only overlap of them is the Trainer and LightningStore. This observation is very clear with the diagram within the trainer section. This design allows us to flexibly scale the runner and algorithm independently, which is crucial for large-scale training.</p> <p>Agent-lightning packages two executable bundles: a runner bundle (Runner, Tracer, Hook, LitAgent) and an algorithm bundle (Algorithm, Adapter, LLM Proxy). Both share the LightningStore. The trainer initializes and connects the bundles.</p> <pre><code>graph TD\n    subgraph Runner_Side[\"Runner Bundle\"]\n        direction LR\n        R[Runner] --- T[Tracer] --- H[Hooks] --- A1[Agent]\n    end\n\n    subgraph Algorithm_Side[\"Algorithm Bundle\"]\n        direction LR\n        ALG[Algorithm] --- AD[Adapter] --- LLM[LLM Proxy]\n    end\n\n    S[(Store)]\n    TR[Trainer]\n\n    Runner_Side &lt;--&gt; S\n    Algorithm_Side &lt;--&gt; S\n    TR --&gt; Runner_Side\n    TR --&gt; Algorithm_Side\n\n    linkStyle 0,1,2,3,4 opacity:0;</code></pre> <p>An execution strategy, defined and owned by the trainer, governs how algorithm and runner bundles are placed, connected, scaled, and aborted. It serves four primary purposes.</p> <p>Execution strategies first determine bundle placement \u2014 whether the two bundles run in the same thread, process, machine, or across separate machines. They also define store management, wrapping the store and specifying how data is shared between bundles.</p> <p>In terms of scalability, the strategy can replicate the runner bundle across multiple threads, processes, or machines to expand throughput on the runner side. The algorithm side remains single-process due to the complexity of parallelization. Mature frameworks such as DeepSpeed and Megatron already support distributed model training, so scaling of the algorithm bundle is delegated to those implementations.</p> <p>Abort handling is another core responsibility. Aborts may be triggered by normal exits, failures in either bundle, or user interrupts. The trainer must include cancellation interfaces for the bundles so that bundles can be cleanly aborted. When the algorithm bundle exits normally, the strategy signals the runner bundle to terminate. If the runner exits first, no signal is sent to the algorithm, as it may still be processing completed rollouts. In cases of failure or user interruption, the strategy signals both bundles to abort; if a bundle fails to respond, the strategy should attempt a forceful termination.</p> <p>Agent-lightning currently provides two execution strategies: shared-memory and client-server, described in the following sections.</p>"},{"location":"deep-dive/birds-eye-view/#shared-memory-strategy","title":"Shared-memory Strategy","text":"<p><code>SharedMemoryExecutionStrategy</code> runs algorithm and runner bundles as threads in one process. The strategy wraps the store with <code>LightningStoreThreaded</code>, which guards calls with a lock for safe concurrency.</p> <p>This is good for lightweight debugging because components share one Python heap and avoid serialization. It is not suitable for heavy RL training or compute-intensive agents.</p> <pre><code>flowchart TB\n    subgraph MainProcess\n        direction TB\n        subgraph AlgorithmThread [Thread 0]\n            Algorithm[Algorithm bundle]\n        end\n        subgraph RunnerThread1 [Thread 1]\n            Runner1[Runner bundle #1]\n        end\n        subgraph RunnerThread2 [Thread 2]\n            Runner2[Runner bundle #2]\n        end\n        subgraph RunnerThread3 [Thread 3]\n            RunnerN[Runner bundle #N]\n        end\n        LightningStoreFacade[LightningStoreThreaded]\n        BaseStore[Underlying LightningStore]\n    end\n    Algorithm -- async calls --&gt; LightningStoreFacade\n    Runner1 -- async calls --&gt; LightningStoreFacade\n    Runner2 -- async calls --&gt; LightningStoreFacade\n    RunnerN -- async calls --&gt; LightningStoreFacade\n    LightningStoreFacade --&gt;|thread-safe delegates| BaseStore</code></pre> <p>You can configure which role runs on the main thread. If the main thread runs the algorithm, it is able to spawn multiple runner threads. If it runs a runner, <code>n_runners</code> must be 1 and the runner lives on the main thread.</p>"},{"location":"deep-dive/birds-eye-view/#client-server-strategy","title":"Client-server Strategy","text":"<p><code>ClientServerExecutionStrategy</code> splits concerns across processes. The algorithm bundle starts a <code>LightningStoreServer</code> (HTTP API) that wraps the underlying store. Runners connect via <code>LightningStoreClient</code> to call the same interface over REST. The server embeds a client to support algorithm-launched subprocesses (e.g., an LLM proxy worker) that need to talk back to the algorithm\u2019s process through the same API.</p> <p>Currently this design introduces an extra wrapper in the Server side (as shown in the diagram), which helps debugging and improves fault tolerance. We might revisit this design in the future and enforce the client to be the only way to communicate with the store.</p> <pre><code>flowchart TD\n    subgraph Algorithm Process Group\n        subgraph StoreServer[LightningStoreServer]\n            StoreHttpClient[HTTP Client]\n            StoreHttpServer[HTTP Server]\n            StoreWrapper[LightningStore Wrapper]\n            StoreHttpClient -- HTTP --&gt; StoreHttpServer\n        end\n        subgraph Algorithm Bundle\n            Algorithm[Algorithm Main Process]\n            subgraph Another subprocess\n                LLMProxy[LLM Proxy]\n            end\n        end\n        LLMProxy -- async calls --&gt; StoreHttpClient\n        Algorithm -- async calls --&gt; StoreWrapper\n    end\n    subgraph RunnerSide [\"Runner Side\"]\n        subgraph Runner Process 1\n            Runner1[Runner bundle #1]\n            Runner1 -- async calls --&gt; LightningStoreClient1\n            LightningStoreClient1[LightningStoreClient]\n        end\n        subgraph Runner Process 2\n            Runner2[Runner bundle #2]\n            Runner2 -- async calls --&gt; LightningStoreClient2\n            LightningStoreClient2[LightningStoreClient]\n        end\n        subgraph Runner Process N\n            RunnerN[Runner bundle #N]\n            RunnerN -- async calls --&gt; LightningStoreClientN\n            LightningStoreClientN[LightningStoreClient]\n        end\n    end\n    LocalStore[Underlying LightningStore]\n    StoreHttpServer --&gt;|delegates| StoreWrapper\n    StoreWrapper --&gt;|delegates| LocalStore\n    LightningStoreClient1 -- HTTP --&gt; StoreHttpServer\n    LightningStoreClient2 -- HTTP --&gt; StoreHttpServer\n    LightningStoreClientN -- HTTP --&gt; StoreHttpServer\n\n    style RunnerSide fill:none;</code></pre>"},{"location":"deep-dive/birds-eye-view/#onlinecontinuous-learning","title":"Online/Continuous Learning","text":"<p>Continuous learning keeps the algorithm loop running while runners report tasks and spans opportunistically. Key differences from batch mode:</p> <ol> <li>The algorithm does not enqueue rollouts from a fixed dataset. Runners report tasks/rollouts and spans spontaneously.</li> <li>The algorithm can wait for rollouts with a expected set of rollout IDs, but more often polls for new rollouts and spans or waits for a count to arrive.</li> <li>The <code>Runner</code> processes one rollout at a time via <code>step(task)</code> instead of exhausting a task queue. It notifies the store when starting a rollout so the store records it.</li> <li>A user or higher-level loop controls which resources the next step uses and when to retry.</li> </ol> <p>Spans, Adapter implementations, and the LLM Proxy work the same way.</p> <pre><code>sequenceDiagram\n    autonumber\n    actor User\n    participant Runner\n    participant Agent\n    participant Store as LightningStore\n    participant Algorithm\n\n    Note over Algorithm: Algorithm is long-running and loops continuously\n\n    loop Continuous Learning Loop\n        activate User\n        opt Decide what to do next\n            User--&gt;&gt;Store: get_resources_by_id\n            Store--&gt;&gt;User: Resources\n            User--&gt;&gt;User: Prepare input for next step\n        end\n        User-&gt;&gt;Runner: step(input, resources)\n        activate Runner\n        Runner--&gt;&gt;Store: Notify: start_rollout(input)\n        Runner-&gt;&gt;Agent: rollout(input, resources)\n        Agent--&gt;&gt;Runner: add_span / reward spans\n        Runner--&gt;&gt;Store: add_span or add_otel_span\n        Runner--&gt;&gt;Store: update_attempt(status=\"finished\")\n        deactivate Runner\n        deactivate User\n        Algorithm-&gt;&gt;Store: poll for new rollouts and spans\n        opt If there is enough new data\n            Store--&gt;&gt;Algorithm: new spans\n            Algorithm-&gt;&gt;Algorithm: adapt spans \u2192 learning signal\n            Algorithm-&gt;&gt;Store: update_resources\n        end\n    end</code></pre>"},{"location":"deep-dive/serving-llm/","title":"Serving LLMs under Agent-lightning","text":"<p>Agent-lightning focuses on data, learning signals, and control flow \u2014 not on running model inference. This deep dive explains how to serve a model alongside Agent-lightning so runners can call it reliably, how the LLM Proxy fits into the loop, and why token IDs matter if you care about correctness in training and evaluation.</p>"},{"location":"deep-dive/serving-llm/#general-background-on-llm-serving","title":"General background on LLM serving","text":"<p>Serving a model is essential if you want to train it, especially when you use the model\u2019s own generations as training data. We\u2019ll briefly review the general background to ensure all readers are aligned.</p> <p>Modern LLM servers solve a difficult scheduling problem: keeping GPUs fully utilized while handling prompts of different lengths, streaming tokens as they arrive, and fitting large KV caches into limited memory. Techniques like continuous batching and paged attention address these challenges. Continuous batching interleaves decoding across requests to reuse weights efficiently; with careful memory planning, it achieves major throughput gains without increasing latency. PagedAttention reduces KV-cache fragmentation so batching remains effective as sequences grow. See vLLM\u2019s PagedAttention paper and industry analyses for details. Balancing inference correctness and efficiency is difficult \u2014 a recent blog from Thinking Machines Labs highlights how inference nondeterminism ultimately affects training.</p> <p>Beyond scheduling, servers expose an HTTP API, often OpenAI-compatible (<code>/v1/chat/completions</code> and <code>/v1/responses</code>), which is itself a complex stack. In addition to text prompts and chat messages, the API defines many parameters and response fields such as tool calls, structured output, and multimodal support. Much effort has been put into implementing all these parameters for many frameworks. Popular engines like vLLM and SGLang ship with OpenAI-compatible frontends so you can reuse existing client code. Ollama and llama.cpp provide similar capabilities. However, because models differ internally, each framework interprets and implements the API slightly differently. Even with identical requests, the tokens passed to the model can vary substantially across frameworks.</p>"},{"location":"deep-dive/serving-llm/#what-agent-lightning-expects-from-a-served-llm","title":"What Agent-lightning expects from a served LLM","text":"<p>Most of the issues above either have workarounds or remain open research problems. Keep them in mind, but the key question is: what does Agent-lightning expect from a served LLM? The answer includes at least two things:</p> <ul> <li>An OpenAI-compatible Chat Completions or Responses endpoint the agent can call during rollouts.</li> <li>Optional training and debugging signals: logprobs, usage, and ideally token IDs. (OpenAI\u2019s public API exposes usage and logprobs, but not token IDs \u2014 more on why IDs matter later.)</li> </ul>"},{"location":"deep-dive/serving-llm/#launching-a-serving-framework","title":"Launching a serving framework","text":"<p>For many algorithms, you\u2019ll start an engine (e.g., vLLM or SGLang) before rollouts, then shut it down afterward to free GPU memory. Most frameworks provide a one-line \u201cserve\u201d command to launch the OpenAI-compatible server. You can use those to bring up <code>/v1/chat/completions</code> with your checkpoint, ensuring streaming and any required tool-calling features are enabled. A working example is shown in Unsloth SFT.</p> <p>Weight updates \u2014 which occur after each training step \u2014 are trickier. Some frameworks like vLLM support hot-updating model weights, but it\u2019s usually simpler and more reliable to restart the engine to load new weights. For medium-sized tasks (hundreds of rollouts taking 10+ minutes), the restart overhead (under 30 seconds) is typically negligible.</p> <p>If you\u2019re using Agent-lightning\u2019s VERL integration, the algorithm can manage the server automatically. The VERL framework intelligently allocates compute resources and wraps vLLM/SGLang behind an <code>AsyncLLMServer</code> abstraction. You can directly use this as the LLM endpoint for agents. Since VERL can spawn multiple vLLM replicas, using <code>LLMProxy</code> to manage them adds an additional safety layer.</p> <p>A full sequence diagram of how VERL interacts with the LLM server and proxy is available here.</p>"},{"location":"deep-dive/serving-llm/#llm-proxy","title":"LLM Proxy","text":"<p>The LLM Proxy is a utility class in Agent-lightning, built on LiteLLM, that sits between runners and your backend engine(s) or server(s). In Agent-lightning it acts as a single URL registered as a <code>Resource</code> in the store, offering three key benefits:</p> <ol> <li>Unified endpoint &amp; hot-swaps. You can redirect traffic between OpenAI, Anthropic, local vLLM/SGLang, or canary checkpoints without modifying agent code \u2014 simply repoint the proxy.</li> <li>First-class tracing. The proxy emits OpenTelemetry spans for every call and sends them to the <code>LightningStore</code>. It includes rollout and attempt identifiers in request headers so spans are correctly attributed. Sequence numbers are allocated monotonically via the store to prevent clock-skew issues and allow reliable reconstruction of execution trees.</li> <li>Token IDs. The proxy can return prompt and response token IDs along with the model output. More details are available in the next section.</li> </ol> <p>Operationally, running the proxy alongside the algorithm works best: the algorithm registers the backend (e.g., the vLLM URL) via <code>LLMProxy.update_model_list</code>, publishes the proxy URL as a resource via <code>LightningStore.add_resources</code>, and runners simply use that URL during rollouts. This mirrors many production client\u2013server setups.</p>"},{"location":"deep-dive/serving-llm/#token-ids-and-why-they-matter","title":"Token IDs and why they matter","text":"<p>This section explains how Agent-lightning handles and uses token IDs \u2014 a subtle but important detail for training stability and accuracy.</p> <p>Most agents interact with LLMs via Chat Completion APIs, exchanging chat messages. There are two main approaches to collecting training data from such agents.</p> <p>Note</p> <p>Tokenization here refers to the process of converting Chat Messages into Token IDs. Detokenization is the reverse process of converting Token IDs back to Chat Messages. Normally, the tokenizer is published along with the pretrained model, which includes a vocabulary, special tokens, and a chat template to dealing with chat messages.</p> <p>1. Retokenizing chat messages. In this approach, you store chat messages as text and let training algorithms retokenize them later, as done in many SFT workflows (e.g., HuggingFace SFT). In practice, we\u2019ve found this method unstable and less accurate. The chart below compares training results. The retokenization approach is run twice. All settings are the same except for the retokenization approach.</p> <p>This instability has three causes. Firstly, chat template used in different frameworks could be slightly different. For example, one single LLaMA model can work with multiple chat templates (multiple in vLLM and one in HuggingFace). It's possible that the chat template used in detokenization is different from the one used in tokenization (this is actually an implementation bug).</p> <p>Secondly, a word might be generated as two tokens (e.g., <code>H + AVING</code>) but later retokenized as <code>HAV + ING</code>. The text looks identical, but the token IDs differ from what the model originally produced.</p> <p>Thirdly, a generated tool call text like <code>&lt;tool_call&gt;{ \"name\": ... }&lt;/tool_call&gt;</code> is parsed by tool call parser into an object that is required by chat completion API. Later, the object is rendered back to <code>&lt;tool_call&gt;{ \"name\": ... }&lt;/tool_call&gt;</code> and retokenized again, tool call parsing and re-rendering might cause changes in whitespace and formatting. In some situations, JSON errors may even be auto-corrected by the tool call parser \u2014 masking the model\u2019s true generation errors and preventing them from being trained away.</p> <p>2. Saving token IDs directly. The alternative is to save the token IDs generated by the model, as done in RL setups like Tinker. This requires a training pipeline that treats tokens as first-class entities, meaning agents must communicate with the inference engine at the token level.</p> <p>However, most agents \u2014 especially those built with frameworks like LangChain \u2014 rely on OpenAI-compatible APIs and can\u2019t tokenize or detokenize themselves. As mentioned earlier, implementing this layer manually is complex and error-prone. Some frameworks implement custom solutions (e.g., VERL Agent Loop, Tinker Renderer), while others leave it to users (e.g., SkyRL Search-R1).</p> <p>A better solution is to use an OpenAI-compatible API that returns token IDs directly. This lets agents continue using familiar APIs while capturing token IDs via tracing for training. The limitation, of course, is that the serving framework must actually support this capability.</p> <p>When Agent-lightning was first released, we implemented an instrumented vLLM server that monkey-patched vLLM\u2019s OpenAI server to return token IDs. Since then, the Agent-lightning and vLLM teams have collaborated to add this feature directly to vLLM core. Starting with vLLM v0.10.2, the OpenAI-compatible API includes a <code>return_token_ids</code> parameter, allowing token IDs to be requested alongside chat messages. SGLang has tracked similar feature requests, though its OpenAI-compatible layer doesn\u2019t yet support them.</p> <p>In short, when using vLLM v0.10.2 or newer, <code>LLMProxy</code> automatically adds <code>return_token_ids</code> to each request so the engine includes token IDs in its response. For older vLLM versions, you still need the instrumented version (via <code>agl vllm</code> CLI command).</p> <p>Finally, if you only save token IDs in spans, it will have its own limitations \u2014 if you train one model using spans from another model with a different tokenizer, incompatibilities can arise. In practice, though, spans in Agent-lightning always store both chat messages and token IDs (actually the full request and response objects), allowing you to fall back to retokenization when necessary.</p>"},{"location":"deep-dive/store/","title":"Understanding Store","text":"<p>The <code>LightningStore</code> is the central coordination point for Agent-lightning. It holds the task queue, rollouts, attempts, spans, and versioned resources, and exposes a small API both Runners and Algorithms use to communicate. This document explains what's in the store, how statuses transition, how spans are recorded, and the concurrency model (threads &amp; processes).</p>"},{"location":"deep-dive/store/#whats-in-the-store","title":"What's in the Store?","text":"<p>At a high level:</p> <ul> <li>Task Queue \u2013 <code>enqueue_rollout</code> adds work; workers poll with <code>dequeue_rollout</code>. When a rollout is dequeued, it automatically creates a new attempt associated with itself.</li> <li>Rollouts \u2013 A rollout is one unit of work. It has input, metadata, links to resources, and a lifecycle (<code>queuing \u2192 preparing \u2192 running \u2192 ...</code>). Valid RolloutStatus are <code>queuing</code>, <code>preparing</code>, <code>running</code>, <code>succeeded</code>, <code>failed</code>, <code>requeuing</code>, <code>cancelled</code>. For algorithms and runners, the rollout can be seen as a whole, without worrying about the internal attempts.</li> <li>Attempts \u2013 Each rollout can have multiple executions (retries). Attempts track <code>status</code>, <code>start_time</code>, <code>end_time</code>, <code>last_heartbeat_time</code> and link to spans. Valid AttemptStatus are <code>preparing</code>, <code>running</code>, <code>succeeded</code>, <code>failed</code>, <code>requeuing</code>, <code>cancelled</code>.</li> <li>Spans \u2013 Structured trace events produced by the Tracer during an attempt. Spans are ordered by a monotonic sequence id per <code>(rollout_id, attempt_id)</code>.</li> <li>Resources \u2013 Versioned, named bundles (e.g., prompt templates) referenced by rollouts.</li> <li>Workers \u2013 Metadata about runner instances: heartbeat timestamps, current assignment, and status.</li> </ul> <p>Rollout and Task share the same surface in practice: <code>Rollout.input</code> is the task input. The queue stores rollouts that are not yet running; Runners dequeue them and update the same rollout's status as work progresses.</p> <p>Before we look at status transitions, it helps to keep in mind that rollouts are the \"outside view,\" while attempts are the \"inside view.\" Attempts are what actually run; rollouts summarize the latest attempt plus a small set of control actions like queueing and cancellation.</p>"},{"location":"deep-dive/store/#attempt-status-transitions","title":"Attempt Status Transitions","text":"<p>The status model is intentionally small and operationally clear.</p> <pre><code>stateDiagram-v2\ndirection LR\n\n[*] --&gt; preparing: &lt;b&gt;Runner calls&lt;/b&gt; dequeue_rollout()&lt;br&gt;or start_rollout()&lt;br&gt;or start_attempt()\npreparing --&gt; running: &lt;b&gt;Runner calls&lt;/b&gt;&lt;br&gt;add_[otel_]span()&lt;br&gt;for the first time\n\nstate c_runner &lt;&lt;choice&gt;&gt;\nstate c_watch  &lt;&lt;choice&gt;&gt;\n\npreparing --&gt; c_runner: &lt;b&gt;Runner calls&lt;/b&gt;&lt;br&gt;update_attempt(...)&lt;/b&gt;\nrunning --&gt; c_runner: &lt;b&gt;Runner calls&lt;/b&gt;&lt;br&gt;update_attempt(...)\nrunning --&gt; c_watch: &lt;b&gt;Watchdog checks&lt;/b&gt;\npreparing --&gt; c_watch: &lt;b&gt;Watchdog checks&lt;/b&gt;\n\nstate \"Client-set outcome\" as Client {\n  direction TB\n  succeeded\n  failed\n}\nstate \"Watchdog / policy\" as Watch {\n  direction TB\n  timeout\n  unresponsive\n}\n\nc_runner --&gt; succeeded: update_attempt(status=succeeded)\nc_runner --&gt; failed: update_attempt(status=failed)\n\nc_watch --&gt; timeout: now - start_time &gt; timeout_seconds\nc_watch --&gt; unresponsive: now - last_heartbeat &gt; unresponsive_seconds\n\nunresponsive --&gt; running: &lt;b&gt;Runner calls&lt;/b&gt;&lt;br&gt;add_[otel_]span()</code></pre> <p>Each attempt begins in preparing, created either when a rollout is dequeued or explicitly started. It transitions to running the first time a span is recorded. From there, a few clear rules govern how it can change:</p> <ul> <li>When the runner explicitly marks completion, the attempt becomes succeeded or failed (when the runner catches exception thrown out by the agent).</li> <li>When the watchdog detects that the total elapsed time since start exceeds the configured limit, it marks the attempt as timeout.</li> <li>If heartbeats stop arriving for too long, the watchdog marks it unresponsive.</li> <li>A new span from the runner can immediately revive an unresponsive attempt back to running.</li> </ul> <p>What's a Watchdog?</p> <p>The watchdog enforces timing and liveness rules defined by each rollout\u2019s <code>RolloutConfig</code>. It\u2019s not a separate thread or service, but a function periodically invoked (e.g., before store mutations) to keep attempts healthy and consistent.</p> <p>This simple model allows the system to distinguish between normal termination, abnormal stalling, and recoverable interruption without additional state flags.</p>"},{"location":"deep-dive/store/#worker-telemetry","title":"Worker Telemetry","text":"<p>Workers track runner-level activity timestamps (<code>last_heartbeat_time</code>, <code>last_dequeue_time</code>, <code>last_busy_time</code>, <code>last_idle_time</code>) plus their current rollout assignment. Those fields are now derived automatically:</p> <ul> <li><code>dequeue_rollout(worker_id=...)</code> records which worker polled the queue and refreshes <code>last_dequeue_time</code>.</li> <li><code>update_attempt(..., worker_id=...)</code> drives the worker status machine. Assigning an attempt marks the worker busy and stamps <code>last_busy_time</code>; finishing with <code>status in {\"succeeded\",\"failed\"}</code> switches to idle, while watchdog transitions such as <code>timeout</code>/<code>unresponsive</code> make the worker unknown and clear <code>current_rollout_id</code> / <code>current_attempt_id</code>.</li> <li><code>update_worker(...)</code> is reserved for heartbeats. It snapshots optional <code>heartbeat_stats</code> and always updates <code>last_heartbeat_time</code>.</li> </ul> <p>Because every transition flows through these APIs, worker status is derived automatically from rollout execution and heartbeats. Note, however, that calling <code>update_worker</code> with a new <code>worker_id</code> will create a new worker record with status \"unknown\" if one does not exist. Thus, while manual status changes are not allowed, new worker records can be created externally via heartbeats.</p>"},{"location":"deep-dive/store/#rollout-transition-map","title":"Rollout Transition Map","text":"<p>Rollout status is an aggregated view of its latest attempt\u2019s status, with additional transitions for queueing and explicit cancellation.</p> <p>A rollout\u2019s retry behavior is controlled by <code>Rollout.config</code> (a <code>RolloutConfig</code>). The key fields are:</p> <ul> <li><code>timeout_seconds</code> \u2013 maximum wall-clock time for an attempt before it is marked <code>timeout</code>.</li> <li><code>unresponsive_seconds</code> \u2013 maximum silence between heartbeats before an attempt is marked <code>unresponsive</code>.</li> <li><code>max_attempts</code> \u2013 total number of attempts allowed for the rollout (including the first).</li> <li><code>retry_condition</code> \u2013 which attempt terminal statuses should trigger a retry (e.g., <code>[\"failed\", \"timeout\", \"unresponsive\"]</code>).</li> </ul> <p>How it plays out: The runner works on attempt <code>k</code>. If the attempt ends in a status that is listed in <code>retry_condition</code>, and <code>k &lt; max_attempts</code>, the rollout moves to requeuing and the store creates attempt <code>k+1</code>. Otherwise, the rollout becomes failed (or succeeded if the runner marked it so). <code>timeout_seconds</code> and <code>unresponsive_seconds</code> are enforced by the watchdog and feed into the same decision flow.</p> <p>A minimal example of how to use <code>RolloutConfig</code>:</p> <pre><code>from agentlightning import RolloutConfig\n\n# Retry on explicit failures or timeouts, up to 3 attempts in total.\ncfg = RolloutConfig(\n    timeout_seconds=600,\n    unresponsive_seconds=120,\n    max_attempts=3,\n    retry_condition=[\"failed\", \"timeout\"]\n)\n\n# When creating/enqueuing a rollout, attach this config.\n# The store will propagate attempt outcomes according to cfg.\nrollout = await store.enqueue_rollout(input, config=cfg)\n</code></pre> Latest attempt status Rollout transition Notes / guards N/A <code>queuing</code> Created by <code>enqueue_rollout()</code>. <code>preparing</code> <code>queuing/requeuing</code> \u2192 <code>preparing</code> Typically <code>dequeue_rollout()</code> or <code>start_rollout()</code>/<code>start_attempt()</code> creates a new attempt. <code>running</code> <code>preparing/queuing/requeuing</code> \u2192 <code>running</code> First <code>add_[otel_]span()</code> flips the attempt to <code>running</code>; rollout follows via <code>rollout_status_from_attempt</code>. <code>succeeded</code> <code>*</code> \u2192 <code>succeeded</code> Terminal. Rollout <code>end_time</code> set. <code>failed</code> / <code>timeout</code> / <code>unresponsive</code> <code>*</code> \u2192 <code>requeuing</code> Only if <code>status \u2208 retry_condition \u2227 sequence_id &lt; max_attempts</code>. <code>failed</code> / <code>timeout</code> / <code>unresponsive</code> <code>*</code> \u2192 <code>failed</code> Otherwise (no retries left or retries disabled). <code>*</code> <code>*</code> \u2192 <code>cancelled</code> Explicitly set by <code>update_rollout(status=cancelled)</code>. <p>Why aggregation?</p> <p>In code, we use <code>rollout_status_from_attempt()</code> which actively updates the rollout based on the latest attempt. Reading the table above is usually easier than reverse-engineering the propagation logic in the code: think of the rollout\u2019s transitions as callbacks on attempt state changes, plus queue/cancel paths.</p>"},{"location":"deep-dive/store/#spans","title":"Spans","text":"<p>Every traceable operation in a rollout is stored as a Span. Spans not only capture fine-grained instrumentation but also act as periodic heartbeats that demonstrate liveness. The first span marks activation; each subsequent one both extends the trace and refreshes the attempt\u2019s <code>last_heartbeat_time</code>. If no span arrives within the configured <code>unresponsive_seconds</code>, the watchdog downgrades the attempt to unresponsive until activity resumes.</p> <p>Spans are indexed by <code>(rollout_id, attempt_id, sequence_id)</code> where the sequence is monotonic. Tracing analysis tools like Adapter usually rely on \"time order\" to reconstruct the trace. However, in a distributed system, the recorded start time and end time of a span are not necessarily in the right order when they aggregated into a central store. Therefore, we enforce every span creation to retrieve a monotonically increasing <code>sequence_id</code> from the store before adding the span.</p> <p>Note</p> <p>In practice, one <code>sequence_id</code> can be used to create multiple spans. In that case, the orders between the multiple spans are determined by the order of <code>start_time</code> and <code>end_time</code> of the spans.</p>"},{"location":"deep-dive/store/#opentelemetry-conversion","title":"OpenTelemetry conversion","text":"<p>Runners often produce OpenTelemetry <code>ReadableSpan</code> objects directly. The store normalizes them into <code>Span</code> as follows:</p> <ol> <li>The runner first requests <code>get_next_span_sequence_id</code> via <code>sequence_id = await store.get_next_span_sequence_id(rollout_id, attempt_id)</code>. This guarantees ordering within the attempt regardless of clock skew.</li> <li><code>trace_id</code>, <code>span_id</code>, <code>parent_id</code>, <code>name</code>, <code>status</code>, timestamps, attributes, events, links, and resource are copied from the OTEL span. Timestamps are auto-normalized to seconds (nanoseconds are converted).</li> <li>OTEL <code>SpanContext</code> and parent context are preserved so downstream tools can correlate traces across systems.</li> <li>Any additional serializable fields present on the <code>ReadableSpan</code> are retained in the stored span (after safe JSON serialization), which keeps the representation forward-compatible.</li> </ol> <p>Programmatically this is encapsulated by <code>Span.from_opentelemetry(readable_span, rollout_id, attempt_id, sequence_id)</code>; <code>store.add_otel_span(...)</code> simply wraps the fetch-then-add flow. The end result is a store span that is stable to sort, merge, and query, while still preserving the richness of the original OTEL payload.</p> <p>Tip</p> <p><code>add_span</code> or <code>add_otel_span</code> both appends a span and acts as a heartbeat that can revive <code>unresponsive</code> \u2192 <code>running</code>.</p>"},{"location":"deep-dive/store/#otlp-compatibility","title":"OTLP Compatibility","text":"<p>Some of the LightningStore implementations support exporting traces via the OTLP/HTTP specification. For example, <code>LightningStoreServer</code> exposes <code>/v1/traces</code> endpoint, it implements the binary Protobuf variant defined by the spec, including the required <code>Content-Type: application/x-protobuf</code>, optional <code>Content-Encoding: gzip</code>, and status responses encoded as <code>google.rpc.Status</code>. Agent-lightning helps parsing <code>ExportTraceServiceRequest</code> messages, validate identifiers, normalize resource metadata, and allocate sequence numbers so store implementations only need to persist <code>Span</code> objects in order.</p> <p>Because the interface speaks standard OTLP, any OpenTelemetry-compatible SDK or collector can emit spans directly to a LightningStore OTLP endpoint without custom shims. The server responds according to the OTLP contract (status code, encoding, and error payloads), which keeps Agent-lightning interoperable with existing observability tooling. This compatibility serves as a strong complement to the OpenTelemetry conversion discussed above.</p> <p>Check whether the store supports OTLP traces via the <code>capabilities[\"otlp_traces\"]</code> property.</p>"},{"location":"deep-dive/store/#implementation-overview","title":"Implementation Overview","text":"<p>The <code>agentlightning.store</code> module is organized into two distinct layers plus optional wrappers:</p> <pre><code>classDiagram\n    direction TB\n\n    class LightningStore {\n        &lt;&lt;abstract&gt;&gt;\n        +enqueue_rollout()\n        +dequeue_rollout()\n        +update_attempt()\n        +add_span()\n        +query_rollouts()\n        ...\n    }\n\n    class LightningCollections {\n        &lt;&lt;abstract&gt;&gt;\n        +rollouts: Collection\n        +attempts: Collection\n        +spans: Collection\n        +resources: Collection\n        +workers: Collection\n        +rollout_queue: Queue\n        +span_sequence_ids: KeyValue\n        +atomic()\n    }\n\n    class CollectionBasedLightningStore~T~ {\n        +collections: T\n        -healthcheck_before()\n        -tracked()\n    }\n\n    class InMemoryLightningStore\n    class MongoLightningStore\n    class InMemoryLightningCollections\n    class MongoLightningCollections\n\n    class LightningStoreServer {\n        +store: LightningStore\n        +start()\n        +stop()\n    }\n    class LightningStoreClient {\n        +server_address: str\n    }\n    class LightningStoreThreaded {\n        +store: LightningStore\n    }\n\n    LightningStore &lt;|-- CollectionBasedLightningStore\n    LightningStore &lt;|-- LightningStoreServer\n    LightningStore &lt;|-- LightningStoreClient\n    LightningStore &lt;|-- LightningStoreThreaded\n\n    CollectionBasedLightningStore &lt;|-- InMemoryLightningStore\n    CollectionBasedLightningStore &lt;|-- MongoLightningStore\n\n    LightningCollections &lt;|-- InMemoryLightningCollections\n    LightningCollections &lt;|-- MongoLightningCollections\n\n    InMemoryLightningStore ..&gt; InMemoryLightningCollections : uses\n    MongoLightningStore ..&gt; MongoLightningCollections : uses\n\n    LightningStoreServer o-- LightningStore : wraps\n    LightningStoreThreaded o-- LightningStore : wraps</code></pre> <ol> <li> <p>Collections Layer \u2013 Low-level storage primitives (<code>LightningCollections</code>) providing CRUD operations via <code>Collection</code>, <code>Queue</code>, and <code>KeyValue</code> interfaces. Each backend (in-memory, MongoDB) implements these primitives.</p> </li> <li> <p>Store Layer \u2013 All <code>LightningStore</code> implementations must inherit from <code>LightningStore</code> and override the methods to implement the storage logic. <code>CollectionBasedLightningStore</code> builds on collections to implement the full <code>LightningStore</code> API, including business logic like status transitions, watchdog health checks, and retry policies.</p> </li> <li> <p>Wrappers \u2013 Cross-cutting concerns live in thin wrappers:</p> <ul> <li><code>LightningStoreThreaded</code> adds mutex-based thread safety.</li> <li><code>LightningStoreServer</code> / <code>LightningStoreClient</code> enable multi-process access over HTTP.</li> </ul> </li> </ol>"},{"location":"deep-dive/store/#collections","title":"Collections","text":"<p>The collections layer provides storage primitives that <code>CollectionBasedLightningStore</code> builds upon. This separation keeps business logic (status transitions, watchdog, retries) in the store layer while allowing different backends to focus purely on persistence.</p> <p>The off-the-shelf implementations are <code>InMemoryLightningCollections</code> and <code>MongoLightningCollections</code>, which are the underlying collections for <code>InMemoryLightningStore</code> and <code>MongoLightningStore</code>, respectively.</p>"},{"location":"deep-dive/store/#collection-primitives","title":"Collection Primitives","text":"<p><code>LightningCollections</code> bundles three primitive types:</p> Primitive Purpose Methods <code>Collection[T]</code> Indexed storage with primary keys <code>query()</code>, <code>get()</code>, <code>insert()</code>, <code>update()</code>, <code>upsert()</code>, <code>delete()</code> <code>Queue[T]</code> FIFO queue for task scheduling <code>enqueue()</code>, <code>dequeue()</code>, <code>peek()</code>, <code>size()</code> <code>KeyValue[K, V]</code> Simple key-value store <code>get()</code>, <code>set()</code>, <code>inc()</code>, <code>chmax()</code>, <code>pop()</code> <p>Every <code>LightningCollections</code> instance exposes these named collections:</p> <ul> <li><code>rollouts</code> \u2013 <code>Collection[Rollout]</code> keyed by <code>rollout_id</code></li> <li><code>attempts</code> \u2013 <code>Collection[Attempt]</code> keyed by <code>(rollout_id, attempt_id)</code></li> <li><code>spans</code> \u2013 <code>Collection[Span]</code> keyed by <code>(rollout_id, attempt_id, span_id)</code></li> <li><code>resources</code> \u2013 <code>Collection[ResourcesUpdate]</code> keyed by <code>resources_id</code></li> <li><code>workers</code> \u2013 <code>Collection[Worker]</code> keyed by <code>worker_id</code></li> <li><code>rollout_queue</code> \u2013 <code>Queue[str]</code> holding rollout IDs awaiting execution</li> <li><code>span_sequence_ids</code> \u2013 <code>KeyValue[str, int]</code> tracking monotonic sequence counters</li> </ul>"},{"location":"deep-dive/store/#atomic-operations","title":"Atomic Operations","text":"<p>Collections support atomic operations through the <code>atomic()</code> context manager:</p> <pre><code>async with collections.atomic(mode=\"rw\", labels=[\"rollouts\", \"attempts\"]) as ctx:\n    rollout = await ctx.rollouts.get(filter={\"rollout_id\": {\"exact\": rollout_id}})\n    # modify and update within the same transaction\n    await ctx.rollouts.update([updated_rollout])\n</code></pre> <p>The arguments passed to <code>atomic()</code> are quite arbitrary and flexible. Different implementations may have different interpretations of the arguments. For example, to <code>InMemoryLightningCollections</code>, the <code>mode</code> parameter controls locking behavior (<code>\"r\"</code> for read-only, <code>\"rw\"</code> for read-write), while <code>labels</code> specifies which collections to lock. Acquiring locks in sorted order prevents deadlocks when multiple operations run concurrently.</p>"},{"location":"deep-dive/store/#implementing-a-custom-backend","title":"Implementing a Custom Backend","text":"<p>To add a new storage backend, implement <code>LightningCollections</code>:</p> <pre><code>from agentlightning.store.collection import LightningCollections, Collection, Queue, KeyValue\n\nclass MyLightningCollections(LightningCollections):\n    @property\n    def rollouts(self) -&gt; Collection[Rollout]:\n        return self._rollouts  # your implementation\n\n    @property\n    def rollout_queue(self) -&gt; Queue[str]:\n        return self._queue  # your implementation\n\n    # ... implement remaining properties\n\n    async def atomic(self, *, mode, snapshot=False, labels=None, **kwargs):\n        # provide transaction / locking semantics\n        ...\n</code></pre> <p>Then instantiate your store:</p> <pre><code>from agentlightning.store.collection_based import CollectionBasedLightningStore\n\nstore = CollectionBasedLightningStore(collections=MyLightningCollections())\n</code></pre> <p>The store layer handles all business logic; your collections just need to provide correct CRUD semantics.</p>"},{"location":"deep-dive/store/#collection-based-store-implementations","title":"Collection-based Store Implementations","text":"<p>Agent-lightning ships with two collection-based store implementations:</p>"},{"location":"deep-dive/store/#inmemorylightningstore","title":"InMemoryLightningStore","text":"<p><code>InMemoryLightningStore</code> uses <code>InMemoryLightningCollections</code> backed by Python data structures. It supports fast startup with zero external dependencies\u2014ideal for local development, CI, and unit tests. It also provides two lock modes, configurable between <code>\"asyncio\"</code> (single-thread, multiple coroutines) and <code>\"thread\"</code> (multi-threaded via aiologic). <code>InMemoryLightningCollections</code> use nested dictionaries for O(1) primary-key lookup and <code>deque</code> for the task queue.</p>"},{"location":"deep-dive/store/#mongolightningstore","title":"MongoLightningStore","text":"<p><code>MongoLightningStore</code> uses <code>MongoLightningCollections</code> backed by MongoDB. It supports persistent storage suitable for production deployments and multi-process safe via database-level atomicity. It also supports partition support via <code>partition_id</code> for running multiple trainers against the same database.</p> <pre><code>from agentlightning.store.mongo import MongoLightningStore\n\nstore = MongoLightningStore(\n    mongo_uri=\"mongodb://localhost:27017/?replicaSet=rs0\",\n    database_name=\"agentlightning\",\n    partition_id=\"trainer-1\",  # optional: isolate data per trainer\n)\n</code></pre> <p>Note</p> <p><code>MongoLightningStore</code> requires the <code>mongo</code> optional dependency. Install with <code>pip install agentlightning[mongo]</code>.</p>"},{"location":"deep-dive/store/#capabilities","title":"Capabilities","text":"<p>Different stores have different capabilities. Check the <code>capabilities</code> property to understand what a store supports:</p> Capability Description InMemory Mongo Server Client <code>thread_safe</code> Safe for concurrent access from multiple threads configurable \u2713 \u2713 \u2713 <code>async_safe</code> Safe for concurrent access from multiple coroutines \u2713 \u2713 \u2713 \u2713 <code>zero_copy</code> Can be shared across processes without serialization \u2717 \u2713 \u2713 \u2713 <code>otlp_traces</code> Exposes an OTLP-compatible <code>/v1/traces</code> endpoint \u2717 \u2717 \u2713 \u2713"},{"location":"deep-dive/store/#thread-safety","title":"Thread Safety","text":"<p>Thread safety can be achieved at different layers:</p> <p>At the collections layer: <code>InMemoryLightningCollections</code> accepts a <code>lock_type</code> parameter:</p> <ul> <li><code>\"asyncio\"</code> \u2013 Uses per-event-loop <code>asyncio.Lock</code> for single-threaded, multi-coroutine scenarios.</li> <li><code>\"thread\"</code> \u2013 Uses <code>aiologic.Lock</code> for true multi-threaded access.</li> </ul> <p>At the store layer: <code>LightningStoreThreaded</code> wraps any <code>LightningStore</code> to add mutex-based thread safety:</p> <ul> <li>Methods like <code>start_rollout</code>, <code>enqueue_rollout</code>, <code>update_attempt</code>, <code>add_span</code>, etc. are guarded by a lock.</li> <li>Non-mutating, potentially blocking calls remain pass-through by design (e.g., <code>wait_for_rollouts</code>), as they don't modify shared state and should not hold the lock for long periods.</li> </ul> <p>Database-based stores like <code>MongoLightningStore</code> are inherently thread-safe through database atomicity guarantees.</p>"},{"location":"deep-dive/store/#process-safety-and-client-server-store","title":"Process Safety and Client-server Store","text":"<p><code>LightningStoreServer</code> wraps another underlying store and runs a FastAPI app to expose the store API over HTTP. <code>LightningStoreClient</code> is a small <code>LightningStore</code> implementation that talks to the HTTP API.</p> <p>Warning</p> <p>The server HTTP API is not considered a stable API at this moment. Users are encouraged to use the <code>LightningStoreClient</code> to communicate with the server as a stable interface.</p> <p>The server tracks the creator PID. In the owner process it delegates directly to the in-memory store; in other processes it lazily constructs a <code>LightningStoreClient</code> to talk to the HTTP API. This prevents accidental cross-process mutation of the wrong memory image. When the server is pickled (e.g., via <code>multiprocessing</code>), only the minimal fields are serialized, but NOT the FastAPI/uvicorn objects. Subprocesses won\u2019t accidentally carry live server state. Forked subprocess should also use <code>LightningStoreClient</code> to communicate with the server in the main process.</p> <p>On the client side, the client retries network/5xx failures using a small backoff, and probes <code>/v1/agl/health</code> between attempts. Application exceptions inside the server are wrapped as HTTP 400 with a traceback\u2014these are not retried. The client also maintains a per-event-loop <code>aiohttp.ClientSession</code> map so that tracer callbacks (often on separate loops/threads) don\u2019t hang by reusing a session from another loop.</p> <p>Minimal lifecycle:</p> <pre><code>import agentlightning as agl\n\n# Server (owner process)\nin_memory_store = agl.InMemoryLightningStore()\nserver = agl.LightningStoreServer(store=in_memory_store, host=\"0.0.0.0\", port=4747)\nawait server.start()      # starts uvicorn in a daemon thread and waits for /health\n# or keep your own event loop and stop via await server.stop()\n# await server.run_forever()\n\n# Client (same or different process)\nclient = agl.LightningStoreClient(\"http://localhost:4747\")\n\nprint(await client.query_rollouts(status_in=[\"queuing\"]))\n\nawait client.close()\nawait server.stop()\n</code></pre> <p>Another approach is to use a dedicated command line to start a long running server process, possibly sharable across multiple processes. In the main process, you can always use <code>LightningStoreClient</code> to communicate with the server.</p> <pre><code>agl store --port 4747\n</code></pre> <p>Note</p> <p><code>LightningStoreClient.wait_for_rollouts</code> intentionally enforces a tiny timeout (\u2264 0.1s) to avoid blocking event loops. Poll with short timeouts or compose with <code>asyncio.wait_for</code> at a higher layer.</p>"},{"location":"how-to/examples-catalog/","title":"Examples Catalog","text":"<p>Want to Contribute?</p> <p>We welcome contributions to the examples catalog! Please refer to the Contributing guide for more details.</p> <ul> <li> <p> APO room selector</p> <p>Prompt-optimize a room-booking agent with the built-in APO algorithm, then contrast it with the write-your-own algorithm and debugging workflows in the tutorials. Pairs well with the Train the First Agent how-to and the Write the First Algorithm guide.</p> <p> Browse source</p> </li> <li> <p> Azure OpenAI SFT</p> <p>Run a supervised fine-tuning loop against Azure OpenAI: roll out the capital-lookup agent, turn traces into JSONL, launch fine-tunes, and redeploy the resulting checkpoints through Azure CLI.</p> <p> Browse source</p> </li> <li> <p> Calc-X VERL math</p> <p>VERL-based reinforcement learning setup for a math-reasoning agent that uses AutoGen plus an MCP calculator tool to solve Calc-X problems end to end.</p> <p> Browse source</p> </li> <li> <p> ChartQA vision-language RL</p> <p>LangGraph-powered workflow for answering chart questions end to end: rollout the multi-modality agent with GPT or vLLM, and train with VERL/GRPO plus self-refinement loops.</p> <p> Browse source</p> </li> <li> <p> Claude Code SWE-bench</p> <p>Instrumented driver that runs Anthropic's Claude Code workflow on SWE-bench instances while streaming traces through Agent-lightning\u2014supports hosted vLLM, official Anthropic, or any OpenAI-compatible backend and emits datasets for downstream tuning.</p> <p> Browse source</p> </li> <li> <p> Minimal building blocks</p> <p>Bite-sized scripts that isolate Agent-lightning primitives (e.g., LightningStore usage, LLM proxying, minimal vLLM host) so you can study each part before composing larger workflows.</p> <p> Browse source</p> </li> <li> <p> RAG (MuSiQue)</p> <p>Retrieval-Augmented Generation pipeline that preps a Wikipedia retriever via MCP and trains a MuSiQue QA agent with GRPO. Documented for historical reference (verified on Agent-lightning v0.1.x).</p> <p> Browse source</p> </li> <li> <p> Spider SQL agent</p> <p>LangGraph-powered text-to-SQL workflow for the Spider benchmark, combining LangChain tooling with Agent-lightning rollouts; follow along with the how-to for training SQL agents.</p> <p> Browse source</p> </li> <li> <p> Tinker integration</p> <p>Adapter package (<code>agl_tinker</code>) with Tinker plus sample CrewAI/OpenAI agents that feed Agent-lightning traces into Tinker\u2019s reinforcement-learning backend for both toy and 20-Questions-style workflows.</p> <p> Browse source</p> </li> <li> <p> Unsloth SFT</p> <p>Supervised fine-tuning loop that ranks math-agent rollouts, fine-tunes with Unsloth\u2019s 4-bit LoRA stack, and mirrors the Fine-tune with Unsloth recipe.</p> <p> Browse source</p> </li> </ul>"},{"location":"how-to/train-first-agent/","title":"Train the First Agent with Agent-lightning","text":"<p>Welcome! This tutorial is your first step into making AI agents smarter using the Agent-lightning framework. We'll show you how to take a simple agent and automatically improve its performance through a process called Automatic Prompt Optimization (APO).</p> <p>The main goal of Agent-lightning is to provide a structured way to train your agents. Just like you train a machine learning model on data, you can train an agent on a task dataset. This could involve using Reinforcement Learning (RL) to teach it new behaviors or, as we'll do today, optimizing its prompts to make it more accurate and reliable.</p> <p>Tip</p> <p>You can open the sample code room_selector_apo.py and room_selector.py as you go through this tutorial.</p>"},{"location":"how-to/train-first-agent/#our-example-the-room-selector-agent","title":"Our Example: The Room Selector Agent","text":"<p>Today, we'll work with an agent whose job is to book a meeting room. It's a common but tricky task with multiple constraints.</p> <p>Here's how the agent works:</p> <ul> <li>Input: It receives a task with specific requirements, like \"<code>Find a room for 4 people at 10:00 AM with a whiteboard.</code>\"</li> <li>Action: The agent uses a Large Language Model (LLM) to understand the request. It can also use tools, which are pre-defined functions it can call, to get more information, such as checking room availability in an external database.</li> <li>Output: Its final decision is the ID of the best room it found, like \"<code>A103</code>\".</li> <li>Reward: After the agent makes its choice, a separate \"grader\" function scores its performance on a scale of 0 to 1. This score is called its reward. A perfect choice gets a 1.0, while a wrong one gets a 0.0.</li> </ul> <p>The agent's logic is sound, but its performance heavily depends on its initial prompt. A poorly worded prompt will confuse the LLM, leading to bad decisions. Our goal is to use Agent-lightning to find the best possible prompt automatically.</p> <p>A Closer Look at the Agent's Logic</p> <p>Modern LLMs can do more than just generate text; they can decide to call functions you provide. This is often called tool use or function calling. Our agent uses this capability to make informed decisions. If you're new to this concept, you can read more about it in OpenAI's documentation.</p> <p>Here is a sketch of the agent's logic, adhering closely to the OpenAI API:</p> <pre><code># Pseudo-code for the Room Selector agent\n\nimport openai\nimport json\n\ndef room_selector_agent(task, prompt):\n    client = openai.OpenAI()\n    messages = [{\"role\": \"user\", \"content\": prompt.format(**task)}]\n    tools = [ ... ] # Tool definition for the LLM\n\n    # 1. First LLM call to decide if a tool is needed.\n    response = client.chat.completions.create(\n        model=\"gpt-5-mini\",\n        messages=messages,\n        tools=tools,\n        tool_choice=\"auto\",\n    )\n    response_message = response.choices[0].message\n    tool_calls = response_message.tool_calls\n\n    # 2. Check if the LLM wants to use a tool.\n    if tool_calls:\n        messages.append(response_message) # Append assistant's reply\n\n        # 3. Execute the tool and get the real-world data.\n        for tool_call in tool_calls:\n            function_name = tool_call.function.name\n            if function_name == \"get_rooms_and_availability\":\n                function_args = json.loads(tool_call.function.arguments)\n                # Query the local room database\n                function_response = get_rooms_and_availability(\n                    date=function_args.get(\"date\"),\n                    time_str=function_args.get(\"time\"),\n                    duration_min=function_args.get(\"duration_min\"),\n                )\n                messages.append({\n                    \"tool_call_id\": tool_call.id,\n                    \"role\": \"tool\",\n                    \"name\": function_name,\n                    \"content\": json.dumps(function_response),\n                })\n\n        # 4. Second LLM call with the tool's output to get a final choice.\n        second_response = client.chat.completions.create(\n            model=\"gpt-5-mini\",\n            messages=messages,\n        )\n        final_choice = second_response.choices[0].message.content\n    else:\n        final_choice = response_message.content\n\n    # 5. Grade the final choice to get a reward.\n    reward = grade_the_choice(final_choice, task[\"expected_choice\"])\n    return reward\n</code></pre> <p>In Agent-lightning, you wrap this logic in a Python function marked with the <code>@rollout</code> decorator, so that the agent can be managed and tuned by Agent-lightning's runner and trainer. The <code>prompt_template</code> that the APO algorithm tunes is passed in as an argument:</p> <pre><code>import agentlightning as agl\n\n@agl.rollout\ndef room_selector(task: RoomSelectionTask, prompt_template: agl.PromptTemplate) -&gt; float:\n    # ... agent logic using the prompt_template ...\n\n    # The final reward is determined by a grader function\n    reward = room_selection_grader(client, final_message, task[\"expected_choice\"])\n    return reward\n</code></pre>"},{"location":"how-to/train-first-agent/#core-concepts-tasks-rollouts-spans-and-prompt-templates","title":"Core Concepts: Tasks, Rollouts, Spans, and Prompt Templates","text":"<p>To understand how Agent-lightning works, you need to know these key terms.</p>"},{"location":"how-to/train-first-agent/#task","title":"Task","text":"<p>A task is a specific input or problem statement given to the agent. It defines what the agent needs to accomplish.</p> <p>Analogy: Task</p> <p>If the agent is a chef, a task is the recipe request: \"Bake a chocolate cake.\"</p>"},{"location":"how-to/train-first-agent/#rollout","title":"Rollout","text":"<p>A rollout is a single, complete execution of an agent attempting to solve a given task. It's the entire story from receiving the task to producing a final result and receiving a reward. A rollout captures a full trace of the agent's execution.</p> <p>Analogy: Rollout</p> <p>A rollout is one full attempt by the chef to bake the chocolate cake, from gathering ingredients to the final taste test.</p>"},{"location":"how-to/train-first-agent/#span","title":"Span","text":"<p>A span represents a single unit of work or an operation within a rollout. Spans are the building blocks of a trace. They have a start and end time and contain details about the specific operation, like an LLM call, a tool execution, or a reward calculation. For a more precise definition, see the OpenTelemetry documentation.</p> <p>Analogy: Span</p> <p>If the rollout is \"baking a cake,\" a span could be \"preheating the oven,\" \"mixing flour and sugar,\" or \"adding frosting.\" Each is a distinct step or unit of work.</p> <p>The picture below from ADK shows a typical rollout, where each rectangle in the waterfall visualizes a span. As can be seen in the visualization, spans can be sequential, parallel or nested among each other. In other frameworks, the terminology might be slightly different. Agent-lightning follows the terminologies used by OpenTelemetry to avoid confusion.</p> <p></p>"},{"location":"how-to/train-first-agent/#prompt-template","title":"Prompt Template","text":"<p>A prompt template is a reusable instruction for the agent, often containing placeholders that can be filled in with specific details from a task. It is a key \"resource\" that the algorithm learns and improves over time.</p> <p>Analogy: Resource (Prompt Template)</p> <p>If the task is the recipe request, the prompt template is the master recipe card that the chef follows. The algorithm's job is to edit this recipe card to make the instructions clearer and the final dish better.</p>"},{"location":"how-to/train-first-agent/#the-training-loop-how-the-magic-happens","title":"The Training Loop: How the Magic Happens","text":"<p>Training in Agent-lightning revolves around a clear, managed loop, orchestrated by the Trainer. The diagram below illustrates this core interaction:</p> <p></p> <p>The Loop Explained:</p> <ul> <li>Algorithm to Agent (via Trainer): The Algorithm (the \"brain\") creates an improved Prompt Template and selects Tasks. The Trainer then sends both to the Agent.</li> <li>Agent to Algorithm (via Trainer): For each task it receives, the Agent uses the provided prompt template to perform a Rollout, executing its logic and potentially using tools. During this rollout, the runner that runs the agent captures Spans that detail every step. The agent also calculates a Reward for its performance on the task. These spans and rewards are then sent back to the Algorithm via the Trainer.</li> <li>Algorithm Learning: The Algorithm then analyzes these spans and rewards to learn how to improve the agent's behavior, for example, by generating a better prompt. This improved prompt is then used in the next iteration of tasks.</li> </ul> <p>This cycle continues, allowing the agent to continuously learn and get better at solving tasks.</p> <p>Note</p> <p>In the next tutorial, we will see that the \"via Trainer\" here is not accurate. It's actually via the runner and store.</p>"},{"location":"how-to/train-first-agent/#the-algorithm","title":"The Algorithm","text":"<p>The algorithm is the smart part of the system that drives the improvement. In this tutorial, we use APO (Automatic Prompt Optimization). It works in a few steps:</p> <ol> <li>Evaluate: The algorithm first asks for rollouts to be run using the current prompt template to see how well it performs.</li> <li>Critique: It then looks at the detailed spans from those rollouts. Using a powerful LLM (<code>gpt-5-mini</code>), it generates a \"textual gradient\", which is a natural language critique of the prompt. For example: \"The prompt is ambiguous about how to handle tie-breakers for equally good rooms.\"</li> <li>Rewrite: Finally, it gives the critique and the original prompt to another LLM (<code>gpt-4.1-mini</code>) and asks it to apply the edits, generating a new, improved prompt template.</li> </ol> <p>This cycle repeats, with each round producing a slightly better prompt. To use it, you simply initialize the APO class with your desired hyperparameters.</p> <pre><code># In the main training script: run_apo.py\nfrom openai import AsyncOpenAI\n\nopenai = AsyncOpenAI()\nalgo = agl.APO(openai)\n</code></pre> <p>Tip</p> <p>Make sure you have <code>OPENAI_API_KEY</code> set in your environment variables.</p>"},{"location":"how-to/train-first-agent/#the-trainer","title":"The Trainer","text":"<p>The Trainer is the central component you'll interact with. It connects everything and manages the entire workflow by running the loop described above. You configure the Trainer, providing the algorithm, the number of parallel runners, and the initial prompt. A single call to <code>trainer.fit()</code> kicks off the entire process!</p> <pre><code># 1. Configure the Trainer with the algorithm and initial prompt\ntrainer = agl.Trainer(\n    algorithm=algo,\n    n_runners=8, # Run 8 agents in parallel to try out the prompts\n    initial_resources={\n        # The initial prompt template to be tuned\n        \"prompt_template\": prompt_template_baseline()\n    },\n    # This is used to convert the span data into a message format consumable by APO algorithm\n    adapter=agl.TraceToMessages(),\n)\n\n# 2. Load datasets: They can be list of task objects consumable by `room_selector`.\ndataset_train, dataset_val = ...\n\n# 3. Start the training process!\ntrainer.fit(\n    agent=room_selector,\n    train_dataset=dataset_train,\n    val_dataset=dataset_val\n)\n</code></pre> <p>Tip</p> <p><code>TraceToMessages</code> is a convenience adapter that converts spans into OpenAI chat messages. It requires <code>openai &gt;= 1.100.0</code> to be installed.</p>"},{"location":"how-to/train-first-agent/#training-results","title":"Training Results","text":"<p>The APO algorithm successfully improved the agent's performance. We ran the example with the following hyper-parameters:</p> <ul> <li><code>val_batch_size</code> = 10</li> <li><code>gradient_batch_size</code> = 4</li> <li><code>beam_width</code> = 2</li> <li><code>branch_factor</code> = 2</li> <li><code>beam_rounds</code> = 2</li> </ul> <p>The validation accuracy on the 29 samples of datasets steadily increase from 0.569 (baseline) to 0.721 (after round 2). The tuning takes around 10 minutes with 8 runners. We ran twice, and the results are shown in the chart below.</p> <p>This demonstrates how Agent-lightning can efficiently and automatically enhance your agent's capabilities with just a few lines of code.</p>"},{"location":"how-to/train-sql-agent/","title":"Train SQL Agent with Agent-lightning and VERL","text":"<p>This walkthrough builds upon the Agent-lightning SQL Agent example and explains how the system components integrate: a LangGraph-based SQL agent wrapped as a <code>LitAgent</code>, the <code>VERL</code> reinforcement learning (RL) algorithm, and the <code>Trainer</code>, which coordinates both training and debugging.</p> <p>The command-line interface in <code>examples/spider/train_sql_agent.py</code> provides a complete runnable example. However, this document focuses on understanding the underlying architecture so you can effectively adapt the workflow to your own agents.</p>"},{"location":"how-to/train-sql-agent/#sql-agent-architecture","title":"SQL Agent Architecture","text":"<p>Agent-lightning integrates seamlessly with various orchestration frameworks, including Agent Framework, AutoGen, CrewAI, LangGraph, and the OpenAI Agents SDK. It can also interoperate with custom Python logic.</p> <p>In this example, LangGraph defines a cyclic workflow that mirrors an analyst\u2019s iterative SQL development process. The following graph (rendered directly from <code>sql_agent.py</code>) illustrates how the agent drafts, executes, critiques, and refines queries until a satisfactory result is achieved.</p> <pre><code>---\nconfig:\n  flowchart:\n    curve: linear\n---\ngraph LR;\n        __start__([&lt;p&gt;__start__&lt;/p&gt;]):::first\n        write_query(write_query)\n        execute_query(execute_query)\n        check_query(check_query)\n        rewrite_query(rewrite_query)\n        __end__([&lt;p&gt;__end__&lt;/p&gt;]):::last\n        __start__ --&gt; write_query;\n        check_query -.-&gt; __end__;\n        check_query -.-&gt; rewrite_query;\n        execute_query --&gt; check_query;\n        rewrite_query --&gt; execute_query;\n        write_query --&gt; execute_query;\n        classDef default fill:#f2f2f2,line-height:1.2\n        classDef first fill-opacity:0\n        classDef last fill:#cccccc</code></pre> <p>Note</p> <p>The workflow proceeds through the following stages:</p> <ol> <li>write_query \u2013 Generates an initial SQL query from the user\u2019s question and the database schema.</li> <li>execute_query \u2013 Executes the generated query against the target database.</li> <li>check_query \u2013 Evaluates the query and its results (or errors) using a specialized prompt (<code>CHECK_QUERY_PROMPT</code>) to detect issues.</li> <li>rewrite_query \u2013 If issues are identified, the agent rewrites the query using feedback from the previous step and re-enters the loop.</li> <li>END \u2013 The cycle terminates when the query is validated or the maximum iteration count (<code>max_turns</code>) is reached. Each turn consists of one full loop through the <code>write_query</code>, <code>execute_query</code>, <code>check_query</code>, and (if applicable) <code>rewrite_query</code> stages.</li> </ol> <p>In this tutorial, reinforcement learning (RL) is used to optimize the <code>write_query</code> and <code>rewrite_query</code> stages. While the <code>check_query</code> step shares the same underlying LLM weights, its trace data is not used for learning.</p> <p>To keep the design modular and maintainable, it is recommended to define the LangGraph-based SQL Agent in a separate file and expose it via a builder function such as:</p> <pre><code>def build_langgraph_sql_agent(\n    database_path: str,\n    openai_base_url: str,\n    model: str,\n    sampling_parameters: Dict[str, Any],\n    max_turns: int,\n    truncate_length: int\n):\n    builder = StateGraph(State)\n    builder.add_node(write_query)\n    ...\n\n    builder.add_edge(START, \"write_query\")\n    ...\n\n    return builder.compile().graph()\n</code></pre> <p>This approach isolates your LangGraph logic from Agent-lightning version changes, improving both readability and debuggability.</p>"},{"location":"how-to/train-sql-agent/#bridging-langgraph-and-agent-lightning","title":"Bridging LangGraph and Agent-lightning","text":"<p>Tip</p> <p>Keep <code>sql_agent.py</code> open on the side while reading this section. This will help you understand how the code snippets shown here work in practice.</p> <p>The <code>LitSQLAgent</code> class defined in <code>sql_agent.py</code> acts as the bridge. It subclasses <code>agl.LitAgent</code>, allowing the runner to provision shared resources (e.g., LLMs) for each rollout.</p> <p>Below is a simplified illustration of the key logic (note: this is conceptual pseudocode; the actual implementation includes dataset-specific details):</p> <pre><code>class LitSQLAgent(agl.LitAgent[Dict[str, Any]]):\n\n    def __init__(self, max_turns: int, truncate_length: int):\n        # Every turn here refers to a full cycle of write/exe/check/rewrite\n        self.max_turns = max_turns\n        self.truncate_length = truncate_length\n\n    def rollout(\n        self,\n        task: Dict[str, Any],\n        resources: agl.NamedResources,\n        rollout: agl.Rollout\n    ) -&gt; float | None:\n        llm: agl.LLM = resources[\"main_llm\"]\n        agent = build_langgraph_sql_agent(\n            database_path=\"sqlite:///\" + task[\"db_id\"],\n            max_turns=self.max_turns,\n            truncate_length=self.truncate_length,\n            openai_base_url=llm.get_base_url(rollout.rollout_id, rollout.attempt.attempt_id),\n            model=llm.model,\n            sampling_parameters=llm.sampling_parameters,\n        )\n        result = agent.invoke({\"question\": question}, {\n            \"callbacks\": [self.tracer.get_langchain_handler()],\n            \"recursion_limit\": 100,\n        })\n        reward = evaluate_query(result[\"query\"], ground_truth, db_path, raise_on_error=False)\n        return reward\n</code></pre> <p>The <code>LitSQLAgent</code> serves as a lightweight wrapper around the LangGraph agent, providing the correct interface for the <code>rollout</code> method. It constructs the LangGraph agent, invokes it, and returns the evaluation result as a reward signal.</p> <p>The <code>\"main_llm\"</code> resource key is a convention between the agent and VERL. It is used to inject an OpenAI-compatible endpoint from the VERL algorithm during rollout. Two approaches are supported to use this agentlightning.LLM resource:</p> <ol> <li>Direct access \u2013 Use <code>llm.endpoint</code> for a simple integration (identical to the v0.1 example).</li> <li>Context-aware access \u2013 Use <code>get_base_url</code> with <code>rollout.rollout_id</code> and <code>rollout.attempt.attempt_id</code>.    This approach enables per-caller trace attribution, improving trace collection per rollout or attempt when runner-side tracers are unavailable. For details, see Working with Traces.</li> </ol>"},{"location":"how-to/train-sql-agent/#reward-signal-and-evaluation","title":"Reward Signal and Evaluation","text":"<p>The <code>evaluate_query</code> function provides the reward mechanism for RL training. In agent training, obtaining a consistent and meaningful reward signal is often challenging. Fortunately, this is simplified when using the Spider dataset. The dataset includes ~8k samples containing natural-language questions, database schemas, and ground-truth SQL queries.</p> <p>Using the Spider evaluator, the agent's generated query is executed and compared to the ground-truth query on the target database. The two queries are considered equivalent if they produce identical execution results.</p> <p>Attention</p> <p>The ground-truth queries must never be exposed to the agent during training to prevent data leakage.</p> <p>In this setup, the reward is returned directly from the <code>rollout</code> method, enabling the runner to forward it back to the RL algorithm.</p> <p>Warning</p> <p>Avoid using <code>emit_reward</code> in conjunction with returning a reward value. Doing both will cause the algorithm to receive duplicate reward signals, leading to inconsistent training behavior.</p>"},{"location":"how-to/train-sql-agent/#configuring-verl-for-reinforcement-learning","title":"Configuring VERL for Reinforcement Learning","text":"<p>View <code>examples/spider/train_sql_agent.py</code> for a full reinforcement learning configuration, which is a plain Python dictionary. It mirrors (and actually is) the shell arguments used to launch training in the VERL framework but is easier to tweak programmatically:</p> <pre><code>verl_config: Dict[str, Any] = {\n    \"algorithm\": {\"adv_estimator\": \"grpo\", \"use_kl_in_reward\": False},\n    \"data\": {\n        # train_files and val_files are no longer needed here\n        # because data are read in agl.Trainer\n        ...,\n        # Controls how many tasks are pooled per step\n        # (multiplied by actor_rollout_ref.rollout.n)\n        \"train_batch_size\": 32,\n        # Prompt and responses larger than these lengths are truncated\n        \"max_prompt_length\": 4096,\n        \"max_response_length\": 2048,\n    },\n    \"actor_rollout_ref\": {\n        \"rollout\": {\n            # Only vLLM is supported currently\n            \"name\": \"vllm\",\n            # Equals to group size of GRPO\n            \"n\": 4,\n            # Used to enable tool call parser in vLLM\n            \"multi_turn\": {\"format\": \"hermes\"},\n            ...\n        },\n        \"actor\": {\"ppo_mini_batch_size\": 32, \"optim\": {\"lr\": 1e-6}, ...},\n        \"model\": {\n            # Config your preferred LLM here\n            \"path\": \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n            ...\n        },\n    },\n    \"trainer\": {\n        \"n_gpus_per_node\": 1,\n        # Validation once before training starts\n        \"val_before_train\": True,\n        # Validation every N training steps\n        \"test_freq\": 32,\n        # Save checkpoints every N training steps\n        \"save_freq\": 64,\n        # Go through the train dataset this many times\n        \"total_epochs\": 2\n    },\n}\n</code></pre> <p>This is equivalent to the following CLI invocation:</p> <pre><code>python3 -m verl.trainer.main_ppo \\\n    algorithm.adv_estimator=grpo \\\n    algorithm.use_kl_in_reward=False \\\n    data.train_batch_size=32 \\\n    data.max_prompt_length=4096 \\\n    data.max_response_length=2048 \\\n    actor_rollout_ref.rollout.name=vllm \\\n    actor_rollout_ref.rollout.n=4 \\\n    actor_rollout_ref.rollout.multi_turn.format=hermes \\\n    actor_rollout_ref.actor.ppo_mini_batch_size=32 \\\n    actor_rollout_ref.actor.optim.lr=1e-6 \\\n    actor_rollout_ref.model.path=Qwen/Qwen2.5-Coder-1.5B-Instruct \\\n    trainer.n_gpus_per_node=1 \\\n    trainer.val_before_train=True \\\n    trainer.test_freq=32 \\\n    trainer.save_freq=64 \\\n    trainer.total_epochs=2\n</code></pre> <p>Warning</p> <p>We used to provide a CLI called <code>python -m agentlightning.verl</code> to launch training in v0.1. This is no longer the recommended approach. Instead, use <code>agl.Trainer</code> to run VERL and agent runners together, or follow the debugging tutorial if you want an isolated experience similar to v0.1.</p>"},{"location":"how-to/train-sql-agent/#orchestrating-training-with-trainer","title":"Orchestrating Training with <code>Trainer</code>","text":"<p><code>Trainer</code> is the high-level orchestrator that integrates the agent, algorithm, dataset, and distributed runners. The key benefits of using the <code>Trainer</code> are:</p> <ol> <li>It allows you to launch everything with a single line of code: <code>trainer.fit(...)</code>.</li> <li>It exposes configuration options such as <code>n_runners</code> to control parallelism and <code>adapter</code> to define how algorithms interpret the trace data produced by the agent.</li> </ol> <p>An example usage is shown below:</p> <pre><code>import agentlightning as agl\n\nagent = LitSQLAgent()\nalgorithm = agl.VERL(verl_config)\ntrainer = agl.Trainer(\n    n_runners=10,\n    algorithm=algorithm,\n    adapter={\"agent_match\": active_agent},\n)\ntrain_data = pd.read_parquet(\"data/train_spider.parquet\").to_dict(\"records\")\nval_data = pd.read_parquet(\"data/test_dev_500.parquet\").to_dict(\"records\")\ntrainer.fit(agent, train_dataset=train_data, val_dataset=val_data)\n</code></pre> <p>First, <code>agl.VERL(verl_config)</code> launches the <code>VERL</code> algorithm and its OpenAI-compatible proxy. The <code>train_data</code> and <code>val_data</code> are passed into <code>VERL</code>, which enqueues tasks to a centralized task queue managed by the <code>LightningStore</code>, accessible to all runners.</p> <p>When <code>Trainer.fit</code> is called, it launches 10 concurrent runners (as specified by <code>n_runners=10</code>). Each runner pulls tasks from the centralized task queue, executes the agent\u2019s <code>rollout</code> method, collects traces, and returns rewards to VERL for training.</p> <p>The <code>Adapter</code>, as discussed earlier, is used at the algorithm side, and receives the traces emitted by the agent and runners. The <code>agent_match</code> parameter ensures <code>VERL</code> only ingests spans from the specific agent you want to optimize. In the example above, there are at least three agents\u2014<code>write_query</code>, <code>rewrite_query</code>, and <code>check_query</code>. By setting <code>agent_match</code> to a regex like <code>\"write\"</code>, both <code>write_query</code> and <code>rewrite_query</code> agents are optimized simultaneously. You can also set it to <code>\"write|check\"</code> or <code>None</code> to include all agents if desired.</p>"},{"location":"how-to/train-sql-agent/#dry-run-the-pipeline-with-trainerdev","title":"Dry-Run the Pipeline with <code>Trainer.dev</code>","text":"<p>Before committing hours of GPU time, you can dry-run the agent with <code>Trainer.dev()</code>. This method swaps in the lightweight <code>Baseline</code> algorithm, enqueues up to ten tasks, and prints every span emitted by the agent. Because it uses the same runner stack as full training, it\u2019s ideal for verifying database connections and LangGraph control flow.</p> <p>To begin, the agent needs a valid OpenAI-compatible endpoint since VERL is not active in this mode. You can use OpenAI\u2019s official API or your own local LLM endpoint. Wrap it as follows:</p> <pre><code>trainer = agl.Trainer(\n    n_workers=1,\n    initial_resources={\n        \"main_llm\": agl.LLM(\n            endpoint=os.environ[\"OPENAI_API_BASE\"],\n            model=\"gpt-4.1-nano\",\n            sampling_parameters={\"temperature\": 0.7},\n        )\n    },\n)\n</code></pre> <p>Then, call <code>trainer.dev(...)</code> with a small number of tasks:</p> <pre><code>dev_data = pd.read_parquet(\"data/test_dev_500.parquet\").to_dict(\"records\")[:10]\ntrainer.dev(agent, dev_dataset=dev_data)\n</code></pre> <p>Run this in a Python session or adapt your script to include a <code>--dev</code> flag. Once the spans appear healthy and the rewards are non-zero, switch back to <code>trainer.fit(...)</code> for full RL training. See the debugging tutorial for more tips on how to debug the agent.</p>"},{"location":"how-to/train-sql-agent/#running-the-sample-code","title":"Running the Sample Code","text":"<p>The following tutorial explains how to run the complete example in <code>examples/spider</code>.</p>"},{"location":"how-to/train-sql-agent/#dataset","title":"Dataset","text":"<p>The trainer expects three Parquet files inside <code>examples/spider/data</code>: <code>train_spider.parquet</code>, <code>test_dev_500.parquet</code>, and <code>test_dev.parquet</code>.</p> <p>Download the curated dataset bundle provided with the repository:</p> <pre><code>cd examples/spider\npip install gdown  # included in the 'experiment' optional dependency\ngdown --fuzzy https://drive.google.com/file/d/1oi9J1jZP9TyM35L85CL3qeGWl2jqlnL6/view\nunzip -q spider-data.zip -d data\nrm spider-data.zip\n</code></pre> <p>If you prefer to generate the files yourself, download Spider 1.0 and run:</p> <pre><code>python spider_eval/convert_dataset.py\n</code></pre> <p>Set <code>VERL_SPIDER_DATA_DIR</code> if you store the dataset outside the default <code>data</code> directory.</p>"},{"location":"how-to/train-sql-agent/#dependencies","title":"Dependencies","text":"<p>Create a clean virtual environment, activate it, and install Agent-lightning with the VERL extras required by this tutorial. Install LangChain-related dependencies as needed.</p> <p>For full training profiles, plan to use a GPU with at least 40 GB of memory.</p>"},{"location":"how-to/train-sql-agent/#launch-training","title":"Launch Training","text":"<p>From <code>examples/spider</code>, run one of the helper scripts depending on your model preference:</p> <pre><code>python train_sql_agent.py qwen   # Default Qwen-2.5-Coder-1.5B run\npython train_sql_agent.py llama  # LLaMA-3.2-1B with llama3_json tool parser\n</code></pre> <p>The script instantiates <code>LitSQLAgent</code> and launches <code>trainer.fit</code>. Provide <code>--active-agent my_agent_variant</code> if you only want to train one of the agents in the graph.</p> <p>For the LLaMA profile, export an <code>HF_TOKEN</code> before running so VERL can download the model weights.</p> <p>Troubleshooting</p> <p>If you have got some Ray worker errors on either <code>WANDB_API_KEY</code> not set, or <code>HF_TOKEN</code> not set, or data not found, please try to restart the Ray cluster with the helper script: scripts/restart_ray.sh, which essentially stops the ray cluster if any, and starts a new one:</p> <pre><code>env RAY_DEBUG=legacy HYDRA_FULL_ERROR=1 VLLM_USE_V1=1 ray start --head --dashboard-host=0.0.0.0\n</code></pre> <p>Launching Training with NPUs</p> <p>The example also supports running with Huawei Ascend NPUs. This feature is contributed by Teams from Huawei. To use it, resort to the function <code>config_train_npu</code> in the script.</p> <p>Hardware Supported: Atlas 200T A2 Box16, Atlas 900 A2 PODc, Atlas 800T A3. At least a single 40GB NPU is required to run the Qwen2.5-Coder-1.5B-Instruct model.</p> <p>Environment Setup: Python 3.11.13, CANN 8.2.RC1, torch 2.7.1+cpu, torch_npu 2.7.1.dev20250724. For basic environment preparation, please refer to this document.</p> <p>Before installing dependencies, configure the following pip mirrors:</p> <pre><code>pip config set global.index-url http://repo.huaweicloud.com/repository/pypi/simple\npip config set global.extra-index-url \"https://download.pytorch.org/whl/cpu/ https://mirrors.huaweicloud.com/ascend/repos/pypi\"\n</code></pre> <p>Then install vLLM, vLLM-Ascend and VERL:</p> <pre><code>pip install vllm==0.10.0 --trusted-host repo.huaweicloud.com\npip install vllm-Ascend==0.10.0rc1 --trusted-host repo.huaweicloud.com\npip install verl==0.5.0\n</code></pre> <p>To ensure the VERL framework runs correctly on NPU, add the following lines to <code>verl/utils/vllm_utils.py</code>:</p> <pre><code>from vllm_ascend.patch import platform\nfrom vllm_ascend.patch import worker\n</code></pre> <p>See the following reference for more details: https://github.com/vllm-project/vllm-ascend/issues/1776.</p> <p>After the above dependencies have been installed, from <code>examples/spider</code> run the following script command:</p> <pre><code>python train_sql_agent.py npu\n</code></pre>"},{"location":"how-to/train-sql-agent/#debugging-the-agent-without-verl","title":"Debugging the Agent without VERL","text":"<p><code>sql_agent.py</code> also provides a <code>debug_sql_agent()</code> helper to run the LangGraph workflow directly against a local or hosted OpenAI-compatible endpoint before using VERL.</p> <p>Set the following environment variables, then execute the file:</p> <pre><code>export OPENAI_API_BASE=&lt;your_api_base&gt;\nexport OPENAI_API_KEY=&lt;your_api_key&gt;\ncd examples/spider\npython sql_agent.py\n</code></pre> <p>This allows you to verify that the workflow and prompts behave as expected before reinforcement learning is introduced.</p>"},{"location":"how-to/train-sql-agent/#evaluation","title":"Evaluation","text":"<p>The following results were obtained by running <code>python train_sql_agent.py qwen</code> on a single 80 GB GPU. Training completes in approximately 12 hours. The training curves below are smoothed by aggregating every 16 steps for better visualization.</p> <p>Additional evaluation results were collected with a legacy version \u2014 Agent-lightning v0.1.1, <code>verl==0.5.0</code>, and <code>vllm==0.10.0</code>. You can find them in this write-up: Training AI Agents to Write and Self-Correct SQL with Reinforcement Learning</p>"},{"location":"how-to/unsloth-sft/","title":"Fine-tune with Unsloth SFT","text":"<p>Prerequisites</p> <p>Please make sure you have read Write the First Algorithm. Although that recipe is based on a simple prompt tuning algorithm, it introduces the core concepts of Agent-lightning and you should be familiar with them before proceeding.</p> <p>This recipe builds on Write the First Algorithm. Instead of iterating on a prompt, we will fine-tune a large language model with Unsloth's SFT Trainer and keep the whole loop inside Agent-lightning. The new pieces you will meet are the LLM proxy, the trace-to-triplet adapter, a vLLM inference endpoint, and an agent implemented with the OpenAI Agents SDK. The full sample code is available in the <code>examples/unsloth</code> folder.</p> <p>Warning</p> <p>You need a GPU that can host the Unsloth base model and run vLLM. The sample defaults to <code>unsloth/Qwen3-4B-Instruct-2507</code>, which requires at least 16GB of GPU memory under 4-bit quantization.</p>"},{"location":"how-to/unsloth-sft/#the-data-and-serving-loop","title":"The Data and Serving Loop","text":"<p>To tune a large language model in Supervised Fine-Tuning (SFT), we commonly need a dataset with input/output samples. For example, the TRL SFT Trainer expects a dataset with samples like the following:</p> <pre><code>{\"messages\": [{\"role\": \"user\", \"content\": \"What color is the sky?\"},\n              {\"role\": \"assistant\", \"content\": \"It is blue.\"}]}\n</code></pre> <p>With supervised fine-tuning, the LLM learns to generate the \"assistant\" response as close as possible to the completion in the dataset.</p> <p>Typically, the dataset used in SFT should be a curated set of samples. The samples can be either hand-written by humans, or generated by a more powerful model, which is known as data distillation. However, in this recipe, we use a different setup that relies on samples generated by the model itself. We use the reward emitted by the agent to select the top-performing samples.</p> <p>Overall, the flow of the algorithm is an iteration of the following steps:</p> <ol> <li>Serve the current checkpoint (with vLLM).</li> <li>Publish the vLLM endpoint through the LLM proxy and let runners roll out some tasks with the current model.</li> <li>Collect the traces from the rollouts and transform the highest-rewarded ones into a dataset that is acceptable for Unsloth SFT Trainer.</li> <li>Launch Unsloth to fine-tune on the dataset and save a new checkpoint.</li> </ol> <p>You will find the full source code of this iteration in <code>sft_one_iter</code> in sft_algorithm.py. We will elaborate on each part below.</p>"},{"location":"how-to/unsloth-sft/#serving-the-model-with-vllm-and-proxy","title":"Serving the Model with vLLM and Proxy","text":"<p>Most modern agents do not use the model directly; instead, they use an API like the OpenAI chat completions API to interact with the model. Therefore, we need a vLLM-based inference server launched before rollouts. The serving code looks like the following. See the <code>vllm_server</code> function in sft_algorithm.py if you want to see a more robust version.</p> <pre><code>from openai import OpenAI\n\nvllm_process = subprocess.Popen([\n    \"vllm\", \"serve\", model_path, \"--port\", str(port),\n    \"--enable-auto-tool-choice\", \"--tool-call-parser\", \"hermes\"\n])\n\n# Wait for the server to be ready\nurl = f\"http://localhost:{port}/health\"\nstart = time.time()\nclient = httpx.Client()\n\nwhile True:\n    if client.get(url).status_code == 200:\n        break\n\nserver_address = f\"http://localhost:{port}/v1\"\n\n# Try using the vLLM server\nopenai = OpenAI(base_url=server_address)\n...\n</code></pre> <p>In this recipe, we do not expose the server address directly to the agent runners, because we want to install a \"middleware\" to collect the prompts and responses of all the requests. In general, it's up to you to decide whether to hide the vLLM server behind a proxy or not.</p> <p>The \"middleware\" here is <code>LLMProxy</code>, which is an independent LiteLLM server that forwards the requests to the vLLM server. It also exposes an OpenAI-compatible API that the runners can target without caring about where the model lives. The benefits of using the proxy are:</p> <ol> <li>Traces: The proxy automatically logs the prompts and responses of all the requests into the store.</li> <li>Token IDs: The proxy augments the requests so that the vLLM server can return the prompt and response token IDs (see more details in Serving LLM).</li> </ol> <p>The <code>LLMProxy</code> accepts a list of model configurations, in the same syntax as LiteLLM's <code>model_list</code>. Include a <code>hosted_vllm/</code> prefix to the models to activate LiteLLM's vLLM integration.</p> <pre><code>import agentlightning as agl\n\nllm_proxy = agl.LLMProxy(port=port, store=store)\nmodel_list = [\n    {\n        \"model_name\": \"Qwen3-4B-Instruct\",\n        \"litellm_params\": {\"model\": f\"hosted_vllm/{model_path}\", \"api_base\": server_address},\n    }\n]\nllm_proxy.update_model_list(model_list)\n# If the proxy is not running, it will start automatically.\nawait llm_proxy.restart()\n# Add the proxy as a resource to the store so that the runners can access it via URL.\nresource_update = await store.add_resources({\"main_llm\": llm_proxy.as_resource()})\n</code></pre>"},{"location":"how-to/unsloth-sft/#spawn-rollout-and-collect-spans","title":"Spawn Rollout and Collect Spans","text":"<p>Once the proxy is registered as a resource, the algorithm schedules work for the rollout runners. Each problem from a training dataset becomes a rollout with the proxy baked into its resources:</p> <pre><code>rollouts: list[Rollout] = []\nfor sample in train_dataset:\n    rollouts.append(\n        await store.enqueue_rollout(\n            input=sample,\n            mode=\"train\",\n            resources_id=resources_update.resources_id,\n        )\n    )\n</code></pre> <p><code>resources_id</code> ties every rollout to the <code>main_llm</code> proxy resource we just uploaded. The runners on the other side poll the store (<code>LitAgentRunner.iter()</code>) and execute the agent for each rollout. On the algorithm side we wait for completions with a non-blocking polling loop:</p> <pre><code>completed_rollouts: list[Rollout] = []\nwhile True:\n    completed_rollouts = await store.wait_for_rollouts(\n        rollout_ids=[r.rollout_id for r in rollouts],\n        timeout=0.0,\n    )\n    if len(completed_rollouts) == len(rollouts):\n        break\n    await asyncio.sleep(5.0)\n</code></pre> <p>Note</p> <p>The <code>timeout=0.0</code> is needed here because this example uses a <code>LightningStoreClient</code>, and <code>wait_for_rollouts</code> establishes an HTTP connection to that store. Currently, only non-blocking wait requests are supported, which avoids holding the store connection open.</p> <p>Once the rollouts complete, we terminate the vLLM server to free up GPU memory.</p> <pre><code>vllm_process.terminate()\nvllm_process.join(timeout=10.0)\n</code></pre>"},{"location":"how-to/unsloth-sft/#adapt-the-spans-to-huggingface-dataset","title":"Adapt the Spans to HuggingFace Dataset","text":"<p><code>LlmProxyTraceToTriplet</code> converts the proxy\u2019s spans (which might be dozens to hundreds per rollout) into <code>Triplet</code> objects that contain prompt/response token IDs plus an optional reward. The adapter may return multiple triplets per rollout (one per chat-completion call). To bias training toward successful reasoning chains the algorithm walks the triplets in reverse order, keeps the most recent reward, and turns each prompt/response pair into Hugging Face dataset rows:</p> <pre><code>all_triplets = []\ndata_adapter = agl.LlmProxyTraceToTriplet()\n\nfor rollout in completed_rollouts:\n    spans = await store.query_spans(rollout.rollout_id, \"latest\")\n    triplets = data_adapter.adapt(spans)\n\n    recent_reward = None\n    for triplet in reversed(triplets):\n        if triplet.reward is not None:\n            recent_reward = triplet.reward\n        if recent_reward is None:\n            continue\n\n        input_ids = triplet.prompt[\"token_ids\"] + triplet.response[\"token_ids\"]\n        # We don't train on prompt tokens, so they are masked out by setting to -100.\n        labels = [-100] * len(triplet.prompt[\"token_ids\"]) + triplet.response[\"token_ids\"]\n        # This matches the dataset format required by the Unsloth SFT trainer.\n        all_triplets.append(\n            {\n                \"input_ids\": input_ids,\n                \"attention_mask\": [1] * len(input_ids),\n                \"labels\": labels,\n                \"reward\": recent_reward,\n            }\n        )\n</code></pre> <p>Note</p> <p>You might notice that the dataset format used here differs from the format described in the SFT Trainer documentation. According to the documentation, dataset samples should be provided as plain text strings or message objects.</p> <p>As a matter of fact, this example leverages some undocumented behavior in the SFT Trainer implementation. When the dataset already includes a <code>\"input_ids\"</code> column, the Trainer automatically marks it as <code>is_processed</code> and skips the internal tokenization step.</p> <p>Since we already have spans with token IDs generated by the <code>LLMProxy</code>, providing them directly avoids unnecessary re-tokenization and related complications. This approach will both save processing time and increase consistency between training and inference.</p> <p>After aggregating every rollout we shuffle, sort by reward, and keep the top fraction (e.g., 50%) before shuffling again. The resulting list feeds directly into <code>datasets.Dataset.from_list</code>, which is the format Unsloth\u2019s SFT trainer expects.</p> <pre><code>from datasets import Dataset as HuggingFaceDataset\n\nrandom.shuffle(all_triplets)\nall_triplets.sort(key=lambda x: x[\"reward\"], reverse=True)\nsliced_triplets = all_triplets[: max(1, int(len(all_triplets) * triplet_fraction))]\n# Shuffle the sliced triplets again\nrandom.shuffle(sliced_triplets)\n\nsft_dataset = HuggingFaceDataset.from_list(sliced_triplets)\n</code></pre>"},{"location":"how-to/unsloth-sft/#launch-unsloth-training","title":"Launch Unsloth Training","text":"<p>The heavy lifting happens in <code>trl.SFTTrainer</code> (see unsloth_helper.py on how it's used). We launch it in a fresh process created with <code>multiprocessing.get_context(\"spawn\")</code> so CUDA memory is reliably reclaimed when training ends. Launching it in the same process will also work for the first iteration, but we found that the memory won't be freed properly for subsequent vLLM serving.</p> <pre><code>context = multiprocessing.get_context(\"spawn\")\nunsloth_process = context.Process(\n    target=unsloth_training,\n    args=(model_path, sft_dataset, next_model_path),\n    daemon=True,\n)\nunsloth_process.start()\nunsloth_process.join(timeout=600.0)\n</code></pre> <p>Inside the <code>unsloth_training</code> subprocess, Unsloth loads the previous checkpoint in 4-bit, applies LoRA adapters, and forwards the Hugging Face dataset to <code>trl.SFTTrainer</code> with the configuration defined in <code>SFTConfig</code> (batch size, accumulation steps, learning rate, etc.). The merged 16-bit weights are saved under <code>models/version_&lt;iteration + 1&gt;</code> so the next iteration can immediately serve them with vLLM.</p> <pre><code>from unsloth import FastLanguageModel\n# TRL is patched by unsloth.\nfrom trl import SFTConfig, SFTTrainer\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=model_path,\n    load_in_4bit=True,  # 4 bit quantization to reduce memory\n)\n\n# Config the model to use LoRA\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=32,\n    ...\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=sft_dataset,\n    ...\n)\n\n# This is the heaviest step.\ntrainer_stats = trainer.train()\n\n# Save in 16-bit for vLLM inference later\nmodel.save_pretrained_merged(next_model_path, tokenizer, save_method=\"merged_16bit\")\n</code></pre>"},{"location":"how-to/unsloth-sft/#math-agent-openai-agents-sdk-with-mcp","title":"Math Agent: OpenAI Agents SDK with MCP","text":"<p>We build an agent with the OpenAI Agents SDK to wire a calculator MCP tool and an OpenAI-compatible chat completion model together. The agent aims to solve a math problem and returns a reward indicating whether the answer is correct or not. The runner injects the <code>LLM</code> resource supplied by the algorithm side:</p> <pre><code>import os\nfrom typing import TypedDict\n\nimport agentlightning as agl\nfrom agents import Agent, ModelSettings, OpenAIChatCompletionsModel, Runner as OpenAIRunner\nfrom agents.mcp import MCPServerStdio\nfrom openai import AsyncOpenAI\n\nclass GsmProblem(TypedDict):\n    input: str\n    target: float\n\ndef compute_reward(result: str, target: float) -&gt; float:\n    ...\n\n@agl.rollout\nasync def math_agent(task: GsmProblem, llm: agl.LLM) -&gt; float:\n    async with MCPServerStdio(\n        name=\"Calculator via uvx\",\n        params={\"command\": \"uvx\", \"args\": [\"mcp-server-calculator\"]},\n    ) as server:\n        agent = Agent(\n            name=\"Assistant\",\n            instructions=(\n                \"Use the calculator tool for every question. \"\n                \"Return only the numeric answer wrapped like ### &lt;answer&gt; ###.\"\n            ),\n            mcp_servers=[server],\n            model=OpenAIChatCompletionsModel(\n                model=llm.model,\n                openai_client=AsyncOpenAI(\n                    base_url=llm.endpoint,\n                    api_key=llm.api_key or \"dummy\",\n                ),\n            ),\n            model_settings=ModelSettings(\n                temperature=llm.sampling_parameters.get(\"temperature\", 0.0),\n            ),\n        )\n        result = await OpenAIRunner.run(agent, task[\"input\"])\n    return compute_reward(result.final_output, task[\"target\"])\n</code></pre> <p>Tip</p> <p>You can test the agent with a dry run:</p> <pre><code>import asyncio\n\nllm = agl.LLM(\n    endpoint=os.environ[\"OPENAI_BASE_URL\"],\n    api_key=os.environ[\"OPENAI_API_KEY\"],\n    model=\"gpt-4.1-mini\",\n)\nasyncio.run(math_agent({\"input\": \"What is 1 + 1?\", \"target\": 2.0}, llm))\n</code></pre>"},{"location":"how-to/unsloth-sft/#run-this-recipe","title":"Run this Recipe","text":"<p>The full runnable script for this recipe resides in <code>examples/unsloth</code> folder.</p> <p>Before running this example, install <code>unsloth</code>, <code>vllm</code>, and the other libraries used in the examples (the project uses CUDA tooling, TRL, rich, datasets, etc.). We tested with <code>unsloth==2025.10.1</code>. <code>unsloth==2025.10.2</code> and <code>2025.10.3</code> are not working because of an issue we have been investigating with the unsloth team.</p> <p>It's recommended to download the base model before running the example, such that the first iteration and subsequent iterations can both load from local checkpoints.</p> <pre><code>hf download unsloth/Qwen3-4B-Instruct-2507 --local-dir models/version_0\n</code></pre> <p>The repository already contains <code>examples/unsloth/data_gsmhard.jsonl</code> (which is a very small subset of the GSM-hard math dataset for demonstration purposes).</p>"},{"location":"how-to/unsloth-sft/#run-manually","title":"Run Manually","text":"<p>Similar to the Write the First Algorithm recipe, you can open three terminals and start each component in parallel.</p> <pre><code>agl store --port 4747\npython examples/unsloth/sft_rollout_runners.py\npython examples/unsloth/sft_algorithm.py\n</code></pre> <p>In this case, <code>sft_rollout_runners.py</code> is a simple spawner implemented in Python that spawns 4 runners in parallel. The runners all connect to the same store server executing in another terminal.</p> <pre><code>import agentlightning as agl\n\ndef run_rollout(store: agl.LightningStore, worker_id: int) -&gt; None:\n    # Since the server side has already used LiteLLM proxy to collect traces,\n    # a simple OtelTracer to collect the rewards is enough.\n    tracer = agl.OtelTracer()\n\n    runner = agl.LitAgentRunner(tracer=tracer)\n\n    with runner.run_context(agent=math_agent, store=store, worker_id=worker_id):\n        asyncio.run(runner.iter())\n\n\ndef spawn_runners(store: agl.LightningStore, n_runners: int) -&gt; None:\n    runners = [\n        multiprocessing.Process(target=run_rollout, args=(store, worker_id))\n        for worker_id in range(n_runners)\n    ]\n    for runner in runners:\n        runner.start()\n\n    for runner in runners:\n        runner.join()\n\n\nstore = agl.LightningStoreClient(\"http://localhost:4747\")\nspawn_runners(store=store, n_runners=4)\n</code></pre> <p>Tip</p> <p>Try to swap <code>OtelTracer</code> in the runners with other tracers like <code>AgentOpsTracer</code>. Try to use a different adapter at the algorithm side such as <code>TracerTraceToTriplet</code> to see what happens.</p>"},{"location":"how-to/unsloth-sft/#run-everything-with-trainer","title":"Run Everything with Trainer","text":"<p>We also show how to wrap everything into a single script using <code>Trainer</code>. <code>sft_allinone.py</code> wires the same components together, replacing the manual management of runners above.</p> <pre><code>class UnslothSupervisedFinetuning(agl.Algorithm):\n\n    async def run(\n        self,\n        train_dataset: Optional[Dataset[GsmProblem]] = None,\n        val_dataset: Optional[Dataset[GsmProblem]] = None,\n    ):\n        # Use the store, llm_proxy, and adapter from the trainer\n        store = self.get_store()\n        llm_proxy = self.get_llm_proxy()\n        data_adapter = self.get_adapter()\n\n        for iteration in range(self.max_iterations):\n            ...  # Same logic as sft_algorithm.py\n\nalgo = UnslothSupervisedFinetuning(\n    max_iterations=2,\n    vllm_port=12316,\n    train_triplet_fraction=0.5,\n    initial_model_path=\"models/version_0\",\n)\n\n# The LLM proxy can be created before Trainer\ntrainer = Trainer(\n    n_runners=4,\n    algorithm=algo,\n    llm_proxy=LLMProxy(port=12358),\n)\n\ntrainer.fit(math_agent, load_math_dataset())\n</code></pre> <p>You might wonder where the initialization of <code>Adapter</code> happens in this code. It turns out that <code>TracerTraceToTriplet</code> is the default adapter in <code>Trainer</code>, so we don't need to create one manually.</p> <p>Now you can run the example with:</p> <pre><code>python examples/unsloth/sft_allinone.py\n</code></pre> <p>It starts an <code>InMemoryLighningStore</code> for you, launches four worker processes, iterates the SFT loop, and prints the final checkpoint path when done. Adjust <code>max_iterations</code>, <code>train_triplet_fraction</code>, <code>n_runners</code>, or the proxy port to match your hardware or training goals. If you already run an external store or proxy you can also pass those objects into <code>Trainer</code> instead of relying on the Trainer-managed defaults.</p> <p>Info</p> <p>As a future plan, we might graduate this example into a more powerful SFT algorithm bundled into Algorithm Zoo. Currently, this <code>UnslothSupervisedFinetuning</code> is still for demo purposes.</p>"},{"location":"how-to/write-first-algorithm/","title":"Write the First Algorithm with Agent-lightning","text":"<p>In the first tutorial, \"Train the First Agent,\" we introduced the Trainer and showed how to use a pre-built algorithm like Automatic Prompt Optimization (APO) to improve an agent's performance. The Trainer handled all the complex interactions, letting us focus on the agent's logic.</p> <p>Now, we'll go a step deeper. What if you have a unique training idea that doesn't fit a standard algorithm? This tutorial will show you how to write your own custom algorithm from scratch. We'll build a simple algorithm that systematically tests a list of prompt templates and identifies the one with the highest reward.</p> <p>By the end, you'll understand the core mechanics of how the Algorithm, Runner, and a new component, the Store, work together to create the powerful training loop at the heart of Agent-lightning.</p> <p>Tip</p> <p>This tutorial helps you build a basic understanding of how to interact with Agent-lightning's core components. It's recommended that all users customizing algorithms should read this tutorial, even for those who are not planning to do prompt optimization.</p>"},{"location":"how-to/write-first-algorithm/#core-concepts-for-training","title":"Core Concepts for Training","text":"<p>Before diving into the LightningStore, let's define two key concepts that are central to any training process in Agent-lightning: Resources and the Tracer.</p>"},{"location":"how-to/write-first-algorithm/#resources-the-tunable-assets","title":"Resources: The Tunable Assets","text":"<p>Resources are the assets your algorithm is trying to improve. Think of them as the \"recipe\" an agent uses to perform its task. This recipe can be:</p> <ul> <li>A prompt template that guides an LLM.</li> <li>The weights of a machine learning model.</li> <li>Any other configuration or data your agent needs.</li> </ul> <p>The algorithm's job is to run experiments and iteratively update these resources to find the best-performing version.</p>"},{"location":"how-to/write-first-algorithm/#tracer-the-data-collector","title":"Tracer: The Data Collector","text":"<p>How does the algorithm know if a change was an improvement? It needs data. This is where the Tracer comes in.</p> <p>The Tracer automatically instruments (aka modifies / patches) the agent's code. This means it watches for important events, like an LLM call, a tool being used, or reward signals, and records a detailed log of what happened. Each of these logs is called a Span (which has already been introduced in the last tutorial).</p> <p>A collection of spans from a single task execution gives the algorithm a complete, step-by-step trace of the agent's behavior, which is essential for learning and making improvements. Our default tracer is built on the AgentOps SDK to support instrumenting code written in various Agent/non-agent frameworks.</p>"},{"location":"how-to/write-first-algorithm/#the-central-hub-the-lightningstore","title":"The Central Hub: The LightningStore","text":"<p>Now, where do all these resources, tasks, and spans live? They are all managed by the LightningStore.</p> <p>The LightningStore acts as the central database and message queue for the entire system. It's the single source of truth that decouples the Algorithm from the Runners.</p> <p>Note</p> <p>In the last tutorial we simplified the training loop, saying the Algorithm and Agent communicate \"via the Trainer.\" That's true at a high level, but the component that makes it all possible is actually the LightningStore.</p> <ul> <li>The Algorithm connects to the Store to <code>enqueue_rollout</code> (tasks) and <code>update_resources</code> (like new prompt templates). It also queries the Store to retrieve the resulting spans and rewards from completed rollouts.</li> <li>The Runners connect to the Store to <code>dequeue_rollout</code> (polling for available tasks). After executing a task, they use the <code>Tracer</code> to write the resulting spans and status updates back to the Store.</li> </ul> <p>This architecture is key to Agent-lightning's scalability. Since the Algorithm and Runners only talk to the Store, they can run in different processes or even on different machines.</p> <p></p> <p>A Mental Model of What the Store Contains</p> <p>The LightningStore isn't just a simple database; it's an organized system for managing the entire training lifecycle. Here's what it keeps track of:</p> <ul> <li>Task Queue: A queue of pending Rollouts waiting for a Runner to pick them up, interactable via <code>enqueue_rollout</code> and <code>dequeue_rollout</code>.</li> <li>Rollouts: The record of a single task. A rollout contains metadata about the task and tracks all Attempts to complete it, interactable via <code>query_rollouts</code> and <code>wait_for_rollouts</code>.</li> <li>Attempts: A single execution of a rollout. If an attempt fails (e.g., due to a network error), the Store can automatically schedules a retry if it's configured. Each attempt is linked to its parent rollout and contains the status and timing information. The rollout status is synced with its children's status. For beginners, you can assume each rollout has only one attempt unless you have explicitly configure the retry.</li> <li>Spans: The detailed, structured logs generated by the <code>Tracer</code> during an attempt. Each span is linked to its parent attempt and rollout.</li> <li>Resources: A versioned collection of the assets (like prompt templates) that the algorithm creates. Each rollout is linked to the specific version of the resources it should use.</li> </ul>"},{"location":"how-to/write-first-algorithm/#building-a-custom-algorithm","title":"Building a Custom Algorithm","text":"<p>Let's build an algorithm that finds the best system prompt from a predefined list. The logic is straightforward:</p> <ol> <li>Start with a list of candidate prompt templates.</li> <li>For each template, create a \"resource\" bundle in the Store.</li> <li>Enqueue a rollout (a task), telling the Runner to use this specific resource.</li> <li>Wait for a Runner to pick up the task and complete it.</li> <li>Query the Store to get the final reward from the rollout's spans.</li> <li>After testing all templates, compare the rewards and declare the best one.</li> </ol> <p>We can implement this as a simple Python function that interacts directly with the LightningStore.</p> <pre><code>async def find_best_prompt(store, prompts_to_test, task_input):\n    \"\"\"A simple algorithm to find the best prompt from a list.\"\"\"\n    results = []\n\n    # Iterate through each prompt to test it\n    for prompt in prompts_to_test:\n        print(f\"[Algo] Updating prompt template to: '{prompt}'\")\n\n        # 1. Update the resources in the store with the new prompt\n        resources_update = await store.add_resources(\n            resources={\"prompt_template\": prompt}\n        )\n\n        # 2. Enqueue a rollout task for a runner to execute\n        print(\"[Algo] Queuing task for clients...\")\n        rollout = await store.enqueue_rollout(\n            input=task_input,\n            resources_id=resources_update.resources_id,\n        )\n        print(f\"[Algo] Task '{rollout.rollout_id}' is now available for clients.\")\n\n        # 3. Wait for the rollout to be completed by a runner\n        await store.wait_for_rollouts([rollout.rollout_id])\n\n        # 4. Query the completed rollout and its spans\n        completed_rollout = await store.get_rollout_by_id(rollout.rollout_id)\n        print(f\"[Algo] Received Result: {completed_rollout.model_dump_json(indent=None)}\")\n\n        spans = await store.query_spans(rollout.rollout_id)\n        # We expect at least two spans: one for the LLM call and one for the final reward\n        print(f\"[Algo] Queried Spans:\\n  - \" + \"\\n  - \".join(str(span) for span in spans))\n        # find_final_reward is a helper function to extract the reward span\n        final_reward = find_final_reward(spans)\n        print(f\"[Algo] Final reward: {final_reward}\\n\")\n\n        results.append((prompt, final_reward))\n\n    # 5. Find and print the best prompt based on the collected rewards\n    print(f\"[Algo] All prompts and their rewards: {results}\")\n    best_prompt, best_reward = max(results, key=lambda item: item[1])\n    print(f\"[Algo] Best prompt found: '{best_prompt}' with reward {best_reward}\")\n</code></pre> <p>Asynchronous Operations</p> <p>You'll notice the <code>async</code> and <code>await</code> keywords. Agent-lightning is built on asyncio to handle concurrent operations efficiently. All interactions with the store are asynchronous network calls, so they must be awaited.</p>"},{"location":"how-to/write-first-algorithm/#the-agent-and-runner","title":"The Agent and Runner","text":"<p>Our algorithm needs an agent to execute the tasks and a runner to manage the process.</p> <p>The runner is a long-lived worker process. Its job is simple:</p> <ol> <li>Connect to the LightningStore via a LightningStoreClient.</li> <li>Enter a loop, constantly asking the LightningStore for new tasks (<code>dequeue_rollout</code>).</li> <li>When it gets a task, it runs the <code>simple_agent</code> function.</li> <li>Crucially, the runner wraps the agent execution with a Tracer. The tracer automatically captures all the important events (like the LLM call and the final reward) as spans and sends them back to the LightningStore.</li> </ol> <pre><code># Connecting to Store\nstore = agl.LightningStoreClient(\"http://localhost:4747\")  # or some other address\nrunner = LitAgentRunner[str](tracer=AgentOpsTracer())\nwith runner.run_context(agent=simple_agent, store=store):  # &lt;-- where the wrapping and instrumentation happens\n    await runner.iter()  # polling for new tasks forever\n</code></pre> <p>For this example, the agent's job is to take the prompt from the resources, use it to ask an LLM a question, and return a score.</p> <pre><code>def simple_agent(task: str, prompt_template: PromptTemplate) -&gt; float:\n    \"\"\"An agent that answers a question and gets judged by an LLM.\"\"\"\n    client = OpenAI()\n\n    # Generate a response using the provided prompt template\n    prompt = prompt_template.format(any_question=task)\n    response = client.chat.completions.create(\n        model=\"gpt-4.1-nano\", messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    llm_output = response.choices[0].message.content\n    print(f\"[Rollout] LLM returned: {llm_output}\")\n\n    # This llm_output and the final score are automatically logged as spans by the Tracer\n    score = random.uniform(0, 1)  # Replace with actual scoring logic if needed\n    return score\n</code></pre>"},{"location":"how-to/write-first-algorithm/#running-the-example","title":"Running the Example","text":"<p>To see everything in action, you'll need three separate terminal windows.</p> <p>Tip</p> <p>If you want to follow along, you can find the complete code for this example in the apo_custom_algorithm.py file.</p> <p>1. Start the Store: In the first terminal, start the LightningStore server. This component will wait for connections from the algorithm and the runner. The store will be listening on port <code>4747</code> \u26a1 by default.</p> <pre><code>agl store\n</code></pre> <p>2. Start the Runner: In the second terminal, start the runner process. It will connect to the store and wait for tasks.</p> <p>The code to start the runner looks like the following:</p> <pre><code>export OPENAI_API_KEY=sk-... # Your OpenAI API key\npython apo_custom_algorithm.py runner\n</code></pre> <p>You will see output indicating the runner has started and is waiting for rollouts.</p> <pre><code>2025-10-14 22:23:41,339 [INFO] ... [Worker 0] Setting up tracer...\n2025-10-14 22:23:41,343 [INFO] ... [Worker 0] Instrumentation applied.\n2025-10-14 22:23:41,494 [INFO] ... [Worker 0] AgentOps client initialized.\n2025-10-14 22:23:41,494 [INFO] ... [Worker 0] Started async rollouts (max: unlimited).\n</code></pre> <p>3. Start the Algorithm: In the third terminal, run the algorithm. This will kick off the entire process.</p> <p>For example, we run the algorithm code shown above with the following parameters:</p> <pre><code>prompts_to_test = [\n    \"You are a helpful assistant. {any_question}\",\n    \"You are a knowledgeable AI. {any_question}\",\n    \"You are a friendly chatbot. {any_question}\",\n]\ntask_input = \"Why is the sky blue?\"\nstore = agl.LightningStoreClient(\"http://localhost:4747\")\nfind_best_prompt(store, prompts_to_test, task_input)\n</code></pre> <p>Or you can simply use our pre-written script to try out:</p> <pre><code>python apo_custom_algorithm.py algo\n</code></pre>"},{"location":"how-to/write-first-algorithm/#understanding-the-output","title":"Understanding the Output","text":"<p>As the algorithm runs, you'll see logs appear across all three terminals, showing the components interacting in real-time.</p> <p>Algorithm Output: The algorithm terminal shows the main control flow: updating prompts, queuing tasks, and receiving the final results. You can also see the raw span data it retrieves from the store.</p> <pre><code>[Algo] Updating prompt template to: 'You are a helpful assistant. {any_question}'\n[Algo] Queuing task for clients...\n[Algo] Task 'ro-1d18988581cd' is now available for clients.\n[Algo] Received Result: rollout_id='ro-1d18988581cd' ... status='succeeded' ...\n[Algo] Queried Spans:\n  - Span(name='openai.chat.completion', attributes={'gen_ai.prompt.0.content': 'You are a helpful assistant...', 'gen_ai.completion.0.content': 'The sky appears blue...'})\n  - Span(name='reward', attributes={'value': 0.95})\n[Algo] Final reward: 0.95\n\n[Algo] Updating prompt template to: 'You are a knowledgeable AI. {any_question}'\n...\n[Algo] Final reward: 0.95\n\n[Algo] Updating prompt template to: 'You are a friendly chatbot. {any_question}'\n...\n[Algo] Final reward: 1.0\n\n[Algo] All prompts and their rewards: [('You are a helpful assistant. {any_question}', 0.95), ('You are a knowledgeable AI. {any_question}', 0.95), ('You are a friendly chatbot. {any_question}', 1.0)]\n[Algo] Best prompt found: 'You are a friendly chatbot. {any_question}' with reward 1.0\n</code></pre> <p>Runner Output: The runner terminal shows it picking up each task, executing the agent logic, and reporting the completion.</p> <pre><code>[Rollout] LLM returned: The sky appears blue due to Rayleigh scattering...\n2025-10-14 22:25:50,803 [INFO] ... [Worker 0 | Rollout ro-a9f54ac19af5] Completed in 4.24s. ...\n\n[Rollout] LLM returned: The sky looks blue because of a process called Rayleigh scattering...\n2025-10-14 22:25:59,863 [INFO] ... [Worker 0 | Rollout ro-c67eaa9016b6] Completed in 4.06s. ...\n</code></pre> <p>Store Server Output: The store terminal shows a detailed log of every interaction, confirming its role as the central hub. You can see requests to enqueue and dequeue rollouts, add spans, and update statuses.</p> <pre><code>... \"POST /enqueue_rollout HTTP/1.1\" 200 ...\n... \"GET /dequeue_rollout HTTP/1.1\" 200 ...\n... \"POST /add_span HTTP/1.1\" 200 ...\n... \"POST /update_attempt HTTP/1.1\" 200 ...\n... \"POST /wait_for_rollouts HTTP/1.1\" 200 ...\n... \"GET /query_spans/ro-c67eaa9016b6 HTTP/1.1\" 200 ...\n</code></pre> <p>So Where is Trainer?</p> <p>You might be wondering why the last tutorial focused on the Trainer class, but we haven't used it here.</p> <p>Think of the Trainer as a convenient wrapper that manages the entire training process for you. It's perfect when you want to apply a pre-built algorithm to your agent without worrying about the underlying mechanics. The Trainer handles starting the LightningStore, coordinating the Runners, managing their lifecycles, and handling errors.</p> <p>In this tutorial, however, our goal is to build a new algorithm. To do that, we need to interact directly with the core components: the Store, the Runner, and the algorithm logic itself. Running them separately gives you more control and clearer, isolated logs, which is ideal for development and debugging.</p> <p>Once your custom algorithm is mature, you can package it to comply with our standard interface (@algo or Algorithm). This allows you to use it with the Trainer again, getting all the benefits of automated lifecycle management while using your own custom logic. A sample code doing this is available in apo_custom_algorithm_trainer.py.</p>"},{"location":"reference/agent/","title":"Agent Developer APIs","text":""},{"location":"reference/agent/#agent-decorators","title":"Agent Decorators","text":"<p>Tip</p> <p>These are convenient helpers for creating agents from functions. First-time users are recommended to use these decorators to create agents.</p> <p>Warning</p> <p>The following two decorators are implementations of <code>agentlightning.rollout</code>. They are not recommended for new users.</p>"},{"location":"reference/agent/#agentlightning.rollout","title":"<code>agentlightning.rollout(func)</code>","text":"<p>Create a <code>FunctionalLitAgent</code> from an arbitrary rollout function.</p> <p>This function inspects the provided callable and creates the appropriate agent type based on its signature. It supports both LLM-based and prompt-template-based agents. The returned agent instance is callable, preserving the original function's behavior and type hints.</p> <p>See <code>llm_rollout</code> and <code>prompt_rollout</code> for more details.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>Union[LlmRolloutFunc[T], PromptRolloutFunc[T], Callable[..., Any]]</code>)           \u2013            <p>Callable that implements the rollout. Supported signatures:</p> <ul> <li><code>[async ](task, llm[, rollout])</code> for LLM-based agents</li> <li><code>[async ](task, prompt_template[, rollout])</code> for prompt-template-based agents</li> </ul> <p>The supported output types of <code>func</code> is same as the return type of <code>rollout</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FunctionalLitAgent[T]</code>           \u2013            <p><code>FunctionalLitAgent</code> that</p> </li> <li> <code>FunctionalLitAgent[T]</code>           \u2013            <p>wraps the supplied function.</p> </li> </ul> <p>Examples:</p> <pre><code># LLM-based agent\n@rollout\ndef my_llm_agent(task, llm):\n    client = OpenAI(base_url=llm.endpoint)\n    response = client.chat.completions.create(\n        model=llm.model,\n        messages=[{\"role\": \"user\", \"content\": task.input}],\n    )\n    return response\n\n# Prompt-template-based agent\n@rollout\ndef my_prompt_agent(task, prompt_template):\n    messages = prompt_template.format(task=task.input)\n    # ... perform rollout with the formatted prompt\n    return response\n\n# Function is still callable with original behavior\nresult = my_llm_agent(task, llm)\n\n# Agent methods are also available\nresult = my_llm_agent.rollout(task, resources, rollout)\n</code></pre> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>If the function signature doesn't match any known patterns.</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.llm_rollout","title":"<code>agentlightning.llm_rollout(func=None, *, strip_proxy=True)</code>","text":"<pre><code>llm_rollout(\n    func: LlmRolloutFunc[T],\n) -&gt; FunctionalLitAgent[T]\n</code></pre><pre><code>llm_rollout(\n    *, strip_proxy: bool = True\n) -&gt; Callable[[LlmRolloutFunc[T]], FunctionalLitAgent[T]]\n</code></pre> <p>Create a <code>FunctionalLitAgent</code> for LLM-based rollouts.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>LlmRolloutFunc[T] | None</code>, default:                   <code>None</code> )           \u2013            <p>Callable defining the agent's behaviour. Supported signatures include:</p> <ul> <li><code>(task, llm) -&gt; result</code></li> <li><code>(task, llm, rollout) -&gt; result</code></li> <li><code>async (task, llm) -&gt; result</code></li> <li><code>async (task, llm, rollout) -&gt; result</code></li> </ul> </li> <li> <code>strip_proxy</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>When <code>True</code>, convert proxy resources into concrete <code>LLM</code> instances before calling the function. Defaults to <code>True</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FunctionalLitAgent[T] | Callable[[LlmRolloutFunc[T]], FunctionalLitAgent[T]]</code>           \u2013            <p><code>FunctionalLitAgent</code> that</p> </li> <li> <code>FunctionalLitAgent[T] | Callable[[LlmRolloutFunc[T]], FunctionalLitAgent[T]]</code>           \u2013            <p>wraps the supplied function.</p> </li> </ul> <p>Examples:</p> <pre><code>@llm_rollout\ndef my_agent(task, llm):\n    return llm.endpoint\n\n@llm_rollout(strip_proxy=False)\ndef my_agent_no_strip(task, llm):\n    return llm.model\n\nresult = my_agent(task, llm)\nresult = my_agent.rollout(task, resources, rollout)\n</code></pre>"},{"location":"reference/agent/#agentlightning.prompt_rollout","title":"<code>agentlightning.prompt_rollout(func=None)</code>","text":"<pre><code>prompt_rollout(\n    func: PromptRolloutFunc[T],\n) -&gt; FunctionalLitAgent[T]\n</code></pre><pre><code>prompt_rollout() -&gt; (\n    Callable[[PromptRolloutFunc[T]], FunctionalLitAgent[T]]\n)\n</code></pre> <p>Create a <code>FunctionalLitAgent</code> for prompt-based rollouts.</p> <p>This decorator is designed for agents that work with tunable prompt templates. It enables a workflow where algorithms manage and optimize the prompt template, while agents consume the template to perform rollouts. This is particularly useful for prompt optimization scenarios.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>PromptRolloutFunc[T] | None</code>, default:                   <code>None</code> )           \u2013            <p>Callable defining the agent's behavior. Supported signatures include:</p> <ul> <li><code>(task, prompt_template) -&gt; result</code></li> <li><code>(task, prompt_template, rollout) -&gt; result</code></li> <li><code>async (task, prompt_template) -&gt; result</code></li> <li><code>async (task, prompt_template, rollout) -&gt; result</code></li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>FunctionalLitAgent[T] | Callable[[PromptRolloutFunc[T]], FunctionalLitAgent[T]]</code>           \u2013            <p><code>FunctionalLitAgent</code> that</p> </li> <li> <code>FunctionalLitAgent[T] | Callable[[PromptRolloutFunc[T]], FunctionalLitAgent[T]]</code>           \u2013            <p>wraps the supplied function.</p> </li> </ul> <p>Examples:</p> <pre><code>@prompt_rollout\ndef my_agent(task, prompt_template):\n    messages = prompt_template.format(task=task.input)\n    return messages\n\nresult = my_agent(task, prompt_template)\nresult = my_agent.rollout(task, resources, rollout)\n</code></pre>"},{"location":"reference/agent/#class-based-agents","title":"Class-based Agents","text":""},{"location":"reference/agent/#agentlightning.LitAgent","title":"<code>agentlightning.LitAgent</code>","text":"<p>               Bases: <code>Generic[T]</code></p> <p>Base class for implementing agent rollouts.</p> <p>Subclasses override the rollout methods to process tasks while the trainer and runner infrastructure manages orchestration, tracing, and persistence.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.runner","title":"<code>runner</code>  <code>property</code>","text":"<p>Return the runner responsible for executing rollouts.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.tracer","title":"<code>tracer</code>  <code>property</code>","text":"<p>Return the tracer configured for this agent.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.trainer","title":"<code>trainer</code>  <code>property</code>","text":"<p>Return the trainer associated with this agent.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.__init__","title":"<code>__init__(*, trained_agents=None)</code>","text":"<p>Initialize the agent instance.</p> <p>Parameters:</p> <ul> <li> <code>trained_agents</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional identifier used by legacy tooling to mark trained agents.</p> </li> </ul> <p>Deprecated</p> <p>The <code>trained_agents</code> flag is deprecated. Configure <code>agent_match</code> in the adapter layer instead. See <code>TracerTraceToTriplet</code> for more details.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.get_runner","title":"<code>get_runner()</code>","text":"<p>Return the runner responsible for executing rollouts.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.get_tracer","title":"<code>get_tracer()</code>","text":"<p>Return the tracer configured for this agent.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.get_trainer","title":"<code>get_trainer()</code>","text":"<p>Return the trainer associated with this agent.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.is_async","title":"<code>is_async()</code>","text":"<p>Return <code>True</code> when the agent overrides any asynchronous rollout methods.</p> <p>Override this method for customized async detection logic.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.on_rollout_end","title":"<code>on_rollout_end(task, rollout, runner, tracer)</code>","text":"<p>Hook invoked after a rollout completes.</p> <p>Subclasses can override this method for cleanup or additional logging. The default implementation is a no-op.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>Task</code>)           \u2013            <p><code>Task</code> that was processed.</p> </li> <li> <code>rollout</code>               (<code>Rollout</code>)           \u2013            <p>Resulting <code>Rollout</code>.</p> </li> <li> <code>runner</code>               (<code>Runner[T]</code>)           \u2013            <p><code>Runner</code> managing the rollout.</p> </li> <li> <code>tracer</code>               (<code>Tracer</code>)           \u2013            <p><code>Tracer</code> associated with the runner.</p> </li> </ul> <p>Deprecated</p> <p>Override <code>Hook.on_rollout_end</code> instead of this method when extending agents.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.on_rollout_start","title":"<code>on_rollout_start(task, runner, tracer)</code>","text":"<p>Hook invoked immediately before a rollout begins.</p> <p>Subclasses can override this method to implement custom logic such as logging, metric collection, or resource setup. The default implementation is a no-op.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>Task</code>)           \u2013            <p><code>Task</code> that will be processed.</p> </li> <li> <code>runner</code>               (<code>Runner[T]</code>)           \u2013            <p><code>Runner</code> managing the rollout.</p> </li> <li> <code>tracer</code>               (<code>Tracer</code>)           \u2013            <p><code>Tracer</code> associated with the runner.</p> </li> </ul> <p>Deprecated</p> <p>Override <code>Hook.on_rollout_start</code> instead of this method when extending agents.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.rollout","title":"<code>rollout(task, resources, rollout)</code>","text":"<p>Execute a rollout synchronously.</p> <p>If you don't wish to implement both training rollout and validation rollout separately, you can just implement <code>rollout</code> which will work for both.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>T</code>)           \u2013            <p>Task payload provided by the scheduler.</p> </li> <li> <code>resources</code>               (<code>NamedResources</code>)           \u2013            <p>Mapping of named resources (for example LLMs or prompt templates).</p> </li> <li> <code>rollout</code>               (<code>Rollout</code>)           \u2013            <p>Rollout metadata. Avoid mutating this object directly unless a subclass needs to override defaults.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RolloutRawResult</code>           \u2013            <p>One of the following values:</p> </li> <li> <code>RolloutRawResult</code>           \u2013            <ul> <li><code>None</code> when tracing is handled by the runner.</li> </ul> </li> <li> <code>RolloutRawResult</code>           \u2013            <ul> <li><code>float</code> representing the final reward.</li> </ul> </li> <li> <code>RolloutRawResult</code>           \u2013            <ul> <li><code>List[ReadableSpan]</code> with OpenTelemetry spans.</li> </ul> </li> <li> <code>RolloutRawResult</code>           \u2013            <ul> <li><code>List[Span]</code> with Agent Lightning spans.</li> </ul> </li> <li> <code>RolloutRawResult</code>           \u2013            <ul> <li><code>List[SpanCoreFields]</code> with Agent Lightning spans.</li> </ul> </li> </ul>"},{"location":"reference/agent/#agentlightning.LitAgent.rollout_async","title":"<code>rollout_async(task, resources, rollout)</code>  <code>async</code>","text":"<p>Execute a rollout asynchronously.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>T</code>)           \u2013            <p>Task payload provided by the scheduler.</p> </li> <li> <code>resources</code>               (<code>NamedResources</code>)           \u2013            <p>Mapping of named resources (for example LLMs or prompt templates).</p> </li> <li> <code>rollout</code>               (<code>Rollout</code>)           \u2013            <p>Rollout metadata. Avoid mutating this object directly unless a subclass needs to override defaults.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RolloutRawResult</code>           \u2013            <p>Same possible return values as</p> </li> <li> <code>RolloutRawResult</code>           \u2013            <p><code>rollout</code>.</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.LitAgent.set_runner","title":"<code>set_runner(runner)</code>","text":"<p>Attach the runner responsible for executing rollouts.</p> <p>Parameters:</p> <ul> <li> <code>runner</code>               (<code>Runner[T]</code>)           \u2013            <p><code>Runner</code> coordinating execution.</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.LitAgent.set_trainer","title":"<code>set_trainer(trainer)</code>","text":"<p>Attach the trainer responsible for orchestration.</p> <p>Parameters:</p> <ul> <li> <code>trainer</code>               (<code>Trainer</code>)           \u2013            <p><code>Trainer</code> that manages the agent.</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.LitAgent.training_rollout","title":"<code>training_rollout(task, resources, rollout)</code>","text":"<p>Process a single training task synchronously.</p> <p>By default, this method delegates to <code>rollout</code>.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.training_rollout_async","title":"<code>training_rollout_async(task, resources, rollout)</code>  <code>async</code>","text":"<p>Process a single training task asynchronously.</p> <p>By default, this method delegates to <code>rollout_async</code>.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.validation_rollout","title":"<code>validation_rollout(task, resources, rollout)</code>","text":"<p>Process a single validation task synchronously.</p> <p>Override this method when validation should differ from training. The default implementation delegates to <code>training_rollout</code>.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.validation_rollout_async","title":"<code>validation_rollout_async(task, resources, rollout)</code>  <code>async</code>","text":"<p>Process a single validation task asynchronously.</p> <p>Override this method when validation should differ from training. The default implementation delegates to <code>training_rollout_async</code>.</p>"},{"location":"reference/agent/#emitter","title":"Emitter","text":""},{"location":"reference/agent/#agentlightning.operation","title":"<code>agentlightning.operation(fn=None, *, propagate=True, name=None, **additional_attributes)</code>","text":"<pre><code>operation(\n    fn: _FnType,\n    *,\n    propagate: bool = True,\n    name: Optional[str] = None,\n    **additional_attributes: Any\n) -&gt; _FnType\n</code></pre><pre><code>operation(\n    *,\n    propagate: bool = True,\n    name: Optional[str] = None,\n    **additional_attributes: Any\n) -&gt; OperationContext\n</code></pre><pre><code>operation(\n    fn: _FnType,\n    *,\n    name: Optional[str] = None,\n    **additional_attributes: Any\n) -&gt; _FnType\n</code></pre><pre><code>operation(\n    *,\n    name: Optional[str] = None,\n    **additional_attributes: Any\n) -&gt; OperationContext\n</code></pre><pre><code>operation(\n    fn: _FnType, **additional_attributes: Any\n) -&gt; _FnType\n</code></pre><pre><code>operation(**additional_attributes: Any) -&gt; OperationContext\n</code></pre> <p>Entry point for tracking operations.</p> <p>This helper can be used either as a decorator or as a context manager. The span name is fixed to <code>AGL_OPERATION</code>; custom span names are not supported. Any keyword arguments are recorded as span attributes.</p> <p>Usage as a decorator:</p> <pre><code>@operation\ndef func(...):\n    ...\n\n@operation(category=\"compute\")\ndef func(...):\n    ...\n</code></pre> <p>Usage as a context manager:</p> <pre><code>with operation(user_id=123) as op:\n    op.set_input(data=data)\n    # ... do work ...\n    op.set_output(result)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>fn</code>               (<code>Optional[_FnType]</code>, default:                   <code>None</code> )           \u2013            <p>When used as <code>@operation</code>, this is the wrapped function. When used as <code>operation(**attrs)</code>, this should be omitted (or left as <code>None</code>) and only keyword attributes are provided.</p> </li> <li> <code>propagate</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether spans should use the active span processor. When False, spans will stay local and not be exported.</p> </li> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional alias that populates <code>LightningSpanAttributes.OPERATION_NAME</code> when <code>additional_attributes</code> does not already define it.</p> </li> <li> <code>**additional_attributes</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional span attributes to attach at creation time.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[_FnType, OperationContext]</code>           \u2013            <p>Either a wrapped callable (when used as a decorator) or an</p> </li> <li> <code>Union[_FnType, OperationContext]</code>           \u2013            <p><code>OperationContext</code></p> </li> <li> <code>Union[_FnType, OperationContext]</code>           \u2013            <p>(when used as a context manager factory).</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.emit_annotation","title":"<code>agentlightning.emit_annotation(annotation, propagate=True)</code>","text":"<p>Emit a new annotation span.</p> <p>This is the underlying implementation of <code>emit_reward</code>.</p> <p>Annotation spans are used to annotate a specific event or a part of rollout. See semconv for conventional annotation keys in Agent-lightning.</p> <p>If annotations contain nested dicts, they will be flattened before emitting. Complex objects will lead to emitting failures.</p> <p>Parameters:</p> <ul> <li> <code>annotation</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Dictionary containing annotation key-value pairs. Representatives are rewards, tags, and metadata.</p> </li> <li> <code>propagate</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to propagate the span to tracers automatically.</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.emit_reward","title":"<code>agentlightning.emit_reward(reward, *, primary_key=None, attributes=None, propagate=True)</code>","text":"<p>Emit a reward value as an OpenTelemetry span.</p> <p>Examples:</p> <p>Emit a single-dimensional reward:</p> <pre><code>&gt;&gt;&gt; emit_reward(1.0)\n</code></pre> <p>Emit multi-dimensional rewards:</p> <pre><code>&gt;&gt;&gt; emit_reward({\"task_completion\": 1.0, \"efficiency\": 0.8}, primary_key=\"task_completion\")\n</code></pre> <p>Emit a reward with additional attributes (for example linking to another response span):</p> <pre><code>&gt;&gt;&gt; from agentlightning.utils.otel import make_link_attributes\n&gt;&gt;&gt; emit_reward(0.5, attributes=make_link_attributes({\"gen_ai.response.id\": \"response-123\"}))\n</code></pre> <p>Or adding tags onto the reward span:</p> <pre><code>&gt;&gt;&gt; from agentlightning.utils.otel import make_tag_attributes\n&gt;&gt;&gt; emit_reward(0.7, attributes=make_tag_attributes([\"fast\", \"reliable\"]))\n</code></pre> <p>Parameters:</p> <ul> <li> <code>reward</code>               (<code>float | Dict[str, Any]</code>)           \u2013            <p>Numeric reward to record. Integers and booleans are converted to floating point numbers for consistency. Use a dictionary to represent a multi-dimensional reward.</p> </li> <li> <code>attributes</code>               (<code>Dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Other optional span attributes.</p> </li> <li> <code>propagate</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to propagate the span to exporters automatically.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SpanCoreFields</code>           \u2013            <p>Span core fields capturing the recorded reward.</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.emit_message","title":"<code>agentlightning.emit_message(message, attributes=None, propagate=True)</code>","text":"<p>Emit a textual message as an OpenTelemetry span.</p> <p>Commonly used for sending debugging and logging messages.</p> <p>Parameters:</p> <ul> <li> <code>message</code>               (<code>str</code>)           \u2013            <p>Human readable message to attach as a span attribute.</p> </li> <li> <code>attributes</code>               (<code>Optional[Dict[str, Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Additional attributes to attach to the message span.</p> </li> <li> <code>propagate</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to propagate the span to exporters automatically.</p> </li> </ul> <p>Note</p> <p>OpenTelemetry distinguishes between logs and spans. Emitting the message as a span keeps all Agent Lightning telemetry in a single data store for analysis.</p>"},{"location":"reference/agent/#agentlightning.emit_object","title":"<code>agentlightning.emit_object(object, attributes=None, propagate=True)</code>","text":"<p>Emit an object's serialized representation as an OpenTelemetry span.</p> <p>Parameters:</p> <ul> <li> <code>object</code>               (<code>Any</code>)           \u2013            <p>Data structure to encode as JSON and attach to the span payload.</p> </li> <li> <code>attributes</code>               (<code>Optional[Dict[str, Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Additional attributes to attach to the object span.</p> </li> <li> <code>propagate</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to propagate the span to exporters automatically.</p> </li> </ul> <p>Note</p> <p>The payload must be JSON serializable. Non-serializable objects will lead to a RuntimeError.</p>"},{"location":"reference/agent/#agentlightning.emit_exception","title":"<code>agentlightning.emit_exception(exception, attributes=None, propagate=True)</code>","text":"<p>Record an exception with OpenTelemetry metadata.</p> <p>Classic OpenTelemetry records exceptions in a dedicated logging service. We simplify the model and use trace spans to record exceptions as well.</p> <p>Parameters:</p> <ul> <li> <code>exception</code>               (<code>BaseException</code>)           \u2013            <p>Raised exception instance to serialize into telemetry attributes.</p> </li> <li> <code>attributes</code>               (<code>Optional[Dict[str, Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Additional attributes to attach to the exception span.</p> </li> <li> <code>propagate</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to propagate the span to exporters automatically.</p> </li> </ul> <p>Note</p> <p>The helper validates its input. If a non-exception value is provided, a TypeError is raised to indicate a programming mistake.</p>"},{"location":"reference/agent/#emitter-helpers","title":"Emitter Helpers","text":""},{"location":"reference/agent/#agentlightning.get_message_value","title":"<code>agentlightning.get_message_value(span)</code>","text":"<p>Extract the message string from a message span.</p> <p>Parameters:</p> <ul> <li> <code>span</code>               (<code>SpanLike</code>)           \u2013            <p>Span-like object to extract the message from.</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.get_object_value","title":"<code>agentlightning.get_object_value(span)</code>","text":"<p>Extract the object payload from an object span.</p> <p>Parameters:</p> <ul> <li> <code>span</code>               (<code>SpanLike</code>)           \u2013            <p>Span object produced by Agent Lightning emitters.</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.find_final_reward","title":"<code>agentlightning.find_final_reward(spans)</code>","text":"<p>Return the last reward value present in the provided spans.</p> <p>Parameters:</p> <ul> <li> <code>spans</code>               (<code>Sequence[SpanLike]</code>)           \u2013            <p>Sequence containing <code>ReadableSpan</code> objects or mocked span-like values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[float]</code>           \u2013            <p>Reward value from the latest reward span, or <code>None</code> when none are found.</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.find_reward_spans","title":"<code>agentlightning.find_reward_spans(spans)</code>","text":"<p>Return all reward spans in the provided sequence.</p> <p>Parameters:</p> <ul> <li> <code>spans</code>               (<code>Sequence[SpanLike]</code>)           \u2013            <p>Sequence containing <code>ReadableSpan</code> objects or mocked span-like values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[SpanLike]</code>           \u2013            <p>List of spans that could be parsed as rewards.</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.get_reward_value","title":"<code>agentlightning.get_reward_value(span)</code>","text":"<p>Extract the reward value from a span, if available.</p> <p>Parameters:</p> <ul> <li> <code>span</code>               (<code>SpanLike</code>)           \u2013            <p>Span object produced by AgentOps or Agent Lightning emitters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[float]</code>           \u2013            <p>The primary reward encoded in the span or <code>None</code> when the span does not represent a reward.</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.get_rewards_from_span","title":"<code>agentlightning.get_rewards_from_span(span)</code>","text":"<p>Extract the reward as a list from a span, if available.</p> <p>Parameters:</p> <ul> <li> <code>span</code>               (<code>SpanLike</code>)           \u2013            <p>Span object produced by AgentOps or Agent Lightning emitters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[RewardPydanticModel]</code>           \u2013            <p>A list of reward dimensions encoded in the span or an empty list when the span does not represent a reward.</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.is_reward_span","title":"<code>agentlightning.is_reward_span(span)</code>","text":"<p>Return <code>True</code> when the provided span encodes a reward value.</p>"},{"location":"reference/algorithm/","title":"Algorithm","text":""},{"location":"reference/algorithm/#algorithm-side-references","title":"Algorithm-side References","text":"<p>Note</p> <p>This reference covers APIs that are designed to be used at \"Algorithm Side\". For built-in algorithms, see Algorithm Zoo.</p>"},{"location":"reference/algorithm/#base-class-and-decorators","title":"Base Class and Decorators","text":""},{"location":"reference/algorithm/#agentlightning.Algorithm","title":"<code>agentlightning.Algorithm</code>","text":"<p>Algorithm is the strategy, or tuner to train the agent.</p>"},{"location":"reference/algorithm/#agentlightning.Algorithm.get_adapter","title":"<code>get_adapter()</code>","text":"<p>Retrieve the adapter for this algorithm to communicate with the runners.</p>"},{"location":"reference/algorithm/#agentlightning.Algorithm.get_client","title":"<code>get_client()</code>","text":"<p>Get the client to communicate with the algorithm.</p> <p>If the algorithm does not require a server-client communication, it can also create a mock client that never communicates with itself.</p> <p>Deprecated and will be removed in a future version.</p> <p>Returns:</p> <ul> <li> <code>AgentLightningClient</code>           \u2013            <p>The AgentLightningClient instance associated with this algorithm.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.Algorithm.get_initial_resources","title":"<code>get_initial_resources()</code>","text":"<p>Get the initial resources for this algorithm.</p>"},{"location":"reference/algorithm/#agentlightning.Algorithm.get_llm_proxy","title":"<code>get_llm_proxy()</code>","text":"<p>Retrieve the configured LLM proxy instance, if one has been set.</p> <p>Returns:</p> <ul> <li> <code>Optional[LLMProxy]</code>           \u2013            <p>The active LLMProxy instance or None when not configured.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.Algorithm.get_store","title":"<code>get_store()</code>","text":"<p>Retrieve the store for this algorithm to communicate with the runners.</p>"},{"location":"reference/algorithm/#agentlightning.Algorithm.get_trainer","title":"<code>get_trainer()</code>","text":"<p>Get the trainer for this algorithm.</p> <p>Returns:</p> <ul> <li> <code>Trainer</code>           \u2013            <p>The Trainer instance associated with this agent.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.Algorithm.is_async","title":"<code>is_async()</code>","text":"<p>Return True if the algorithm is asynchronous.</p>"},{"location":"reference/algorithm/#agentlightning.Algorithm.run","title":"<code>run(train_dataset=None, val_dataset=None)</code>","text":"<p>Subclasses should implement this method to implement the algorithm.</p> <p>Parameters:</p> <ul> <li> <code>train_dataset</code>               (<code>Optional[Dataset[Any]]</code>, default:                   <code>None</code> )           \u2013            <p>The dataset to train on. Not all algorithms require a training dataset.</p> </li> <li> <code>val_dataset</code>               (<code>Optional[Dataset[Any]]</code>, default:                   <code>None</code> )           \u2013            <p>The dataset to validate on. Not all algorithms require a validation dataset.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[None, Awaitable[None]]</code>           \u2013            <p>Algorithm should refrain from returning anything. It should just run the algorithm.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.Algorithm.set_adapter","title":"<code>set_adapter(adapter)</code>","text":"<p>Set the adapter for this algorithm to collect and convert traces.</p>"},{"location":"reference/algorithm/#agentlightning.Algorithm.set_initial_resources","title":"<code>set_initial_resources(resources)</code>","text":"<p>Set the initial resources for this algorithm.</p>"},{"location":"reference/algorithm/#agentlightning.Algorithm.set_llm_proxy","title":"<code>set_llm_proxy(llm_proxy)</code>","text":"<p>Set the LLM proxy for this algorithm to reuse when available.</p> <p>Parameters:</p> <ul> <li> <code>llm_proxy</code>               (<code>LLMProxy | None</code>)           \u2013            <p>The LLMProxy instance configured by the trainer, if any.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.Algorithm.set_store","title":"<code>set_store(store)</code>","text":"<p>Set the store for this algorithm to communicate with the runners.</p> <p>Store is set directly instead of using weakref because its copy is meant to be maintained throughout the algorithm's lifecycle.</p>"},{"location":"reference/algorithm/#agentlightning.Algorithm.set_trainer","title":"<code>set_trainer(trainer)</code>","text":"<p>Set the trainer for this algorithm.</p> <p>Parameters:</p> <ul> <li> <code>trainer</code>               (<code>Trainer</code>)           \u2013            <p>The Trainer instance that will handle training and validation.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.algo","title":"<code>agentlightning.algo(func)</code>","text":"<pre><code>algo(\n    func: AlgorithmFuncAsync,\n) -&gt; FunctionalAlgorithm[Literal[True]]\n</code></pre><pre><code>algo(\n    func: AlgorithmFuncAsyncFallback,\n) -&gt; FunctionalAlgorithm[Any]\n</code></pre><pre><code>algo(\n    func: AlgorithmFuncSync,\n) -&gt; FunctionalAlgorithm[Literal[False]]\n</code></pre><pre><code>algo(\n    func: AlgorithmFuncSyncFallback,\n) -&gt; FunctionalAlgorithm[Any]\n</code></pre> <p>Convert a callable into a <code>FunctionalAlgorithm</code>.</p> <p>The decorator inspects the callable signature to decide which dependencies to inject at runtime, enabling concise algorithm definitions that still leverage the full training runtime.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>Union[AlgorithmFuncSync, AlgorithmFuncAsync, AlgorithmFuncSyncFallback, AlgorithmFuncAsyncFallback]</code>)           \u2013            <p>Function implementing the algorithm logic. May be synchronous or asynchronous. The function can expect all of, or a subset of the following parameters:</p> <ul> <li><code>store</code>: <code>LightningStore</code>,</li> <li><code>train_dataset</code>: <code>Dataset</code>,</li> <li><code>val_dataset</code>: <code>Dataset</code>,</li> <li><code>llm_proxy</code>: <code>LLMProxy</code>,</li> <li><code>adapter</code>: <code>TraceAdapter</code>,</li> <li><code>initial_resources</code>: <code>NamedResources</code>,</li> </ul> <p>If the function does not expect a parameter, the wrapper will not inject it into the call. Using <code>*args</code> and <code>**kwargs</code> will not work and no parameters will be injected.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[FunctionalAlgorithm[Literal[False]], FunctionalAlgorithm[Literal[True]]]</code>           \u2013            <p>FunctionalAlgorithm that proxies the callable while exposing the</p> </li> <li> <code>Union[FunctionalAlgorithm[Literal[False]], FunctionalAlgorithm[Literal[True]]]</code>           \u2013            <p><code>Algorithm</code> interface.</p> </li> </ul> <p>Examples:</p> <pre><code>from agentlightning.algorithm.decorator import algo\n\n@algo\ndef batching_algorithm(*, store, train_dataset, val_dataset):\n    for sample in train_dataset:\n        store.enqueue_rollout(input=sample, mode=\"train\")\n\n@algo\nasync def async_algorithm(*, store, train_dataset=None, val_dataset=None):\n    await store.enqueue_rollout(input={\"prompt\": \"hello\"}, mode=\"train\")\n</code></pre>"},{"location":"reference/algorithm/#fast-algorithms-for-debugging","title":"Fast Algorithms (for Debugging)","text":""},{"location":"reference/algorithm/#agentlightning.FastAlgorithm","title":"<code>agentlightning.FastAlgorithm</code>","text":"<p>               Bases: <code>Algorithm</code></p> <p>Base class for lightweight algorithms optimised for developer workflows.</p> <p>Fast algorithms prioritise short feedback loops so an agent developer can run small-scale experiments without waiting for long-running training jobs to finish.</p>"},{"location":"reference/algorithm/#agentlightning.Baseline","title":"<code>agentlightning.Baseline</code>","text":"<p>               Bases: <code>FastAlgorithm</code></p> <p>Reference implementation that streams the full dataset through the rollout queue.</p> <p>The baseline algorithm batches task submissions, waits for each rollout to finish, and logs every collected span and reward. It is primarily useful as a smoke test for the platform plumbing rather than a performant trainer.</p> <p>The baseline algorithm will auto-start a LLM proxy if one is provided and not yet started.</p> <p>Parameters:</p> <ul> <li> <code>n_epochs</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of dataset passes to execute for both the train and val splits during developer experiments.</p> </li> <li> <code>train_split</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Fraction of the concatenated dataset to treat as training data. Must be strictly between 0 and 1.</p> </li> <li> <code>polling_interval</code>               (<code>float</code>, default:                   <code>5.0</code> )           \u2013            <p>Interval, in seconds, to poll the store for queue depth and rollout completion.</p> </li> <li> <code>max_queue_length</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of rollouts allowed to wait in the queue before throttling additional submissions.</p> </li> <li> <code>span_verbosity</code>               (<code>Literal['keys', 'key_values', 'none']</code>, default:                   <code>'keys'</code> )           \u2013            <p>Level of detail to include when logging span metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>train_split</code> falls outside the <code>(0, 1)</code> interval.</p> </li> </ul> <p>Examples:</p> <pre><code>from agentlightning.algorithm.fast import Baseline\n\nalgorithm = Baseline(n_epochs=2, train_split=0.8, span_verbosity=\"key_values\")\ntrainer.fit(algorithm, train_dataset=my_train, val_dataset=my_val)\n</code></pre>"},{"location":"reference/algorithm/#agentlightning.Baseline.run","title":"<code>run(store, llm_proxy, train_dataset=None, val_dataset=None)</code>  <code>async</code>","text":"<p>Execute the baseline loop across the provided datasets.</p>"},{"location":"reference/algorithm/#adapter","title":"Adapter","text":""},{"location":"reference/algorithm/#agentlightning.Adapter","title":"<code>agentlightning.Adapter</code>","text":"<p>               Bases: <code>Generic[T_from, T_to]</code></p> <p>Base class for synchronous adapters that convert data from one format to another.</p> <p>The class defines a minimal protocol so that adapters can be treated like callables while still allowing subclasses to supply the concrete transformation logic.</p> <p>Note</p> <p>Subclasses must override <code>adapt()</code> to provide the actual conversion.</p> <p>Type Variables:</p> <pre><code>T_from: Source data type supplied to the adapter.\n\nT_to: Target data type produced by the adapter.\n</code></pre> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class IntToStrAdapter(Adapter[int, str]):\n...     def adapt(self, source: int) -&gt; str:\n...         return str(source)\n...\n&gt;&gt;&gt; adapter = IntToStrAdapter()\n&gt;&gt;&gt; adapter(42)\n'42'\n</code></pre>"},{"location":"reference/algorithm/#agentlightning.Adapter.__call__","title":"<code>__call__(source)</code>","text":"<p>Convert the data to the target format.</p> <p>This method delegates to <code>adapt()</code> so that an instance of <code>Adapter</code> can be used like a standard function.</p> <p>Parameters:</p> <ul> <li> <code>source</code>               (<code>T_from</code>)           \u2013            <p>Input data in the source format.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>T_to</code>           \u2013            <p>Data converted to the target format.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.Adapter.adapt","title":"<code>adapt(source)</code>","text":"<p>Convert the data to the target format.</p> <p>Subclasses must override this method with the concrete transformation logic. The base implementation raises <code>NotImplementedError</code> to make the requirement explicit.</p> <p>Parameters:</p> <ul> <li> <code>source</code>               (<code>T_from</code>)           \u2013            <p>Input data in the source format.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>T_to</code>           \u2013            <p>Data converted to the target format.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.TraceAdapter","title":"<code>agentlightning.TraceAdapter</code>","text":"<p>               Bases: <code>Adapter[Sequence[Span], T_to]</code>, <code>Generic[T_to]</code></p> <p>Base class for adapters that convert trace spans into other formats.</p> <p>This class specializes <code>Adapter</code> for working with <code>Span</code> instances emitted by Agent Lightning instrumentation. Subclasses receive entire trace slices and return a format suited for the downstream consumer, for example reinforcement learning training data or observability metrics.</p>"},{"location":"reference/algorithm/#agentlightning.OtelTraceAdapter","title":"<code>agentlightning.OtelTraceAdapter</code>","text":"<p>               Bases: <code>Adapter[Sequence[ReadableSpan], T_to]</code>, <code>Generic[T_to]</code></p> <p>Base class for adapters that convert OpenTelemetry trace spans into other formats.</p> <p>This specialization of <code>Adapter</code> expects a list of <code>opentelemetry.sdk.trace.ReadableSpan</code> instances and produces any target format, such as reinforcement learning trajectories, structured logs, or analytics-ready payloads.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class TraceToDictAdapter(OtelTraceAdapter[dict]):\n...     def adapt(self, spans: List[ReadableSpan]) -&gt; dict:\n...         return {\"count\": len(spans)}\n...\n&gt;&gt;&gt; adapter = TraceToDictAdapter()\n&gt;&gt;&gt; adapter([span1, span2])\n{'count': 2}\n</code></pre>"},{"location":"reference/algorithm/#agentlightning.TraceToTripletBase","title":"<code>agentlightning.TraceToTripletBase</code>","text":"<p>               Bases: <code>TraceAdapter[List[Triplet]]</code></p> <p>Base class for adapters that emit <code>Triplet</code> trajectories.</p>"},{"location":"reference/algorithm/#agentlightning.TracerTraceToTriplet","title":"<code>agentlightning.TracerTraceToTriplet</code>","text":"<p>               Bases: <code>TraceToTripletBase</code></p> <p>Convert tracer-emitted spans into triplet trajectories.</p> <p>Attributes:</p> <ul> <li> <code>repair_hierarchy</code>           \u2013            <p>When <code>True</code>, repair the span tree using <code>TraceTree.repair_hierarchy()</code> before matching calls and rewards.</p> </li> <li> <code>llm_call_match</code>           \u2013            <p>Regular expression pattern that selects LLM call span names.</p> </li> <li> <code>agent_match</code>           \u2013            <p>Optional regular expression pattern for agent span names. When omitted, spans from any agent are considered.</p> </li> <li> <code>exclude_llm_call_in_reward</code>           \u2013            <p>When <code>True</code>, ignore matches under reward spans while searching for rewards.</p> </li> <li> <code>reward_match</code>           \u2013            <p>Strategy used to associate rewards with LLM calls.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.TracerTraceToTriplet.adapt","title":"<code>adapt(source)</code>","text":"<p>Convert tracer spans into <code>Triplet</code> trajectories.</p> <p>Parameters:</p> <ul> <li> <code>source</code>               (<code>Union[Sequence[Span], Sequence[ReadableSpan]]</code>)           \u2013            <p>Agent Lightning spans or raw OpenTelemetry spans that form a trace.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Triplet]</code>           \u2013            <p>Ordered list of trajectory transitions with prompt, response, and reward information.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.TracerTraceToTriplet.visualize","title":"<code>visualize(source, /, filename='trace_tree', interested_span_match=None)</code>","text":"<p>Visualize the trace tree built from the supplied spans.</p> <p>Parameters:</p> <ul> <li> <code>source</code>               (<code>Union[List[Span], List[ReadableSpan]]</code>)           \u2013            <p>Collection of Agent Lightning <code>Span</code> objects or raw <code>opentelemetry.sdk.trace.ReadableSpan</code> instances.</p> </li> <li> <code>filename</code>               (<code>str</code>, default:                   <code>'trace_tree'</code> )           \u2013            <p>Base filename for the generated image; <code>.png</code> is appended automatically.</p> </li> <li> <code>interested_span_match</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional regular expression used to highlight a subset of spans.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TraceTree</code>           \u2013            <p>The <code>TraceTree</code> built from the provided</p> </li> <li> <code>TraceTree</code>           \u2013            <p>spans.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.LlmProxyTraceToTriplet","title":"<code>agentlightning.LlmProxyTraceToTriplet</code>","text":"<p>               Bases: <code>TraceToTripletBase</code></p> <p>Convert telemetry emitted by the LLM Proxy into triplet trajectories.</p> <p>Warning</p> <p>This adapter is experimental and might be merged with <code>TracerTraceToTriplet</code> in the future.</p> <p>Danger</p> <p>Do not rely on timestamps when using this adapter. Proxy spans can originate on different machines with unsynchronised clocks, so <code>sequence_id</code> is treated as the sole source of ordering.</p> <p>Strategy:</p> <ol> <li>Sort spans by <code>(sequence_id, start_time)</code> for deterministic processing.</li> <li>Extract token identifiers from <code>litellm_request</code> or <code>raw_gen_ai_request</code> spans.</li> <li>Extract rewards from spans exposing AgentOps-style payloads or explicit reward spans.</li> <li>Match each reward to the most recent unmatched LLM call whose sequence is smaller.</li> </ol>"},{"location":"reference/algorithm/#agentlightning.LlmProxyTraceToTriplet.adapt","title":"<code>adapt(source)</code>","text":"<p>Convert LLM Proxy spans into <code>Triplet</code> trajectories.</p> <p>Parameters:</p> <ul> <li> <code>source</code>               (<code>Sequence[Span]</code>)           \u2013            <p>Spans emitted by the LLM Proxy containing prompt, response, and reward data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Triplet]</code>           \u2013            <p>Ordered trajectory transitions matched purely by <code>sequence_id</code>.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.TraceToMessages","title":"<code>agentlightning.TraceToMessages</code>","text":"<p>               Bases: <code>TraceAdapter[List[OpenAIMessages]]</code></p> <p>Convert trace spans into OpenAI-compatible conversation messages.</p> <p>The adapter reconstructs prompts, completions, tool calls, and function definitions from <code>gen_ai.*</code> span attributes. The resulting objects match the JSONL structure expected by the OpenAI fine-tuning pipeline.</p> <p>Warning</p> <p>The adapter assumes all spans share a common trace and that tool call spans are direct children of the associated completion span.</p>"},{"location":"reference/algorithm/#agentlightning.TraceToMessages.adapt","title":"<code>adapt(source)</code>","text":"<p>Transform trace spans into OpenAI chat payloads.</p> <p>Parameters:</p> <ul> <li> <code>source</code>               (<code>Sequence[Span]</code>)           \u2013            <p>Spans containing <code>gen_ai.*</code> attributes emitted by the tracing pipeline.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[OpenAIMessages]</code>           \u2013            <p>A list of <code>OpenAIMessages</code> entries that</p> </li> <li> <code>List[OpenAIMessages]</code>           \u2013            <p>capture prompts, completions, tools, and metadata.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.TraceToMessages.get_tool_calls","title":"<code>get_tool_calls(completion, all_spans)</code>","text":"<p>Yield tool call payloads for a completion span.</p> <p>Parameters:</p> <ul> <li> <code>completion</code>               (<code>Span</code>)           \u2013            <p>The completion span whose descendants should be inspected.</p> </li> <li> <code>all_spans</code>               (<code>Sequence[Span]</code>)           \u2013            <p>The complete span list belonging to the trace.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Iterable[Dict[str, Any]]</code>           \u2013            <p>Dictionaries describing tool calls with identifiers, names, and arguments.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If a candidate tool span cannot be converted into a dictionary.</p> </li> </ul>"},{"location":"reference/algorithm/#llm-proxy","title":"LLM Proxy","text":""},{"location":"reference/algorithm/#agentlightning.LLMProxy","title":"<code>agentlightning.LLMProxy</code>","text":"<p>Host a LiteLLM OpenAI-compatible proxy bound to a LightningStore.</p> <p>The proxy:</p> <ul> <li>Serves an OpenAI-compatible API via uvicorn.</li> <li>Adds rollout/attempt routing and headers via middleware.</li> <li>Registers OTEL export and token-id callbacks.</li> <li>Writes a LiteLLM worker config file with <code>model_list</code> and settings.</li> </ul> <p>Lifecycle:</p> <ul> <li><code>start()</code> writes config, starts uvicorn server in a thread, and waits until ready.</li> <li><code>stop()</code> tears down the server and removes the temp config file.</li> <li><code>restart()</code> convenience wrapper to stop then start.</li> </ul> <p>Note</p> <p>As the LLM Proxy sets up an OpenTelemetry tracer, it's recommended to run it in a different process from the main runner (i.e., tracer from agents). See <code>launch_mode</code> for how to change that.</p> <p>Warning</p> <p>By default (or when \"stream_conversion\" middleware is enabled), the LLM Proxy will convert OpenAI and Anthropic requests with <code>stream=True</code> to a non-streaming request before going through the LiteLLM proxy. This is because the OpenTelemetry tracer provided by LiteLLM is buggy with streaming responses. You can disable this by removing the \"stream_conversion\" middleware. In that case, you might lose some tracing information like token IDs.</p> <p>Danger</p> <p>Do not run LLM proxy in the same process as the main runner. It's easy to cause conflicts in the tracer provider with tracers like <code>AgentOpsTracer</code>.</p> <p>Parameters:</p> <ul> <li> <code>port</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>TCP port to bind. Will bind to a random port if not provided.</p> </li> <li> <code>model_list</code>               (<code>List[ModelConfig] | None</code>, default:                   <code>None</code> )           \u2013            <p>LiteLLM <code>model_list</code> entries.</p> </li> <li> <code>store</code>               (<code>Optional[LightningStore]</code>, default:                   <code>None</code> )           \u2013            <p>LightningStore used for span sequence and persistence.</p> </li> <li> <code>host</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Publicly reachable host used in resource endpoints. See <code>host</code> of <code>launcher_args</code> for more details.</p> </li> <li> <code>litellm_config</code>               (<code>Dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Extra LiteLLM proxy config merged with <code>model_list</code>.</p> </li> <li> <code>num_retries</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Default LiteLLM retry count injected into <code>litellm_settings</code>.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of workers to run in the server. Only applicable for \"mp\" launch mode. Ignored if launcher_args is provided. When <code>num_workers &gt; 1</code>, the server will be run using gunicorn.</p> </li> <li> <code>launch_mode</code>               (<code>LaunchMode</code>, default:                   <code>'mp'</code> )           \u2013            <p>Launch mode for the server. Defaults to \"mp\". Cannot be used together with launcher_args. Ignored if launcher_args is provided. It's recommended to use <code>launch_mode=\"mp\"</code> to launch the proxy, which will launch the server in a separate process. <code>launch_mode=\"thread\"</code> can also be used if used in caution. It will launch the server in a separate thread. <code>launch_mode=\"asyncio\"</code> launches the server in the current thread as an asyncio task. It is NOT recommended because it often causes hanging requests. Only use it if you know what you are doing.</p> </li> <li> <code>launcher_args</code>               (<code>PythonServerLauncherArgs | None</code>, default:                   <code>None</code> )           \u2013            <p>Arguments for the server launcher. If this is provided, host, port, and launch_mode will be ignored. Cannot be used together with port, host, and launch_mode.</p> </li> <li> <code>middlewares</code>               (<code>Sequence[Union[Type[BaseHTTPMiddleware], str]] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of FastAPI middleware classes or strings to register. You can specify the class aliases or classes that have been imported. If not provided, the default middlewares (RolloutAttemptMiddleware and StreamConversionMiddleware) will be used. Available middleware aliases are: \"rollout_attempt\", \"stream_conversion\", \"message_inspection\". Middlewares are the first layer of request processing. They are applied to all requests before the LiteLLM proxy.</p> </li> <li> <code>callbacks</code>               (<code>Sequence[Union[Type[CustomLogger], str]] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of LiteLLM callback classes or strings to register. You can specify the class aliases or classes that have been imported. If not provided, the default callbacks (AddReturnTokenIds and LightningOpenTelemetry) will be used. Available callback aliases are: \"return_token_ids\", \"opentelemetry\", \"logprobs\".</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.LLMProxy.as_resource","title":"<code>as_resource(rollout_id=None, attempt_id=None, model=None, sampling_parameters=None)</code>","text":"<p>Create an <code>LLM</code> resource pointing at this proxy with rollout context.</p> The returned endpoint is <p><code>http://{host}:{port}/rollout/{rollout_id}/attempt/{attempt_id}</code></p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Rollout identifier used for span attribution. If None, will instantiate a ProxyLLM resource.</p> </li> <li> <code>attempt_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Attempt identifier used for span attribution. If None, will instantiate a ProxyLLM resource.</p> </li> <li> <code>model</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Logical model name to use. If omitted and exactly one model is configured or all models have the same name, that model is used.</p> </li> <li> <code>sampling_parameters</code>               (<code>Dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional default sampling parameters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LLM</code> (              <code>LLM</code> )          \u2013            <p>Configured resource ready for OpenAI-compatible calls.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>model</code> is omitted and zero or multiple models are configured.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.LLMProxy.get_store","title":"<code>get_store()</code>","text":"<p>Get the store used by the proxy.</p> <p>Returns:</p> <ul> <li> <code>Optional[LightningStore]</code>           \u2013            <p>The store used by the proxy.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.LLMProxy.initialize","title":"<code>initialize()</code>","text":"<p>Initialize global middleware and LiteLLM callbacks.</p> <p>Installs:</p> <ul> <li>A FastAPI middleware that rewrites /rollout/{rid}/attempt/{aid}/... paths, injects rollout/attempt/sequence headers, and forwards downstream.</li> <li>LiteLLM callbacks for token ids and OpenTelemetry export.</li> </ul> <p>The middleware can only be installed once because once the FastAPI app has started, the middleware cannot be changed any more.</p> <p>This function does not start any server. It only wires global hooks.</p>"},{"location":"reference/algorithm/#agentlightning.LLMProxy.is_running","title":"<code>is_running()</code>","text":"<p>Return whether the uvicorn server is active.</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if server was started and did not signal exit.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.LLMProxy.restart","title":"<code>restart(*, _port=None)</code>  <code>async</code>","text":"<p>Restart the proxy if running, else start it.</p> <p>Convenience wrapper calling <code>stop()</code> followed by <code>start()</code>.</p>"},{"location":"reference/algorithm/#agentlightning.LLMProxy.set_store","title":"<code>set_store(store)</code>","text":"<p>Set the store for the proxy.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>LightningStore</code>)           \u2013            <p>The store to use for the proxy.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.LLMProxy.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Start the proxy server thread and initialize global wiring.</p> <p>Side effects:</p> <ul> <li>Sets the module-level global store for middleware/exporter access.</li> <li>Calls <code>initialize()</code> once to register middleware and callbacks.</li> <li>Writes a temporary YAML config consumed by LiteLLM worker.</li> <li>Launches uvicorn in a daemon thread and waits for readiness.</li> </ul>"},{"location":"reference/algorithm/#agentlightning.LLMProxy.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop the proxy server and clean up temporary artifacts.</p> <p>This is a best-effort graceful shutdown with a bounded join timeout.</p>"},{"location":"reference/algorithm/#agentlightning.LLMProxy.update_model_list","title":"<code>update_model_list(model_list)</code>","text":"<p>Replace the in-memory model list.</p> <p>Parameters:</p> <ul> <li> <code>model_list</code>               (<code>List[ModelConfig]</code>)           \u2013            <p>New list of model entries.</p> </li> </ul>"},{"location":"reference/cli/","title":"Command Line Interface","text":"<p>Warning</p> <p>This document is a work in progress and might not be updated with the latest changes. Try to use <code>agl -h</code> to get the latest help message.</p> <p>Tip</p> <p>Agent-lightning also provides utilities to help you build your own CLI for LitAgent and Trainer. See Trainer for references.</p>"},{"location":"reference/cli/#agl","title":"agl","text":"<pre><code>usage: agl [-h] {vllm,store,prometheus,agentops}\n\nAgent Lightning CLI entry point.\n\nAvailable subcommands:\n  vllm        Run the vLLM CLI with Agent Lightning instrumentation.\n  store       Run a LightningStore server.\n  prometheus  Serve Prometheus metrics from the multiprocess registry.\n  agentops    Start the AgentOps server manager.\n\npositional arguments:\n  {vllm,store,prometheus,agentops}\n                        Subcommand to run.\n\noptions:\n  -h, --help            show this help message and exit\n</code></pre>"},{"location":"reference/cli/#agl-vllm","title":"agl vllm","text":"<p>Agent-lightning's instrumented vLLM CLI.</p> <pre><code>usage: agl vllm [-h] [-v] {chat,complete,serve,bench,collect-env,run-batch} ...\n\nvLLM CLI\n\npositional arguments:\n  {chat,complete,serve,bench,collect-env,run-batch}\n    chat                Generate chat completions via the running API server.\n    complete            Generate text completions based on the given prompt via the running API server.\n    collect-env         Start collecting environment information.\n    run-batch           Run batch prompts and write results to file.\n\noptions:\n  -h, --help            show this help message and exit\n  -v, --version         show program's version number and exit\n\nFor full list:            vllm [subcommand] --help=all\nFor a section:            vllm [subcommand] --help=ModelConfig    (case-insensitive)\nFor a flag:               vllm [subcommand] --help=max-model-len  (_ or - accepted)\nDocumentation:            https://docs.vllm.ai\n</code></pre>"},{"location":"reference/cli/#agl-store","title":"agl store","text":"<p>Agent-lightning's LightningStore CLI. Use it to start an independent LightningStore server.</p> <p>Currently the store data are stored in memory and will be lost when the server is stopped.</p> <pre><code>usage: agl store [-h] [--host HOST] [--port PORT] [--cors-origin CORS_ORIGINS] [--log-level {DEBUG,INFO,WARNING,ERROR}] [--tracker {prometheus,console} [{prometheus,console} ...]] [--n-workers N_WORKERS] [--backend {memory,mongo}]\n                 [--mongo-uri MONGO_URI]\n\nRun a LightningStore server\n\noptions:\n  -h, --help            show this help message and exit\n  --host HOST           Host to bind the server to\n  --port PORT           Port to run the server on\n  --cors-origin CORS_ORIGINS\n                        Allowed CORS origin. Repeat for multiple origins. Use '*' to allow all origins.\n  --log-level {DEBUG,INFO,WARNING,ERROR}\n                        Configure the logging level for the store.\n  --tracker {prometheus,console} [{prometheus,console} ...]\n                        Enable metrics tracking. Repeat for multiple trackers.\n  --n-workers N_WORKERS\n                        Number of workers to run in the server. When it's greater than 1, the server will be run using `mp` launch mode. Only applicable for zero-copy stores such as MongoDB backend.\n  --backend {memory,mongo}\n                        Backend to use for the store.\n  --mongo-uri MONGO_URI\n                        MongoDB URI to use for the store. Applicable only if --backend is 'mongo'.\n</code></pre> <p>Tip</p> <p>After launching the store via CLI, you can tell the <code>Trainer</code> to use the store by passing the store address to the trainer.</p> <pre><code>store_client = agl.LightningStoreClient(\"http://localhost:4747\")\ntrainer = agl.Trainer(store=store_client, ...)\n</code></pre> <p>See using external store for more details.</p>"},{"location":"reference/cli/#agl-prometheus","title":"agl prometheus","text":"<p>Expose the Prometheus multiprocess registry on a dedicated FastAPI server. This is useful when the main LightningStore service is under heavy load; exporters can scrape this auxiliary endpoint instead.</p> <pre><code>usage: agl prometheus [-h] [--host HOST] [--port PORT] [--metrics-path METRICS_PATH] [--log-level {DEBUG,INFO,WARNING,ERROR}] [--access-log]\n\nServe Prometheus metrics outside the LightningStore server.\n\noptions:\n  -h, --help            show this help message and exit\n  --host HOST           Host to bind the metrics server to.\n  --port PORT           Port to expose the Prometheus metrics on.\n  --metrics-path METRICS_PATH\n                        HTTP path used to expose metrics. Must start with '/' and not be the root path.\n  --log-level {DEBUG,INFO,WARNING,ERROR}\n                        Configure the logging level for the metrics server.\n  --access-log          Enable uvicorn access logs. Disabled by default to reduce noise.\n</code></pre>"},{"location":"reference/instrumentation/","title":"Instrumentation API","text":""},{"location":"reference/instrumentation/#agentlightning.instrumentation.instrument_all","title":"<code>agentlightning.instrumentation.instrument_all()</code>","text":"<p>Instrument all the instrumentation libraries.</p>"},{"location":"reference/instrumentation/#agentlightning.instrumentation.uninstrument_all","title":"<code>agentlightning.instrumentation.uninstrument_all()</code>","text":"<p>Uninstrument all the instrumentation libraries.</p>"},{"location":"reference/instrumentation/#agentops-langchain","title":"AgentOps LangChain","text":""},{"location":"reference/instrumentation/#agentlightning.instrumentation.agentops_langchain","title":"<code>agentlightning.instrumentation.agentops_langchain</code>","text":""},{"location":"reference/instrumentation/#agentlightning.instrumentation.agentops_langchain.instrument_agentops_langchain","title":"<code>instrument_agentops_langchain()</code>","text":"<p>Bypass AgentOp's native support for Langchain.</p>"},{"location":"reference/instrumentation/#agentlightning.instrumentation.agentops_langchain.uninstrument_agentops_langchain","title":"<code>uninstrument_agentops_langchain()</code>","text":"<p>Restore AgentOp's native support for Langchain.</p>"},{"location":"reference/instrumentation/#agentops","title":"AgentOps","text":""},{"location":"reference/instrumentation/#agentlightning.instrumentation.agentops","title":"<code>agentlightning.instrumentation.agentops</code>","text":""},{"location":"reference/instrumentation/#agentlightning.instrumentation.agentops.BypassableAuthenticatedOTLPExporter","title":"<code>BypassableAuthenticatedOTLPExporter</code>","text":"<p>               Bases: <code>LightningStoreOTLPExporter</code>, <code>AuthenticatedOTLPExporter</code></p> <p>AuthenticatedOTLPExporter with switchable service control.</p> <p>When <code>_agentops_service_enabled</code> is False, skip export and return success.</p>"},{"location":"reference/instrumentation/#agentlightning.instrumentation.agentops.BypassableOTLPMetricExporter","title":"<code>BypassableOTLPMetricExporter</code>","text":"<p>               Bases: <code>OTLPMetricExporter</code></p> <p>OTLPMetricExporter with switchable service control. When <code>_agentops_service_enabled</code> is False, skip export and return success.</p>"},{"location":"reference/instrumentation/#agentlightning.instrumentation.agentops.BypassableOTLPSpanExporter","title":"<code>BypassableOTLPSpanExporter</code>","text":"<p>               Bases: <code>LightningStoreOTLPExporter</code></p> <p>OTLPSpanExporter with switchable service control. When <code>_agentops_service_enabled</code> is False, skip export and return success.</p> <p>This is used instead of BypassableAuthenticatedOTLPExporter on legacy AgentOps versions.</p>"},{"location":"reference/instrumentation/#agentlightning.instrumentation.agentops.BypassableV3Client","title":"<code>BypassableV3Client</code>","text":"<p>               Bases: <code>V3Client</code></p> <p>V3Client with toggleable authentication calls. Returns dummy auth response when <code>_agentops_service_enabled</code> is False.</p>"},{"location":"reference/instrumentation/#agentlightning.instrumentation.agentops.BypassableV4Client","title":"<code>BypassableV4Client</code>","text":"<p>               Bases: <code>V4Client</code></p> <p>V4Client with toggleable post requests. Returns dummy response when <code>_agentops_service_enabled</code> is False.</p>"},{"location":"reference/instrumentation/#agentlightning.instrumentation.agentops.enable_agentops_service","title":"<code>enable_agentops_service(enabled=True)</code>","text":"<p>Enable or disable communication with the AgentOps service.</p> <p>By default, AgentOps exporters and clients will run in local mode and will NOT attempt to communicate with the remote AgentOps service.</p> <p>Parameters:</p> <ul> <li> <code>enabled</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, enable all AgentOps exporters and clients. All exporters and clients will operate in normal mode and send data to the AgentOps service.</p> </li> </ul>"},{"location":"reference/instrumentation/#agentlightning.instrumentation.agentops.instrument_agentops","title":"<code>instrument_agentops()</code>","text":"<p>Instrument agentops to capture token IDs. Automatically detects and uses the appropriate patching method based on the installed agentops version.</p>"},{"location":"reference/instrumentation/#agentlightning.instrumentation.agentops.uninstrument_agentops","title":"<code>uninstrument_agentops()</code>","text":"<p>Uninstrument agentops to stop capturing token IDs.</p>"},{"location":"reference/instrumentation/#litellm","title":"LiteLLM","text":""},{"location":"reference/instrumentation/#agentlightning.instrumentation.litellm","title":"<code>agentlightning.instrumentation.litellm</code>","text":"<p>LiteLLM instrumentations.</p> <p>It's unclear whether or not this file is useful. It seems that LiteLLM owns its own telemetry from their own entrance</p> <p>Related documentation.</p>"},{"location":"reference/instrumentation/#agentlightning.instrumentation.litellm.instrument_litellm","title":"<code>instrument_litellm()</code>","text":"<p>Instrument litellm to capture token IDs.</p>"},{"location":"reference/instrumentation/#agentlightning.instrumentation.litellm.uninstrument_litellm","title":"<code>uninstrument_litellm()</code>","text":"<p>Uninstrument litellm to stop capturing token IDs.</p>"},{"location":"reference/instrumentation/#vllm","title":"vLLM","text":""},{"location":"reference/instrumentation/#agentlightning.instrumentation.vllm","title":"<code>agentlightning.instrumentation.vllm</code>","text":""},{"location":"reference/instrumentation/#agentlightning.instrumentation.vllm.instrument_vllm","title":"<code>instrument_vllm()</code>","text":"<p>Instrument vLLM to capture token IDs generated by engine.</p> <p>This instrumentation has been merged to upstream vLLM since v0.10.2.</p>"},{"location":"reference/instrumentation/#agentlightning.instrumentation.vllm.uninstrument_vllm","title":"<code>uninstrument_vllm()</code>","text":"<p>Uninstrument vLLM to stop capturing token IDs generated by engine.</p>"},{"location":"reference/internal/","title":"Internal API References","text":"<p>Danger</p> <p>The following APIs should be used with extra caution because they are very likely to change in the future.</p>"},{"location":"reference/internal/#algorithms-and-adapters","title":"Algorithms and Adapters","text":""},{"location":"reference/internal/#agentlightning.adapter.messages.OpenAIMessages","title":"<code>agentlightning.adapter.messages.OpenAIMessages</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>OpenAI-style chat messages with optional tool definitions.</p> <p>Attributes:</p> <ul> <li> <code>messages</code>               (<code>List[ChatCompletionMessageParam]</code>)           \u2013            <p>Ordered chat messages that describe the conversation.</p> </li> <li> <code>tools</code>               (<code>Optional[List[ChatCompletionFunctionToolParam]]</code>)           \u2013            <p>Tool specifications available to the assistant, if any.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree","title":"<code>agentlightning.adapter.triplet.TraceTree</code>","text":"<p>Tree representation of a trace span and its descendants.</p> <p>Attributes:</p> <ul> <li> <code>id</code>           \u2013            <p>Unique identifier for the span node.</p> </li> <li> <code>span</code>           \u2013            <p><code>Span</code> backing this node.</p> </li> <li> <code>children</code>           \u2013            <p>Child nodes connected to the current span.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.agent_name","title":"<code>agent_name()</code>","text":"<p>Return the agent name associated with the span, if any.</p> <p>Returns:</p> <ul> <li> <code>Optional[str]</code>           \u2013            <p>Agent name extracted from known attributes, otherwise <code>None</code>.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.extract_prompt_image_urls","title":"<code>extract_prompt_image_urls(prompt_raw_content)</code>","text":"<p>Extract image URLs from the span attributes, in order of appearance.</p> <p>Parameters:</p> <ul> <li> <code>prompt_raw_content</code>               (<code>Any</code>)           \u2013            <p>The raw content of the prompt, which can be in one of several formats:</p> <ul> <li>List[dict]: A list of message entries, each being a dict with at least a \"content\" key.</li> <li>Dict[str, Any]: A dictionary, often with numeric string keys (e.g., <code>{\"0\": {...}, \"1\": {...}}</code>), where each value is a message entry.   If the dict does not have numeric keys, it is treated as a single message entry.</li> </ul> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.find_llm_calls","title":"<code>find_llm_calls(*, llm_call_match, agent_match, within_matching_subtree=None, within_reward=None, within_llm_call=None, existing_llm_call_response_ids=None)</code>","text":"<p>Find LLM call spans matching the supplied filters.</p> <p>Parameters:</p> <ul> <li> <code>llm_call_match</code>               (<code>str</code>)           \u2013            <p>Regular expression used to match span names that qualify as LLM calls.</p> </li> <li> <code>agent_match</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional regular expression that must match the enclosing agent span name.</p> </li> <li> <code>within_matching_subtree</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Marker propagated through recursive calls to record matching agents.</p> </li> <li> <code>within_reward</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>When <code>True</code>, suppresses LLM matches under reward spans.</p> </li> <li> <code>within_llm_call</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>When <code>True</code>, prevents duplicate matches for nested LLM calls.</p> </li> <li> <code>existing_llm_call_response_ids</code>               (<code>Optional[set[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Known response identifiers used to deduplicate spans.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Tuple['TraceTree', str]]</code>           \u2013            <p>A list of tuples pairing the matching node with the agent subtree label that triggered the</p> </li> <li> <code>List[Tuple['TraceTree', str]]</code>           \u2013            <p>match.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.from_spans","title":"<code>from_spans(spans)</code>  <code>classmethod</code>","text":"<p>Construct a tree from a flat list of spans.</p> <p>Parameters:</p> <ul> <li> <code>spans</code>               (<code>List[Span]</code>)           \u2013            <p>Spans that collectively form a single trace segment.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'TraceTree'</code>           \u2013            <p>A <code>TraceTree</code> rooted at either the</p> </li> <li> <code>'TraceTree'</code>           \u2013            <p>discovered root span or a synthetic root when multiple roots are present.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the span list is empty or no root span can be inferred.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.is_reward_span","title":"<code>is_reward_span()</code>","text":"<p>Return whether the span explicitly encodes a reward.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p><code>True</code> when the span payload describes a reward, otherwise <code>False</code>.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.match_rewards","title":"<code>match_rewards(reward_match, llm_calls)</code>","text":"<p>Assign rewards to previously matched LLM calls.</p> <p>Parameters:</p> <ul> <li> <code>reward_match</code>               (<code>str</code>)           \u2013            <p>Strategy identifier from <code>RewardMatchPolicy</code>.</p> </li> <li> <code>llm_calls</code>               (<code>List['TraceTree']</code>)           \u2013            <p>Trace nodes representing LLM call spans.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Optional[float]]</code>           \u2013            <p>Mapping from span identifier to reward value or <code>None</code> when no reward is available.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.maybe_reward_dict","title":"<code>maybe_reward_dict()</code>","text":"<p>Return a reward payload if the span encodes one.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Dictionary containing reward metadata, or an empty dictionary when no reward is found.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.names_tuple","title":"<code>names_tuple()</code>","text":"<p>Return the span name alongside nested child names.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>A tuple of the current span name and a list of tuples for each child containing the</p> </li> <li> <code>List[Any]</code>           \u2013            <p>child name and its descendants.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.repair_hierarchy","title":"<code>repair_hierarchy()</code>","text":"<p>Repair missing parent-child relationships introduced by mixed tracing systems.</p> <p>Some agent frameworks emit spans via multiple subsystems, which can cause LLM completion spans to float directly under the root span instead of being nested under the correct agent. The method re-parents those spans to the closest ancestor that fully envelopes the child in time.</p> <p>If we don't, when we want to select the LLM completion span with agent as filter. We will never get the correct span underneath.</p>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.span_to_triplet","title":"<code>span_to_triplet(span, agent_name)</code>","text":"<p>Convert a span to a triplet.</p> <p>Subclass can override this method to add more fields to the triplet, such as chat messages and tool calls.</p>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.to_json","title":"<code>to_json()</code>","text":"<p>Convert the tree node into a JSON-serialisable structure.</p>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.to_trajectory","title":"<code>to_trajectory(llm_call_match='openai\\\\.chat\\\\.completion', agent_match=None, exclude_llm_call_in_reward=True, dedup_llm_call=True, reward_match=RewardMatchPolicy.FIRST_OCCURRENCE, final_reward=None, _skip_empty_token_spans=False)</code>","text":"<p>Convert the trace tree into a trajectory of <code>Triplet</code> items.</p> <p>Parameters:</p> <ul> <li> <code>llm_call_match</code>               (<code>str</code>, default:                   <code>'openai\\\\.chat\\\\.completion'</code> )           \u2013            <p>Regular expression for LLM call span names.</p> </li> <li> <code>agent_match</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional regular expression for agent span names.</p> </li> <li> <code>exclude_llm_call_in_reward</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>When <code>True</code>, prevents searching for rewards under the LLM call subtree.</p> </li> <li> <code>dedup_llm_call</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>When <code>True</code>, deduplicates spans using the LLM response identifier.</p> </li> <li> <code>reward_match</code>               (<code>RewardMatchPolicy</code>, default:                   <code>FIRST_OCCURRENCE</code> )           \u2013            <p>Reward matching policy used to associate reward spans with LLM calls.</p> </li> <li> <code>final_reward</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Optional reward appended to the final transition when provided.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Triplet]</code>           \u2013            <p>A list of <code>Triplet</code> objects ordered by call sequence.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.traverse","title":"<code>traverse()</code>","text":"<p>Traverse the tree depth first and return every node.</p>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.visualize","title":"<code>visualize(filename, interested_span_match=None)</code>","text":"<p>Render the trace tree with Graphviz for debugging purposes.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>               (<code>str</code>)           \u2013            <p>Base filename for the generated <code>.png</code> diagram.</p> </li> <li> <code>interested_span_match</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional regular expression used to keep only matching spans (and their ancestors) in the output.</p> </li> </ul> <p>Note</p> <p>The method requires the optional <code>graphviz</code> dependency to be available in the runtime environment.</p>"},{"location":"reference/internal/#agentlightning.adapter.triplet.Transition","title":"<code>agentlightning.adapter.triplet.Transition</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single transition within a reinforcement learning trajectory.</p> <p>Attributes:</p> <ul> <li> <code>state</code>               (<code>List[int]</code>)           \u2013            <p>Token identifiers describing the model input state.</p> </li> <li> <code>action</code>               (<code>List[int]</code>)           \u2013            <p>Token identifiers representing the model output.</p> </li> <li> <code>response_id</code>               (<code>Optional[str]</code>)           \u2013            <p>Identifier of the LLM response used to deduplicate spans.</p> </li> <li> <code>agent_name</code>               (<code>str</code>)           \u2013            <p>Human-readable agent name captured from the trace.</p> </li> <li> <code>reward</code>               (<code>Optional[float]</code>)           \u2013            <p>Scalar reward associated with the transition, if available.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.RewardMatchPolicy","title":"<code>agentlightning.adapter.triplet.RewardMatchPolicy</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Strategies for matching rewards to LLM call spans.</p> <p>Note</p> <p>Each reward span must expose a payload shaped like <code>{\"type\": \"reward\", \"value\": &lt;float&gt;|None}</code> as described in <code>reward.py</code>.</p>"},{"location":"reference/internal/#agentlightning.adapter.triplet.RewardMatchPolicy.FIRST_OCCURRENCE","title":"<code>FIRST_OCCURRENCE = 'first_occurrence'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the first reward encountered in chronological order after the current LLM call match.</p>"},{"location":"reference/internal/#agentlightning.adapter.triplet.RewardMatchPolicy.FIRST_SIBLING","title":"<code>FIRST_SIBLING = 'first_sibling'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the first sibling in the current trace subtree as the reward unless another LLM call match is found.</p>"},{"location":"reference/internal/#agentlightning.algorithm.decorator.FunctionalAlgorithm","title":"<code>agentlightning.algorithm.decorator.FunctionalAlgorithm</code>","text":"<p>               Bases: <code>Algorithm</code>, <code>Generic[AF]</code></p> <p>An algorithm wrapper built from a callable implementation.</p> <p>Functional algorithms let you provide an ordinary function instead of subclassing <code>Algorithm</code>. The wrapper inspects the callable signature to supply optional dependencies such as the store, adapter, and LLM proxy.</p>"},{"location":"reference/internal/#agentlightning.algorithm.decorator.FunctionalAlgorithm.__init__","title":"<code>__init__(algorithm_func)</code>","text":"<pre><code>__init__(algorithm_func: AlgorithmFuncSyncLike) -&gt; None\n</code></pre><pre><code>__init__(algorithm_func: AlgorithmFuncAsyncLike) -&gt; None\n</code></pre> <p>Wrap a function that implements algorithm behaviour.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_func</code>               (<code>Union[AlgorithmFuncSyncLike, AlgorithmFuncAsyncLike]</code>)           \u2013            <p>Sync or async callable implementing the algorithm contract. Arguments are detected automatically based on the function signature.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.algorithm.decorator.FunctionalAlgorithm.run","title":"<code>run(train_dataset=None, val_dataset=None)</code>","text":"<pre><code>run(\n    train_dataset: Optional[Dataset[Any]] = None,\n    val_dataset: Optional[Dataset[Any]] = None,\n) -&gt; None\n</code></pre><pre><code>run(\n    train_dataset: Optional[Dataset[Any]] = None,\n    val_dataset: Optional[Dataset[Any]] = None,\n) -&gt; Awaitable[None]\n</code></pre> <p>Execute the wrapped function with injected dependencies.</p> <p>Parameters:</p> <ul> <li> <code>train_dataset</code>               (<code>Optional[Dataset[Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional training dataset passed through when the callable declares a <code>train_dataset</code> parameter.</p> </li> <li> <code>val_dataset</code>               (<code>Optional[Dataset[Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional validation dataset passed through when the callable declares a <code>val_dataset</code> parameter.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[None, Awaitable[None]]</code>           \u2013            <p>None for sync callables or an awaitable when the callable is async.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If a dataset is provided but the function signature does not accept the corresponding argument.</p> </li> </ul>"},{"location":"reference/internal/#litagent","title":"LitAgent","text":""},{"location":"reference/internal/#agentlightning.litagent.decorator.FunctionalLitAgent","title":"<code>agentlightning.litagent.decorator.FunctionalLitAgent</code>","text":"<p>               Bases: <code>LitAgent[T]</code></p> <p>Adapter that turns plain rollout functions into <code>LitAgent</code> instances.</p> <p>The helper inspects the wrapped function to determine which resources to inject, allowing both synchronous and asynchronous callables to participate in the training loop without writing a dedicated subclass.</p>"},{"location":"reference/internal/#agentlightning.litagent.decorator.FunctionalLitAgent.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Make the agent instance callable, preserving the original function behavior.</p>"},{"location":"reference/internal/#agentlightning.litagent.decorator.FunctionalLitAgent.__init__","title":"<code>__init__(rollout_func, *, strip_proxy=True)</code>","text":"<p>Initialize the wrapper around a rollout function.</p> <p>Parameters:</p> <ul> <li> <code>rollout_func</code>               (<code>FunctionalLitAgentFunc[T]</code>)           \u2013            <p>Callable that implements the rollout. It may be synchronous or asynchronous and can optionally receive a <code>Rollout</code> alongside resources such as <code>llm</code> or <code>prompt_template</code>.</p> </li> <li> <code>strip_proxy</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>When <code>True</code>, convert <code>ProxyLLM</code> inputs into <code>LLM</code> instances before calling the rollout function. Defaults to <code>True</code>.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.litagent.decorator.FunctionalLitAgent.rollout","title":"<code>rollout(task, resources, rollout)</code>","text":"<p>Execute a synchronous rollout using the wrapped function.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>T</code>)           \u2013            <p>Task input data.</p> </li> <li> <code>resources</code>               (<code>NamedResources</code>)           \u2013            <p>Mapping of named resources available to the agent.</p> </li> <li> <code>rollout</code>               (<code>Rollout</code>)           \u2013            <p>Rollout metadata provided by the runtime.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RolloutRawResult</code>           \u2013            <p>Result produced by the wrapped rollout function.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the wrapped function is asynchronous.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.litagent.decorator.FunctionalLitAgent.rollout_async","title":"<code>rollout_async(task, resources, rollout)</code>  <code>async</code>","text":"<p>Execute an asynchronous rollout using the wrapped function.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>T</code>)           \u2013            <p>Task input data.</p> </li> <li> <code>resources</code>               (<code>NamedResources</code>)           \u2013            <p>Mapping of named resources available to the agent.</p> </li> <li> <code>rollout</code>               (<code>Rollout</code>)           \u2013            <p>Rollout metadata provided by the runtime.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RolloutRawResult</code>           \u2013            <p>Result produced by the wrapped rollout coroutine.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the wrapped function is synchronous.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.litagent.decorator.llm_rollout","title":"<code>agentlightning.litagent.decorator.llm_rollout(func=None, *, strip_proxy=True)</code>","text":"<pre><code>llm_rollout(\n    func: LlmRolloutFunc[T],\n) -&gt; FunctionalLitAgent[T]\n</code></pre><pre><code>llm_rollout(\n    *, strip_proxy: bool = True\n) -&gt; Callable[[LlmRolloutFunc[T]], FunctionalLitAgent[T]]\n</code></pre> <p>Create a <code>FunctionalLitAgent</code> for LLM-based rollouts.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>LlmRolloutFunc[T] | None</code>, default:                   <code>None</code> )           \u2013            <p>Callable defining the agent's behaviour. Supported signatures include:</p> <ul> <li><code>(task, llm) -&gt; result</code></li> <li><code>(task, llm, rollout) -&gt; result</code></li> <li><code>async (task, llm) -&gt; result</code></li> <li><code>async (task, llm, rollout) -&gt; result</code></li> </ul> </li> <li> <code>strip_proxy</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>When <code>True</code>, convert proxy resources into concrete <code>LLM</code> instances before calling the function. Defaults to <code>True</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FunctionalLitAgent[T] | Callable[[LlmRolloutFunc[T]], FunctionalLitAgent[T]]</code>           \u2013            <p><code>FunctionalLitAgent</code> that</p> </li> <li> <code>FunctionalLitAgent[T] | Callable[[LlmRolloutFunc[T]], FunctionalLitAgent[T]]</code>           \u2013            <p>wraps the supplied function.</p> </li> </ul> <p>Examples:</p> <pre><code>@llm_rollout\ndef my_agent(task, llm):\n    return llm.endpoint\n\n@llm_rollout(strip_proxy=False)\ndef my_agent_no_strip(task, llm):\n    return llm.model\n\nresult = my_agent(task, llm)\nresult = my_agent.rollout(task, resources, rollout)\n</code></pre>"},{"location":"reference/internal/#agentlightning.litagent.decorator.prompt_rollout","title":"<code>agentlightning.litagent.decorator.prompt_rollout(func=None)</code>","text":"<pre><code>prompt_rollout(\n    func: PromptRolloutFunc[T],\n) -&gt; FunctionalLitAgent[T]\n</code></pre><pre><code>prompt_rollout() -&gt; (\n    Callable[[PromptRolloutFunc[T]], FunctionalLitAgent[T]]\n)\n</code></pre> <p>Create a <code>FunctionalLitAgent</code> for prompt-based rollouts.</p> <p>This decorator is designed for agents that work with tunable prompt templates. It enables a workflow where algorithms manage and optimize the prompt template, while agents consume the template to perform rollouts. This is particularly useful for prompt optimization scenarios.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>PromptRolloutFunc[T] | None</code>, default:                   <code>None</code> )           \u2013            <p>Callable defining the agent's behavior. Supported signatures include:</p> <ul> <li><code>(task, prompt_template) -&gt; result</code></li> <li><code>(task, prompt_template, rollout) -&gt; result</code></li> <li><code>async (task, prompt_template) -&gt; result</code></li> <li><code>async (task, prompt_template, rollout) -&gt; result</code></li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>FunctionalLitAgent[T] | Callable[[PromptRolloutFunc[T]], FunctionalLitAgent[T]]</code>           \u2013            <p><code>FunctionalLitAgent</code> that</p> </li> <li> <code>FunctionalLitAgent[T] | Callable[[PromptRolloutFunc[T]], FunctionalLitAgent[T]]</code>           \u2013            <p>wraps the supplied function.</p> </li> </ul> <p>Examples:</p> <pre><code>@prompt_rollout\ndef my_agent(task, prompt_template):\n    messages = prompt_template.format(task=task.input)\n    return messages\n\nresult = my_agent(task, prompt_template)\nresult = my_agent.rollout(task, resources, rollout)\n</code></pre>"},{"location":"reference/internal/#agentlightning.emitter.annotation.OperationContext","title":"<code>agentlightning.emitter.annotation.OperationContext</code>","text":"<p>Context manager and decorator for tracing operations.</p> <p>This class manages a tracer-backed span for a logical unit of work. It can be used either:</p> <ul> <li>As a decorator, in which case inputs and outputs are inferred   automatically from the wrapped function's signature.</li> <li>As a context manager, in which case inputs and outputs can be recorded   explicitly via <code>set_input</code>   and <code>set_output</code>.</li> </ul> <p>Attributes:</p> <ul> <li> <code>name</code>           \u2013            <p>Human-readable span name.</p> </li> <li> <code>initial_attributes</code>           \u2013            <p>Attributes applied when the span is created.</p> </li> <li> <code>tracer</code>           \u2013            <p>Tracer implementation used to create spans.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.emitter.annotation.OperationContext.__call__","title":"<code>__call__(fn)</code>","text":"<p>Wrap a callable so its execution is traced in a span.</p> <p>When used as a decorator, a new span is created for each call to the wrapped function. The bound arguments are recorded as input attributes, the return value is recorded as an output attribute, and any exception is recorded and marks the span as an error.</p> <p>Parameters:</p> <ul> <li> <code>fn</code>               (<code>_FnType</code>)           \u2013            <p>The function or coroutine function to wrap.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>_FnType</code>           \u2013            <p>The wrapped callable.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.emitter.annotation.OperationContext.__enter__","title":"<code>__enter__()</code>","text":"<p>Enter the context manager and start a new span.</p> <p>Returns:</p> <ul> <li> <code>OperationContext</code>           \u2013            <p>The current :class:<code>OperationContext</code> instance with an active span.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.emitter.annotation.OperationContext.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Exit the context manager and finish the span.</p>"},{"location":"reference/internal/#agentlightning.emitter.annotation.OperationContext.__init__","title":"<code>__init__(name, attributes, propagate=True)</code>","text":"<p>Initialize a new operation context.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Human-readable name of the span.</p> </li> <li> <code>attributes</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Initial attributes attached to the span. Values are JSON-serialized where necessary.</p> </li> <li> <code>propagate</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether the span should be sent to active exporters.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.emitter.annotation.OperationContext.set_input","title":"<code>set_input(*args, **kwargs)</code>","text":"<p>Record input arguments on the current span.</p> <p>Positional arguments are stored under the <code>input.args.&lt;index&gt;</code> attributes, and keyword arguments are stored under <code>input.&lt;name&gt;</code> attributes.</p> <p>This is intended for use inside a <code>with operation(...) as op</code> block.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Positional arguments to record.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments to record.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.emitter.annotation.OperationContext.set_output","title":"<code>set_output(output)</code>","text":"<p>Record the output value on the current span.</p> <p>This is intended for use inside a <code>with operation(...) as op</code> block.</p> <p>Parameters:</p> <ul> <li> <code>output</code>               (<code>Any</code>)           \u2013            <p>The output value to record.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.emitter.annotation.OperationContext.span","title":"<code>span()</code>","text":"<p>Get the span that was created by this context manager.</p>"},{"location":"reference/internal/#llm-proxy","title":"LLM Proxy","text":""},{"location":"reference/internal/#agentlightning.llm_proxy.ModelConfig","title":"<code>agentlightning.llm_proxy.ModelConfig</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>LiteLLM model registration entry.</p> <p>This mirrors the items in LiteLLM's <code>model_list</code> section.</p> <p>Attributes:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Logical model name exposed by the proxy.</p> </li> <li> <code>litellm_params</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Parameters passed to LiteLLM for this model (e.g., backend model id, api_base, additional options).</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.llm_proxy.LightningSpanExporter","title":"<code>agentlightning.llm_proxy.LightningSpanExporter</code>","text":"<p>               Bases: <code>SpanExporter</code></p> <p>Buffered OTEL span exporter with subtree flushing and training-store sink.</p> <p>Design:</p> <ul> <li>Spans are buffered until a root span's entire subtree is available.</li> <li>A private event loop on a daemon thread runs async flush logic.</li> <li>Rollout/attempt/sequence metadata is reconstructed by merging headers   from any span within a subtree.</li> </ul> <p>Thread-safety:</p> <ul> <li>Buffer access is protected by a re-entrant lock.</li> <li>Export is synchronous to the caller yet schedules an async flush on the   internal loop, then waits for completion.</li> </ul>"},{"location":"reference/internal/#agentlightning.llm_proxy.LightningSpanExporter.export","title":"<code>export(spans)</code>","text":"<p>Export spans via buffered subtree flush.</p> <p>Appends spans to the internal buffer, then triggers an async flush on the private event loop. Blocks until that flush completes.</p> <p>Parameters:</p> <ul> <li> <code>spans</code>               (<code>Sequence[ReadableSpan]</code>)           \u2013            <p>Sequence of spans to export.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SpanExportResult</code> (              <code>SpanExportResult</code> )          \u2013            <p>SUCCESS on flush success, else FAILURE.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.llm_proxy.LightningSpanExporter.shutdown","title":"<code>shutdown()</code>","text":"<p>Shut down the exporter event loop.</p> <p>Safe to call at process exit.</p>"},{"location":"reference/internal/#agentlightning.llm_proxy.LightningOpenTelemetry","title":"<code>agentlightning.llm_proxy.LightningOpenTelemetry</code>","text":"<p>               Bases: <code>OpenTelemetry</code></p> <p>OpenTelemetry integration that exports spans to the Lightning store.</p> <p>Responsibilities:</p> <ul> <li>Ensures each request is annotated with a per-attempt sequence id so spans   are ordered deterministically even with clock skew across nodes.</li> <li>Uses <code>LightningSpanExporter</code> to persist spans for analytics and training.</li> </ul>"},{"location":"reference/internal/#agentlightning.llm_proxy.LightningOpenTelemetry.async_pre_call_deployment_hook","title":"<code>async_pre_call_deployment_hook(kwargs, call_type=None)</code>  <code>async</code>","text":"<p>The root span is sometimes missing (e.g., when Anthropic endpoint is used). It is created in an auth module in LiteLLM. If it's missing, we create it here.</p>"},{"location":"reference/internal/#agentlightning.llm_proxy.AddReturnTokenIds","title":"<code>agentlightning.llm_proxy.AddReturnTokenIds</code>","text":"<p>               Bases: <code>CustomLogger</code></p> <p>LiteLLM logger hook to request token ids from vLLM.</p> <p>This mutates the outgoing request payload to include <code>return_token_ids=True</code> for backends that support token id return (e.g., vLLM).</p> See also <p>vLLM PR #22587</p>"},{"location":"reference/internal/#agentlightning.llm_proxy.AddReturnTokenIds.async_pre_call_hook","title":"<code>async_pre_call_hook(*args, **kwargs)</code>  <code>async</code>","text":"<p>Async pre-call hook to adjust request payload.</p> <p>Parameters:</p> <ul> <li> <code>args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Positional args from LiteLLM.</p> </li> <li> <code>kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword args from LiteLLM.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Union[Exception, str, Dict[str, Any]]]</code>           \u2013            <p>Either an updated payload dict or an Exception to short-circuit.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.llm_proxy.StreamConversionMiddleware","title":"<code>agentlightning.llm_proxy.StreamConversionMiddleware</code>","text":"<p>               Bases: <code>BaseHTTPMiddleware</code></p> <p>Middleware to convert streaming responses to non-streaming responses.</p> <p>Useful for backend that only supports non-streaming responses.</p> <p>LiteLLM's OpenTelemetry is also buggy with streaming responses. The conversion will hopefully bypass the bug.</p>"},{"location":"reference/internal/#agentlightning.llm_proxy.StreamConversionMiddleware.anthropic_stream_generator","title":"<code>anthropic_stream_generator(original_response)</code>  <code>async</code>","text":"<p>Generate Anthropic SSE-formatted chunks from complete content blocks</p> <p>This is a dirty hack for Anthropic-style streaming from non-streaming response. The sse format is subject to change based on Anthropic's implementation. If so, try to use <code>MessageInspectionMiddleware</code> to inspect the update and fix accordingly.</p>"},{"location":"reference/internal/#agentlightning.llm_proxy.StreamConversionMiddleware.openai_stream_generator","title":"<code>openai_stream_generator(response_json)</code>  <code>async</code>","text":"<p>Convert a complete OpenAI chat.completions choice into a stream of OpenAI-compatible SSE chunks.</p> <p>This emits:</p> <ul> <li>an initial delta with the role (\"assistant\"),</li> <li>a sequence of deltas for message.content (split into small chunks),</li> <li>deltas for any tool_calls (including id/name and chunked arguments),</li> <li>a terminal chunk with finish_reason,</li> <li>and finally the literal '[DONE]'.</li> </ul> <p>Notes:</p> <ul> <li>We only handle a single choice (index 0 typically).</li> <li>We purposefully don't attempt to stream logprobs.</li> <li>Chunking strategy is simple and conservative to avoid splitting   multi-byte characters: we slice on spaces where possible, then fall   back to fixed-size substrings.</li> </ul>"},{"location":"reference/internal/#agentlightning.llm_proxy.MessageInspectionMiddleware","title":"<code>agentlightning.llm_proxy.MessageInspectionMiddleware</code>","text":"<p>               Bases: <code>BaseHTTPMiddleware</code></p> <p>Middleware to inspect the request and response bodies.</p> <p>It's for debugging purposes. Add it via \"message_inspection\" middleware alias.</p>"},{"location":"reference/internal/#agentlightning.llm_proxy.RolloutAttemptMiddleware","title":"<code>agentlightning.llm_proxy.RolloutAttemptMiddleware</code>","text":"<p>               Bases: <code>BaseHTTPMiddleware</code></p> <p>Rewrites /rollout/{rid}/attempt/{aid}/... -&gt; /... and injects x-rollout-id, x-attempt-id, x-sequence-id headers.</p> <p>LLMProxy can update store later without rebuilding middleware.</p>"},{"location":"reference/internal/#store","title":"Store","text":""},{"location":"reference/internal/#agentlightning.store.base.UNSET","title":"<code>agentlightning.store.base.UNSET = _UnsetType()</code>  <code>module-attribute</code>","text":""},{"location":"reference/internal/#agentlightning.store.utils.rollout_status_from_attempt","title":"<code>agentlightning.store.utils.rollout_status_from_attempt(attempt, config)</code>  <code>async</code>","text":"<p>Propagate the status of an attempt to the rollout.</p> <p>Returns:</p> <ul> <li> <code>RolloutStatus</code>           \u2013            <p>The status of the rollout from the perspective of the attempt.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.store.utils.scan_unhealthy_rollouts","title":"<code>agentlightning.store.utils.scan_unhealthy_rollouts(rollouts)</code>  <code>async</code>","text":"<p>Perform health check on all running rollouts in the store.</p> <p>This method should be called periodically to:</p> <ol> <li>Check for unresponsive attempts (no heartbeat or spans for a while)</li> <li>Check for timed-out rollouts (running too long since start_time)</li> </ol> <p>This operation is completely unlocked. The caller is responsible for locking the store.</p> <p>Parameters:</p> <ul> <li> <code>rollouts</code>               (<code>List[AttemptedRollout]</code>)           \u2013            <p>The list of running rollouts to check.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[Tuple[str, str], AttemptStatus]</code>           \u2013            <p>A dictionary of updates to the rollouts.</p> </li> </ul>"},{"location":"reference/internal/#tracing-and-opentelemetry","title":"Tracing and OpenTelemetry","text":""},{"location":"reference/internal/#agentlightning.tracer.otel.LightningSpanProcessor","title":"<code>agentlightning.tracer.otel.LightningSpanProcessor</code>","text":"<p>               Bases: <code>SpanProcessor</code></p> <p>Span processor that subclasses OpenTelemetry's <code>SpanProcessor</code> and adds support to dump traces to a <code>LightningStore</code>.</p> <p>It serves two purposes:</p> <ol> <li>Records all the spans in a local buffer.</li> <li>Submits the spans to the event loop to be added to the store.</li> </ol>"},{"location":"reference/internal/#agentlightning.tracer.otel.LightningSpanProcessor.attempt_id","title":"<code>attempt_id</code>  <code>property</code>","text":"<p>The attempt ID to submit the spans to.</p>"},{"location":"reference/internal/#agentlightning.tracer.otel.LightningSpanProcessor.disable_store_submission","title":"<code>disable_store_submission</code>  <code>property</code> <code>writable</code>","text":"<p>Whether to disable submitting spans to the store.</p>"},{"location":"reference/internal/#agentlightning.tracer.otel.LightningSpanProcessor.rollout_id","title":"<code>rollout_id</code>  <code>property</code>","text":"<p>The rollout ID to submit the spans to.</p>"},{"location":"reference/internal/#agentlightning.tracer.otel.LightningSpanProcessor.store","title":"<code>store</code>  <code>property</code>","text":"<p>The store to submit the spans to.</p>"},{"location":"reference/internal/#agentlightning.tracer.otel.LightningSpanProcessor.on_end","title":"<code>on_end(span)</code>","text":"<p>Process a span when it ends.</p> <p>Parameters:</p> <ul> <li> <code>span</code>               (<code>ReadableSpan</code>)           \u2013            <p>The span that has ended.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.tracer.otel.LightningSpanProcessor.spans","title":"<code>spans()</code>","text":"<p>Get the list of spans collected by this processor. This is useful for debugging and testing purposes.</p> <p>Returns:</p> <ul> <li> <code>List[Span]</code>           \u2013            <p>List of <code>Span</code> objects collected during tracing.</p> </li> </ul>"},{"location":"reference/internal/#deprecated-apis","title":"Deprecated APIs","text":""},{"location":"reference/internal/#agentlightning.emitter.reward.reward","title":"<code>agentlightning.emitter.reward.reward(fn)</code>","text":"<p>Decorate a reward function so its outputs are tracked as spans.</p> <p>The decorator integrates with AgentOps when it is available and falls back to the built-in telemetry otherwise. Both synchronous and asynchronous functions are supported transparently.</p> Deprecated <p>This decorator is deprecated. Use <code>emit_reward</code> instead.</p> <p>Parameters:</p> <ul> <li> <code>fn</code>               (<code>_FnType</code>)           \u2013            <p>Callable that produces a numeric reward.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>_FnType</code>           \u2013            <p>Wrapped callable that preserves the original signature.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.server.AgentLightningServer","title":"<code>agentlightning.server.AgentLightningServer</code>","text":"<p>High-level controller for the legacy Agent Lightning FastAPI server.</p> <p>The controller orchestrates server start-up, task queueing, resource updates, and retrieval of client rollouts. It is primarily used by existing systems that still rely on the HTTP-based workflow.</p> <p>Deprecated</p> <p><code>AgentLightningServer</code> is part of the legacy client/server stack. Prefer the store-based runtime for new integrations.</p>"},{"location":"reference/internal/#agentlightning.server.AgentLightningServer.__init__","title":"<code>__init__(host='127.0.0.1', port=8000, task_timeout_seconds=300.0)</code>","text":"<p>Initialize the controller.</p> <p>Parameters:</p> <ul> <li> <code>host</code>               (<code>str</code>, default:                   <code>'127.0.0.1'</code> )           \u2013            <p>Hostname or IP address to bind the HTTP server to.</p> </li> <li> <code>port</code>               (<code>int</code>, default:                   <code>8000</code> )           \u2013            <p>TCP port exposed by the server.</p> </li> <li> <code>task_timeout_seconds</code>               (<code>float</code>, default:                   <code>300.0</code> )           \u2013            <p>Seconds before a claimed task is considered stale and re-queued.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.server.AgentLightningServer.get_completed_rollout","title":"<code>get_completed_rollout(rollout_id)</code>  <code>async</code>","text":"<p>Retrieve a specific completed rollout by identifier.</p>"},{"location":"reference/internal/#agentlightning.server.AgentLightningServer.poll_completed_rollout","title":"<code>poll_completed_rollout(rollout_id, timeout=None)</code>  <code>async</code>","text":"<p>Poll for a completed rollout until it becomes available or a timeout expires.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the rollout to wait for.</p> </li> <li> <code>timeout</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of seconds to wait. <code>None</code> waits indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[RolloutLegacy]</code>           \u2013            <p>Retrieved rollout, or <code>None</code> when the timeout is reached without success.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.server.AgentLightningServer.queue_task","title":"<code>queue_task(sample, mode=None, resources_id=None, metadata=None)</code>  <code>async</code>","text":"<p>Add a task to the queue for a client to process.</p>"},{"location":"reference/internal/#agentlightning.server.AgentLightningServer.retrieve_completed_rollouts","title":"<code>retrieve_completed_rollouts()</code>  <code>async</code>","text":"<p>Return every completed rollout and clear the internal buffer.</p>"},{"location":"reference/internal/#agentlightning.server.AgentLightningServer.run_forever","title":"<code>run_forever()</code>  <code>async</code>","text":"<p>Run the server indefinitely until <code>stop()</code> is invoked.</p>"},{"location":"reference/internal/#agentlightning.server.AgentLightningServer.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Start the FastAPI server in the background.</p>"},{"location":"reference/internal/#agentlightning.server.AgentLightningServer.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop the FastAPI server and wait for a graceful shutdown.</p>"},{"location":"reference/internal/#agentlightning.server.AgentLightningServer.update_resources","title":"<code>update_resources(resources)</code>  <code>async</code>","text":"<p>Publish a new resource bundle and return its generated identifier.</p>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore","title":"<code>agentlightning.server.ServerDataStore</code>","text":"<p>Async-safe container for in-memory server state.</p> <p>The store tracks queued tasks, claimed tasks, uploaded rollouts, and the currently published resources. All interactions are guarded by asyncio locks so that the FastAPI handlers can safely run in parallel.</p> <p>Deprecated</p> <p><code>ServerDataStore</code> is part of the legacy client/server stack. Use <code>LightningStore</code> instead.</p>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore.add_task","title":"<code>add_task(sample, mode=None, resources_id=None, metadata=None)</code>  <code>async</code>","text":"<p>Enqueue a new task and return the generated rollout identifier.</p> <p>Parameters:</p> <ul> <li> <code>sample</code>               (<code>Any</code>)           \u2013            <p>Payload that describes the task input.</p> </li> <li> <code>mode</code>               (<code>Literal['train', 'val', 'test'] | None</code>, default:                   <code>None</code> )           \u2013            <p>Phase in which the sample should be executed (<code>\"train\"</code>, <code>\"val\"</code>, or <code>\"test\"</code>).</p> </li> <li> <code>resources_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Identifier of a resource bundle that the executor should load before running the task.</p> </li> <li> <code>metadata</code>               (<code>Dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional metadata forwarded to the executor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Unique rollout identifier assigned to the task.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore.get_latest_resources","title":"<code>get_latest_resources()</code>  <code>async</code>","text":"<p>Return the most recent resource bundle, if one exists.</p>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore.get_next_task","title":"<code>get_next_task()</code>  <code>async</code>","text":"<p>Retrieve the next task from the queue without blocking.</p> <p>Returns:</p> <ul> <li> <code>Optional[Task]</code>           \u2013            <p>Next <code>Task</code> ready to execute, or <code>None</code></p> </li> <li> <code>Optional[Task]</code>           \u2013            <p>when the queue is empty.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore.get_processing_tasks","title":"<code>get_processing_tasks()</code>","text":"<p>Return a copy of currently processing tasks for timeout checking.</p>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore.get_resources_by_id","title":"<code>get_resources_by_id(resources_id)</code>  <code>async</code>","text":"<p>Retrieve a specific resource bundle by identifier.</p> <p>Parameters:</p> <ul> <li> <code>resources_id</code>               (<code>str</code>)           \u2013            <p>Identifier that was previously published to the store.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>Matching <code>ResourcesUpdate</code></p> </li> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>instance, or <code>None</code> when the identifier is unknown.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore.requeue_task","title":"<code>requeue_task(task)</code>  <code>async</code>","text":"<p>Requeue a task that timed out while being processed.</p>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore.retrieve_completed_rollouts","title":"<code>retrieve_completed_rollouts()</code>  <code>async</code>","text":"<p>Return all completed rollouts and clear the internal buffer.</p>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore.retrieve_rollout","title":"<code>retrieve_rollout(rollout_id)</code>  <code>async</code>","text":"<p>Retrieve and remove a stored rollout by identifier.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the rollout to fetch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[RolloutLegacy]</code>           \u2013            <p>Stored <code>RolloutLegacy</code>, or <code>None</code></p> </li> <li> <code>Optional[RolloutLegacy]</code>           \u2013            <p>when the identifier is unknown.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore.store_rollout","title":"<code>store_rollout(rollout)</code>  <code>async</code>","text":"<p>Persist a completed rollout for later inspection.</p> <p>Parameters:</p> <ul> <li> <code>rollout</code>               (<code>RolloutLegacy</code>)           \u2013            <p>Rollout returned by a client.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore.update_resources","title":"<code>update_resources(update)</code>  <code>async</code>","text":"<p>Persist a new resource bundle and mark it as the latest version.</p> <p>Parameters:</p> <ul> <li> <code>update</code>               (<code>ResourcesUpdate</code>)           \u2013            <p>Resource payload received from a client.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.AgentLightningClient","title":"<code>agentlightning.client.AgentLightningClient</code>","text":"<p>Client wrapper for the legacy version-aware Agent Lightning server.</p> <p>The client exposes synchronous and asynchronous helpers for polling tasks, retrieving resource bundles, and submitting rollouts. It also maintains a simple in-memory cache keyed by the server-provided resource identifier to avoid redundant network requests.</p> <p>Deprecated</p> <p><code>AgentLightningClient</code> is part of the legacy client/server stack. New code should rely on the store-based APIs implemented in <code>agentlightning.store</code>.</p> <p>Attributes:</p> <ul> <li> <code>endpoint</code>           \u2013            <p>Base URL of the Agent Lightning server.</p> </li> <li> <code>poll_interval</code>           \u2013            <p>Delay in seconds between polling attempts when no task is available.</p> </li> <li> <code>timeout</code>           \u2013            <p>Timeout in seconds applied to HTTP requests.</p> </li> <li> <code>task_count</code>           \u2013            <p>Number of tasks claimed during the lifetime of this client.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.AgentLightningClient.__init__","title":"<code>__init__(endpoint, poll_interval=5.0, timeout=10.0)</code>","text":"<p>Initialize the client.</p> <p>Parameters:</p> <ul> <li> <code>endpoint</code>               (<code>str</code>)           \u2013            <p>Root URL of the Agent Lightning server.</p> </li> <li> <code>poll_interval</code>               (<code>float</code>, default:                   <code>5.0</code> )           \u2013            <p>Seconds to wait between polling attempts.</p> </li> <li> <code>timeout</code>               (<code>float</code>, default:                   <code>10.0</code> )           \u2013            <p>Seconds before a request to the server is considered timed out.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.AgentLightningClient.get_latest_resources","title":"<code>get_latest_resources()</code>","text":"<p>Fetch the most recent resource bundle advertised by the server.</p> <p>Returns:</p> <ul> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p><code>ResourcesUpdate</code> for the</p> </li> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>newest version, or <code>None</code> when unavailable.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.AgentLightningClient.get_latest_resources_async","title":"<code>get_latest_resources_async()</code>  <code>async</code>","text":"<p>Fetch the most recent resource bundle advertised by the server.</p> <p>Returns:</p> <ul> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p><code>ResourcesUpdate</code> for the</p> </li> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>newest version, or <code>None</code> when unavailable.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.AgentLightningClient.get_resources_by_id","title":"<code>get_resources_by_id(resource_id)</code>","text":"<p>Fetch a specific resource bundle by identifier.</p> <p>Parameters:</p> <ul> <li> <code>resource_id</code>               (<code>str</code>)           \u2013            <p>Identifier sourced from the task metadata.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>Cached or freshly downloaded</p> </li> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p><code>ResourcesUpdate</code>, or</p> </li> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p><code>None</code> when the server returns an error.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.AgentLightningClient.get_resources_by_id_async","title":"<code>get_resources_by_id_async(resource_id)</code>  <code>async</code>","text":"<p>Fetch a specific resource bundle by identifier.</p> <p>Parameters:</p> <ul> <li> <code>resource_id</code>               (<code>str</code>)           \u2013            <p>Identifier sourced from the task metadata.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>Cached or freshly downloaded</p> </li> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p><code>ResourcesUpdate</code>, or</p> </li> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p><code>None</code> when the server returns an error.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.AgentLightningClient.poll_next_task","title":"<code>poll_next_task()</code>","text":"<p>Poll the server synchronously until a task becomes available.</p> <p>Returns:</p> <ul> <li> <code>Optional[Task]</code>           \u2013            <p>The next <code>Task</code> available for execution, or</p> </li> <li> <code>Optional[Task]</code>           \u2013            <p><code>None</code> if polling fails.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.AgentLightningClient.poll_next_task_async","title":"<code>poll_next_task_async()</code>  <code>async</code>","text":"<p>Poll the server asynchronously until a task becomes available.</p> <p>Returns:</p> <ul> <li> <code>Optional[Task]</code>           \u2013            <p>The next <code>Task</code> exposed by the server,</p> </li> <li> <code>Optional[Task]</code>           \u2013            <p>or <code>None</code> if polling fails.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.AgentLightningClient.post_rollout","title":"<code>post_rollout(rollout)</code>","text":"<p>Submit a completed rollout back to the server.</p> <p>Parameters:</p> <ul> <li> <code>rollout</code>               (<code>RolloutLegacy</code>)           \u2013            <p>Legacy rollout payload produced by the executor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Dict[str, Any]]</code>           \u2013            <p>Parsed JSON response returned by the server, or <code>None</code> when the request fails.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.AgentLightningClient.post_rollout_async","title":"<code>post_rollout_async(rollout)</code>  <code>async</code>","text":"<p>Submit a completed rollout back to the server.</p> <p>Parameters:</p> <ul> <li> <code>rollout</code>               (<code>RolloutLegacy</code>)           \u2013            <p>Legacy rollout payload produced by the executor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Dict[str, Any]]</code>           \u2013            <p>Parsed JSON response returned by the server, or <code>None</code> when the request fails.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.DevTaskLoader","title":"<code>agentlightning.client.DevTaskLoader</code>","text":"<p>               Bases: <code>AgentLightningClient</code></p> <p>In-memory task loader used for development and integration tests.</p> <p>The loader mimics the behavior of the legacy HTTP server by storing tasks and resources locally. Polling methods simply iterate over the provided collection, allowing rapid iteration without provisioning any external infrastructure.</p> <p>Deprecated</p> <p><code>DevTaskLoader</code> is a compatibility shim. Prefer <code>Trainer.dev</code> for new code.</p>"},{"location":"reference/internal/#agentlightning.client.DevTaskLoader.rollouts","title":"<code>rollouts</code>  <code>property</code>","text":"<p>Return the rollouts posted back to the loader during development runs.</p>"},{"location":"reference/internal/#agentlightning.client.DevTaskLoader.__init__","title":"<code>__init__(tasks, resources, **kwargs)</code>","text":"<p>Initialize the loader with predefined tasks and resources.</p> <p>Parameters:</p> <ul> <li> <code>tasks</code>               (<code>Union[List[TaskInput], List[Task]]</code>)           \u2013            <p>Sequence of task inputs or preconstructed tasks that will be served in order.</p> </li> <li> <code>resources</code>               (<code>Union[NamedResources, ResourcesUpdate]</code>)           \u2013            <p>Static resources returned for any <code>resources_id</code> query.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments forwarded to the parent client.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no tasks are provided or both <code>Task</code> and <code>TaskInput</code> instances are mixed.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.DevTaskLoader.poll_next_task","title":"<code>poll_next_task()</code>","text":"<p>Return the next task from the local queue.</p> <p>If <code>TaskInput</code> instances were provided, they are converted into <code>Task</code> objects on the fly. Otherwise, the preconstructed tasks are returned in sequence.</p> <p>Returns:</p> <ul> <li> <code>Optional[Task]</code>           \u2013            <p>Next task to execute.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.Task","title":"<code>agentlightning.Task</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Rollout request served to client agents.</p> <p>Deprecated</p> <p>The legacy HTTP client/server stack still uses this model. Prefer <code>LightningStore</code> APIs for new workflows.</p>"},{"location":"reference/internal/#agentlightning.TaskInput","title":"<code>agentlightning.TaskInput = Any</code>  <code>module-attribute</code>","text":"<p>Task input type. Accepts arbitrary payloads.</p>"},{"location":"reference/internal/#agentlightning.TaskIfAny","title":"<code>agentlightning.TaskIfAny</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A task or indication that no task is available.</p> <p>Deprecated</p> <p>Use <code>LightningStore</code> APIs for new workflows.</p>"},{"location":"reference/internal/#agentlightning.TaskIfAny.is_available","title":"<code>is_available</code>  <code>instance-attribute</code>","text":"<p>Indication that a task is available.</p>"},{"location":"reference/internal/#agentlightning.RolloutRawResultLegacy","title":"<code>agentlightning.RolloutRawResultLegacy = Union[None, float, List[Triplet], List[Dict[str, Any]], List[ReadableSpan], RolloutLegacy]</code>  <code>module-attribute</code>","text":"<p>Legacy rollout result type.</p> <p>Deprecated</p> <p>Use <code>RolloutRawResult</code> instead.</p>"},{"location":"reference/internal/#agentlightning.RolloutLegacy","title":"<code>agentlightning.RolloutLegacy</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Legacy reporting payload exchanged with the deprecated HTTP server.</p> <p>Deprecated</p> <p>Use <code>Rollout</code> instead.</p>"},{"location":"reference/restful/","title":"RESTful API References","text":"<p>Note</p> <p>Shown in the following is the RESTful API for Lightning Store.</p> <p></p>"},{"location":"reference/runner/","title":"Runner-side References","text":"<p>Note</p> <p>This reference covers APIs that are designed to be used at \"Runner Side\".</p>"},{"location":"reference/runner/#runners","title":"Runners","text":""},{"location":"reference/runner/#agentlightning.LitAgentRunner","title":"<code>agentlightning.LitAgentRunner</code>","text":"<p>               Bases: <code>Runner[T_task]</code></p> <p>Execute <code>LitAgent</code> tasks with tracing support.</p> <p>This runner manages the complete lifecycle of agent rollout execution, including task polling, resource management, tracing, and hooks. It supports both continuous iteration over tasks from the store and single-step execution.</p> <p>Attributes:</p> <ul> <li> <code>worker_id</code>               (<code>Optional[int]</code>)           \u2013            <p>Identifier for the active worker process, if any.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.tracer","title":"<code>tracer</code>  <code>property</code>","text":"<p>Get the tracer instance.</p> <p>Returns:</p> <ul> <li> <code>Tracer</code>           \u2013            <p>The Tracer instance used by this runner.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.__init__","title":"<code>__init__(tracer, max_rollouts=None, poll_interval=5.0, heartbeat_interval=10.0, interval_jitter=0.5, heartbeat_launch_mode='thread', heartbeat_include_gpu=False)</code>","text":"<p>Initialize the agent runner.</p> <p>Parameters:</p> <ul> <li> <code>tracer</code>               (<code>Tracer</code>)           \u2013            <p><code>Tracer</code> used for rollout spans.</p> </li> <li> <code>max_rollouts</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Optional cap on iterations processed by <code>iter</code>.</p> </li> <li> <code>poll_interval</code>               (<code>float</code>, default:                   <code>5.0</code> )           \u2013            <p>Seconds to wait between store polls when no work is available.</p> </li> <li> <code>heartbeat_interval</code>               (<code>float</code>, default:                   <code>10.0</code> )           \u2013            <p>Seconds to wait between sending heartbeats to the store.</p> </li> <li> <code>interval_jitter</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Jitter factor for the poll interval. The actual interval will be between poll_interval - interval_jitter and poll_interval + interval_jitter. This is to avoid the overload caused by the synchronization of the runners.</p> </li> <li> <code>heartbeat_launch_mode</code>               (<code>Literal['asyncio', 'thread']</code>, default:                   <code>'thread'</code> )           \u2013            <p>Launch mode for the heartbeat loop. Can be \"asyncio\" or \"thread\". \"thread\" is the default and recommended mode as it prevents blocking the event loop under load. Use \"asyncio\" for simpler deployments with low worker counts.</p> </li> <li> <code>heartbeat_include_gpu</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to include GPU stats in heartbeat snapshots. Querying GPU stats can be slow under load, so this is disabled by default.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.get_agent","title":"<code>get_agent()</code>","text":"<p>Get the agent instance.</p> <p>Returns:</p> <ul> <li> <code>LitAgent[T_task]</code>           \u2013            <p>The LitAgent instance managed by this runner.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the agent has not been initialized via <code>init</code>.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.get_store","title":"<code>get_store()</code>","text":"<p>Get the store instance.</p> <p>Returns:</p> <ul> <li> <code>LightningStore</code>           \u2013            <p>The LightningStore instance for this worker.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the store has not been initialized via <code>init_worker</code>.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.get_worker_id","title":"<code>get_worker_id()</code>","text":"<p>Get the formatted worker ID string.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>A formatted string like \"Worker-0\" if initialized, or \"Worker-Unknown\"</p> </li> <li> <code>str</code>           \u2013            <p>if the worker ID has not been set.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.init","title":"<code>init(agent, *, hooks=None, **kwargs)</code>","text":"<p>Initialize the runner with the agent.</p> <p>This sets up the agent-runner relationship, registers hooks, and initializes the tracer.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>LitAgent[T_task]</code>)           \u2013            <p><code>LitAgent</code> instance executed by the runner.</p> </li> <li> <code>hooks</code>               (<code>Optional[Sequence[Hook]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional sequence of <code>Hook</code> callbacks invoked around tracing and rollout boundaries.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional initialization arguments (currently unused).</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.init_worker","title":"<code>init_worker(worker_id, store, **kwargs)</code>","text":"<p>Initialize the runner for each worker with worker_id and store.</p> <p>This method is called once per worker in a distributed setup to provide the worker with its ID and store connection.</p> <p>Parameters:</p> <ul> <li> <code>worker_id</code>               (<code>int</code>)           \u2013            <p>Unique identifier for this worker process.</p> </li> <li> <code>store</code>               (<code>LightningStore</code>)           \u2013            <p><code>LightningStore</code> used for task coordination and persistence.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional worker-specific initialization arguments (currently unused).</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.iter","title":"<code>iter(*, event=None)</code>  <code>async</code>","text":"<p>Run the runner, continuously iterating over tasks in the store.</p> <p>This method polls the store for new rollouts and executes them until:</p> <ul> <li>The event is set (if provided)</li> <li>The max_rollouts limit is reached (if configured)</li> <li>No more tasks are available</li> </ul> <p>All exceptions during rollout execution are caught and logged but not propagated, allowing the runner to continue processing subsequent tasks.</p> <p>Parameters:</p> <ul> <li> <code>event</code>               (<code>Optional[ExecutionEvent]</code>, default:                   <code>None</code> )           \u2013            <p>Optional ExecutionEvent object to signal the runner to stop. The runner will check this event periodically and stop gracefully when set.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.step","title":"<code>step(input, *, resources=None, mode=None, event=None)</code>  <code>async</code>","text":"<p>Execute a single task directly, bypassing the task queue.</p> <p>This method creates a new rollout for the given input and executes it immediately. Unlike <code>iter()</code>, exceptions are propagated to the caller.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>T_task</code>)           \u2013            <p>The task input to be processed by the agent.</p> </li> <li> <code>resources</code>               (<code>Optional[NamedResources]</code>, default:                   <code>None</code> )           \u2013            <p>Optional named resources to be used for this specific task. If provided, a new resources entry will be created in the store. If not provided, the latest resources from the store will be used.</p> </li> <li> <code>mode</code>               (<code>Optional[RolloutMode]</code>, default:                   <code>None</code> )           \u2013            <p>Optional rollout mode (\"train\" or \"validation\"). If not provided, the agent's default mode will be used.</p> </li> <li> <code>event</code>               (<code>Optional[ExecutionEvent]</code>, default:                   <code>None</code> )           \u2013            <p>Optional ExecutionEvent object to signal interruption (currently unused but included for interface consistency).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Rollout</code>           \u2013            <p>The completed rollout.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>Exception</code>             \u2013            <p>Any exception that occurs during rollout execution will be re-raised to the caller.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.teardown","title":"<code>teardown(*args, **kwargs)</code>","text":"<p>Teardown the runner and clean up all resources.</p> <p>This method resets all internal state including the agent, store, hooks, and worker ID, and calls the tracer's teardown method.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Additional teardown arguments (currently unused).</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional teardown keyword arguments (currently unused).</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.teardown_worker","title":"<code>teardown_worker(worker_id, *args, **kwargs)</code>","text":"<p>Teardown the runner for a specific worker.</p> <p>This method cleans up worker-specific resources and resets the worker ID.</p> <p>Parameters:</p> <ul> <li> <code>worker_id</code>               (<code>int</code>)           \u2013            <p>Unique identifier of the worker being torn down.</p> </li> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Additional teardown arguments (currently unused).</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional teardown keyword arguments (currently unused).</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Runner","title":"<code>agentlightning.Runner</code>","text":"<p>               Bases: <code>ParallelWorkerBase</code>, <code>Generic[T_task]</code></p> <p>Abstract base class for long-running agent executors.</p> <p>Runner implementations coordinate <code>LitAgent</code> instances, acquire work from a <code>LightningStore</code>, and emit <code>Rollout</code> objects. Subclasses decide how to schedule work (polling, streaming, etc.) while this base class provides a minimal lifecycle contract.</p>"},{"location":"reference/runner/#agentlightning.Runner.init","title":"<code>init(agent, **kwargs)</code>","text":"<p>Prepare the runner to execute tasks for <code>agent</code>.</p> <p>This method is called only once during the setup for all workers, not for each worker.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>LitAgent[T_task]</code>)           \u2013            <p>Agent instance providing task-specific logic.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Optional runner-specific configuration.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must supply the initialization routine.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Runner.init_worker","title":"<code>init_worker(worker_id, store, **kwargs)</code>","text":"<p>Configure worker-local state before processing tasks.</p> <p>This method is called for each worker during the setup.</p> <p>Parameters:</p> <ul> <li> <code>worker_id</code>               (<code>int</code>)           \u2013            <p>Unique identifier for this worker process or thread.</p> </li> <li> <code>store</code>               (<code>LightningStore</code>)           \u2013            <p>Shared <code>LightningStore</code> backing task coordination.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Optional worker-specific configuration.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must prepare per-worker resources.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Runner.iter","title":"<code>iter(*, event=None)</code>  <code>async</code>","text":"<p>Run the runner, continuously iterating over tasks in the store.</p> <p>This method runs in a loop, polling the store for new tasks and executing them until interrupted by the event or when no more tasks are available.</p> <p>Parameters:</p> <ul> <li> <code>event</code>               (<code>Optional[ExecutionEvent]</code>, default:                   <code>None</code> )           \u2013            <p>Cooperative stop signal. When set, the runner should complete the current unit of work and exit the loop.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses provide the iteration behavior.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Runner.run","title":"<code>run(*args, **kwargs)</code>","text":"<p>Deprecated synchronous entry point.</p> <p>Use <code>iter()</code> or <code>step()</code> instead.</p> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>Always raised to direct callers to iter() or step().</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Runner.run_context","title":"<code>run_context(*, agent, store, hooks=None, worker_id=None)</code>","text":"<p>Initialize and tear down a runner within a simple context manager.</p> <p>The helper is primarily intended for debugging runner implementations outside of a full <code>Trainer</code> stack.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>LitAgent[T_task]</code>)           \u2013            <p>Agent executed by this runner.</p> </li> <li> <code>store</code>               (<code>LightningStore</code>)           \u2013            <p>Backing <code>LightningStore</code>. If you don't have one, you can easily create one with <code>InMemoryLightningStore</code>.</p> </li> <li> <code>hooks</code>               (<code>Optional[Sequence[Hook]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional sequence of hooks recognised by the runner. Not all runners support hooks.</p> </li> <li> <code>worker_id</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Override the worker identifier used during setup. Defaults to <code>0</code>.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Runner.step","title":"<code>step(input, *, resources=None, mode=None, event=None)</code>  <code>async</code>","text":"<p>Execute a single task with the given input.</p> <p>This method provides fine-grained control for executing individual tasks directly, bypassing the store's task queue.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>T_task</code>)           \u2013            <p>Task payload consumed by the agent.</p> </li> <li> <code>resources</code>               (<code>Optional[NamedResources]</code>, default:                   <code>None</code> )           \u2013            <p>Optional named resources scoped to this invocation.</p> </li> <li> <code>mode</code>               (<code>Optional[RolloutMode]</code>, default:                   <code>None</code> )           \u2013            <p>Optional rollout mode such as <code>\"train\"</code> or <code>\"eval\"</code>.</p> </li> <li> <code>event</code>               (<code>Optional[ExecutionEvent]</code>, default:                   <code>None</code> )           \u2013            <p>Cooperative stop signal for long-running tasks.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Rollout</code>           \u2013            <p>Completed rollout produced by the agent.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses provide the execution behavior.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Runner.teardown","title":"<code>teardown(*args, **kwargs)</code>","text":"<p>Release resources acquired during <code>init()</code>.</p> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement the shutdown routine.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Runner.teardown_worker","title":"<code>teardown_worker(worker_id, *args, **kwargs)</code>","text":"<p>Release per-worker resources allocated by <code>init_worker()</code>.</p> <p>Parameters:</p> <ul> <li> <code>worker_id</code>               (<code>int</code>)           \u2013            <p>Identifier of the worker being torn down.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement the shutdown routine.</p> </li> </ul>"},{"location":"reference/runner/#tracer","title":"Tracer","text":""},{"location":"reference/runner/#agentlightning.AgentOpsTracer","title":"<code>agentlightning.AgentOpsTracer</code>","text":"<p>               Bases: <code>OtelTracer</code></p> <p>Traces agent execution using AgentOps.</p> <p>This tracer provides functionality to capture execution details using the AgentOps library. It manages the AgentOps client initialization, server setup, and integration with the OpenTelemetry tracing ecosystem.</p> <p>Attributes:</p> <ul> <li> <code>agentops_managed</code>           \u2013            <p>Whether to automatically manage <code>agentops</code>.               When set to true, tracer calls <code>agentops.init()</code>               automatically and launches an agentops endpoint locally.               If not, you are responsible for calling and using it               before using the tracer.</p> </li> <li> <code>instrument_managed</code>           \u2013            <p>Whether to automatically manage instrumentation.                 When set to false, you will manage the instrumentation                 yourself and the tracer might not work as expected.</p> </li> <li> <code>daemon</code>           \u2013            <p>Whether the AgentOps server runs as a daemon process.     Only applicable if <code>agentops_managed</code> is True.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.AgentOpsTracer.get_langchain_handler","title":"<code>get_langchain_handler(tags=None)</code>","text":"<p>Get the Langchain callback handler for integrating with Langchain.</p> <p>Parameters:</p> <ul> <li> <code>tags</code>               (<code>List[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of tags to apply to the Langchain callback handler.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LangchainCallbackHandler</code>           \u2013            <p>An instance of the Langchain callback handler.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.AgentOpsTracer.trace_context","title":"<code>trace_context(name=None, *, store=None, rollout_id=None, attempt_id=None)</code>  <code>async</code>","text":"<p>Starts a new tracing context. This should be used as a context manager.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional name for the tracing context.</p> </li> <li> <code>store</code>               (<code>Optional[LightningStore]</code>, default:                   <code>None</code> )           \u2013            <p>Optional store to add the spans to.</p> </li> <li> <code>rollout_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional rollout ID to add the spans to.</p> </li> <li> <code>attempt_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional attempt ID to add the spans to.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>AsyncGenerator[Tracer, None]</code>           \u2013            <p>The OpenTelemetry tracer instance to collect spans.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.OtelTracer","title":"<code>agentlightning.OtelTracer</code>","text":"<p>               Bases: <code>Tracer</code></p> <p>Tracer that provides a basic OpenTelemetry tracer provider.</p> <p>You should be able to collect agent-lightning signals like rewards with this tracer, but no other function instrumentations like <code>openai.chat.completion</code>.</p>"},{"location":"reference/runner/#agentlightning.OtelTracer.get_last_trace","title":"<code>get_last_trace()</code>","text":"<p>Retrieves the raw list of captured spans from the most recent trace.</p> <p>Returns:</p> <ul> <li> <code>List[Span]</code>           \u2013            <p>A list of <code>Span</code> objects captured during the most recent trace.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.OtelTracer.trace_context","title":"<code>trace_context(name=None, *, store=None, rollout_id=None, attempt_id=None)</code>  <code>async</code>","text":"<p>Starts a new tracing context. This should be used as a context manager.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional name for the tracing context.</p> </li> <li> <code>store</code>               (<code>Optional[LightningStore]</code>, default:                   <code>None</code> )           \u2013            <p>Optional store to add the spans to.</p> </li> <li> <code>rollout_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional rollout ID to add the spans to.</p> </li> <li> <code>attempt_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional attempt ID to add the spans to.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>AsyncGenerator[Tracer, None]</code>           \u2013            <p>The OpenTelemetry tracer instance to collect spans.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Tracer","title":"<code>agentlightning.Tracer</code>","text":"<p>               Bases: <code>ParallelWorkerBase</code></p> <p>An abstract base class for tracers.</p> <p>This class defines a standard interface for tracing code execution, capturing the resulting spans, and providing them for analysis. It is designed to be backend-agnostic, allowing for different implementations (e.g., for AgentOps, OpenTelemetry, Docker, etc.).</p> <p>The primary interaction pattern is through the <code>trace_context</code> context manager, which ensures that traces are properly started and captured, even in the case of exceptions.</p> <p>A typical workflow:</p> <pre><code>tracer = YourTracerImplementation()\n\ntry:\n    async with tracer.trace_context(name=\"my_traced_task\"):\n        # ... code to be traced ...\n        await run_my_agent_logic()\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n\n# Retrieve the trace data after the context block\nspans: list[ReadableSpan] = tracer.get_last_trace()\n\n# Process the trace data\nif trace_tree:\n    rl_triplets = TracerTraceToTriplet().adapt(spans)\n    # ... do something with the triplets\n</code></pre>"},{"location":"reference/runner/#agentlightning.Tracer.create_span","title":"<code>create_span(name, attributes=None, timestamp=None, status=None)</code>","text":"<p>Notify the tracer that a span should be created here.</p> <p>It uses a fire-and-forget approach and doesn't wait for the span to be created.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the span.</p> </li> <li> <code>attributes</code>               (<code>Optional[Attributes]</code>, default:                   <code>None</code> )           \u2013            <p>The attributes of the span.</p> </li> <li> <code>timestamp</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>The timestamp of the span.</p> </li> <li> <code>status</code>               (<code>Optional[TraceStatus]</code>, default:                   <code>None</code> )           \u2013            <p>The status of the span.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SpanCoreFields</code>           \u2013            <p>The core fields of the span.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Tracer.get_langchain_handler","title":"<code>get_langchain_handler()</code>","text":"<p>Get a handler to install in langchain agent callback.</p> <p>Agents are expected to use this handler in their agents to enable tracing.</p>"},{"location":"reference/runner/#agentlightning.Tracer.get_last_trace","title":"<code>get_last_trace()</code>","text":"<p>Retrieves the raw list of captured spans from the most recent trace.</p> <p>Returns:</p> <ul> <li> <code>List[Span]</code>           \u2013            <p>A list of <code>Span</code> objects collected during the last trace.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Tracer.init_worker","title":"<code>init_worker(worker_id, store=None)</code>","text":"<p>Initialize the tracer for a worker.</p> <p>Parameters:</p> <ul> <li> <code>worker_id</code>               (<code>int</code>)           \u2013            <p>The ID of the worker.</p> </li> <li> <code>store</code>               (<code>Optional[LightningStore]</code>, default:                   <code>None</code> )           \u2013            <p>The store to add the spans to. If it's provided, traces will be added to the store when tracing.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Tracer.lifespan","title":"<code>lifespan(store=None)</code>","text":"<p>A context manager to manage the lifespan of the tracer.</p> <p>This can be used to set up and tear down any necessary resources for the tracer, useful for debugging purposes.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>Optional[LightningStore]</code>, default:                   <code>None</code> )           \u2013            <p>The store to add the spans to. If it's provided, traces will be added to the store when tracing.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Tracer.operation_context","title":"<code>operation_context(name, attributes=None, start_time=None, end_time=None)</code>","text":"<p>Start to record an operation to a span.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the operation.</p> </li> <li> <code>attributes</code>               (<code>Optional[Attributes]</code>, default:                   <code>None</code> )           \u2013            <p>The attributes of the operation.</p> </li> <li> <code>start_time</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>The start time of the operation.</p> </li> <li> <code>end_time</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>The end time of the operation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ContextManager[SpanRecordingContext]</code>           \u2013            <p>A <code>SpanRecordingContext</code> for recording the operation on the span.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Tracer.trace_context","title":"<code>trace_context(name=None, *, store=None, rollout_id=None, attempt_id=None)</code>","text":"<p>Starts a new tracing context. This should be used as a context manager.</p> <p>The implementation should handle the setup and teardown of the tracing for the enclosed code block. It must ensure that any spans generated within the <code>with</code> block are collected and made available via <code>get_last_trace</code>.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The name for the root span of this trace context.</p> </li> <li> <code>store</code>               (<code>Optional[LightningStore]</code>, default:                   <code>None</code> )           \u2013            <p>The store to add the spans to. Deprecated in favor of passing store to init_worker().</p> </li> <li> <code>rollout_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The rollout ID to add the spans to.</p> </li> <li> <code>attempt_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The attempt ID to add the spans to.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Tracer.trace_run","title":"<code>trace_run(func, *args, **kwargs)</code>","text":"<p>A convenience wrapper to trace the execution of a single synchronous function.</p> <p>Deprecated in favor of customizing Runners.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>Callable[..., Any]</code>)           \u2013            <p>The synchronous function to execute and trace.</p> </li> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Positional arguments to pass to the function.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments to pass to the function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The return value of the function.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Tracer.trace_run_async","title":"<code>trace_run_async(func, *args, **kwargs)</code>  <code>async</code>","text":"<p>A convenience wrapper to trace the execution of a single asynchronous function.</p> <p>Deprecated in favor of customizing Runners.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>Callable[..., Awaitable[Any]]</code>)           \u2013            <p>The asynchronous function to execute and trace.</p> </li> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Positional arguments to pass to the function.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments to pass to the function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The return value of the function.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.tracer.weave.WeaveTracer","title":"<code>agentlightning.tracer.weave.WeaveTracer</code>","text":"<p>               Bases: <code>Tracer</code></p> <p>Tracer implementation using Weave for telemetry and trace logging.</p> <p>This replaces AgentOpsTracer with a Weave-based manual trace context. It tracks:</p> <ul> <li>Function/method calls</li> <li>Input/Output data</li> <li>Exceptions</li> </ul> <p>and logs them to Weave Cloud (W&amp;B backend) or optionally bypasses the network for testing.</p>"},{"location":"reference/runner/#agentlightning.tracer.weave.WeaveTracer.__init__","title":"<code>__init__(*, project_name=None, weave_user_settings=None, instrument_managed=True)</code>","text":"<p>Initialize a WeaveTracer instance.</p> <p>Parameters:</p> <ul> <li> <code>project_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional project name for Weave; defaults to the current module name.</p> </li> <li> <code>weave_user_settings</code>               (<code>UserSettings | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional UserSettings for Weave.</p> </li> <li> <code>instrument_managed</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to patch the Weave/W&amp;B integration to bypass actual network calls for testing.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.tracer.weave.WeaveTracer.complete_call_handler","title":"<code>complete_call_handler(call)</code>  <code>async</code>","text":"<p>Handler called when a Weave Call finishes.</p> <p>Converts the call (including nested children) into spans and stores them in LightningStore.</p>"},{"location":"reference/runner/#agentlightning.tracer.weave.WeaveTracer.convert_call_to_span","title":"<code>convert_call_to_span(call, rollout_id=None, attempt_id=None, sequence_id=None)</code>  <code>async</code>","text":"<p>Convert a Weave Call (with nested children) into a Agent-lightning Span.</p> <p><code>rollout_id</code> and <code>attempt_id</code> are required to attach the spans to the store.</p> <p>Parameters:</p> <ul> <li> <code>call</code>               (<code>CallSchema</code>)           \u2013            <p>The Weave Call object.</p> </li> <li> <code>rollout_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional rollout ID to attach to spans.</p> </li> <li> <code>attempt_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional attempt ID to attach to spans.</p> </li> <li> <code>sequence_id</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Optional sequence ID to attach to spans.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Span</code>           \u2013            <p>List of converted spans.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.tracer.weave.WeaveTracer.init_worker","title":"<code>init_worker(worker_id, store=None)</code>","text":"<p>Initialize the tracer for a worker thread/process.</p> <p>Parameters:</p> <ul> <li> <code>worker_id</code>               (<code>int</code>)           \u2013            <p>Identifier of the worker.</p> </li> <li> <code>store</code>               (<code>Optional[LightningStore]</code>, default:                   <code>None</code> )           \u2013            <p>Optional LightningStore for storing spans.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.tracer.weave.WeaveTracer.partial_call_handler","title":"<code>partial_call_handler(request_content)</code>  <code>async</code>","text":"<p>Handler called when a Weave Call starts.</p> <p>Parameters:</p> <ul> <li> <code>request_content</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>The partial Weave Call object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The sequence ID for the call.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.tracer.weave.WeaveTracer.teardown_worker","title":"<code>teardown_worker(worker_id)</code>","text":"<p>Clean up tracer resources for the worker.</p> <p>Parameters:</p> <ul> <li> <code>worker_id</code>               (<code>int</code>)           \u2013            <p>Identifier of the worker.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.tracer.weave.WeaveTracer.trace_context","title":"<code>trace_context(name=None, *, rollout_id=None, attempt_id=None, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronous implementation of the tracing context.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional operation name.</p> </li> <li> <code>rollout_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional rollout ID.</p> </li> <li> <code>attempt_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional attempt ID.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If store, rollout_id, and attempt_id are inconsistently provided.</p> </li> <li> <code>RuntimeError</code>             \u2013            <p>If Weave is not installed or client is uninitialized.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.DummyTracer","title":"<code>agentlightning.DummyTracer</code>","text":"<p>               Bases: <code>Tracer</code></p> <p>A dummy tracer that does not trace anything, but it is compatible with the emitter API.</p> <p>It doesn't rely on any backend tracer, and also doesn't use any stores.</p>"},{"location":"reference/runner/#agentlightning.set_active_tracer","title":"<code>agentlightning.set_active_tracer(tracer)</code>","text":"<p>Set the active tracer for the current process.</p> <p>Parameters:</p> <ul> <li> <code>tracer</code>               (<code>Tracer</code>)           \u2013            <p>The tracer to set as active.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.get_active_tracer","title":"<code>agentlightning.get_active_tracer()</code>","text":"<p>Get the active tracer for the current process.</p> <p>Returns:</p> <ul> <li> <code>Optional[Tracer]</code>           \u2013            <p>The active tracer, or None if no tracer is active.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.clear_active_tracer","title":"<code>agentlightning.clear_active_tracer()</code>","text":"<p>Clear the active tracer for the current process.</p>"},{"location":"reference/runner/#agentlightning.tracer.weave.WeaveTracer","title":"<code>agentlightning.tracer.weave.WeaveTracer</code>","text":"<p>               Bases: <code>Tracer</code></p> <p>Tracer implementation using Weave for telemetry and trace logging.</p> <p>This replaces AgentOpsTracer with a Weave-based manual trace context. It tracks:</p> <ul> <li>Function/method calls</li> <li>Input/Output data</li> <li>Exceptions</li> </ul> <p>and logs them to Weave Cloud (W&amp;B backend) or optionally bypasses the network for testing.</p>"},{"location":"reference/runner/#agentlightning.tracer.weave.WeaveTracer.__init__","title":"<code>__init__(*, project_name=None, weave_user_settings=None, instrument_managed=True)</code>","text":"<p>Initialize a WeaveTracer instance.</p> <p>Parameters:</p> <ul> <li> <code>project_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional project name for Weave; defaults to the current module name.</p> </li> <li> <code>weave_user_settings</code>               (<code>UserSettings | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional UserSettings for Weave.</p> </li> <li> <code>instrument_managed</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to patch the Weave/W&amp;B integration to bypass actual network calls for testing.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.tracer.weave.WeaveTracer.complete_call_handler","title":"<code>complete_call_handler(call)</code>  <code>async</code>","text":"<p>Handler called when a Weave Call finishes.</p> <p>Converts the call (including nested children) into spans and stores them in LightningStore.</p>"},{"location":"reference/runner/#agentlightning.tracer.weave.WeaveTracer.convert_call_to_span","title":"<code>convert_call_to_span(call, rollout_id=None, attempt_id=None, sequence_id=None)</code>  <code>async</code>","text":"<p>Convert a Weave Call (with nested children) into a Agent-lightning Span.</p> <p><code>rollout_id</code> and <code>attempt_id</code> are required to attach the spans to the store.</p> <p>Parameters:</p> <ul> <li> <code>call</code>               (<code>CallSchema</code>)           \u2013            <p>The Weave Call object.</p> </li> <li> <code>rollout_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional rollout ID to attach to spans.</p> </li> <li> <code>attempt_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional attempt ID to attach to spans.</p> </li> <li> <code>sequence_id</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Optional sequence ID to attach to spans.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Span</code>           \u2013            <p>List of converted spans.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.tracer.weave.WeaveTracer.init_worker","title":"<code>init_worker(worker_id, store=None)</code>","text":"<p>Initialize the tracer for a worker thread/process.</p> <p>Parameters:</p> <ul> <li> <code>worker_id</code>               (<code>int</code>)           \u2013            <p>Identifier of the worker.</p> </li> <li> <code>store</code>               (<code>Optional[LightningStore]</code>, default:                   <code>None</code> )           \u2013            <p>Optional LightningStore for storing spans.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.tracer.weave.WeaveTracer.partial_call_handler","title":"<code>partial_call_handler(request_content)</code>  <code>async</code>","text":"<p>Handler called when a Weave Call starts.</p> <p>Parameters:</p> <ul> <li> <code>request_content</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>The partial Weave Call object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The sequence ID for the call.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.tracer.weave.WeaveTracer.teardown_worker","title":"<code>teardown_worker(worker_id)</code>","text":"<p>Clean up tracer resources for the worker.</p> <p>Parameters:</p> <ul> <li> <code>worker_id</code>               (<code>int</code>)           \u2013            <p>Identifier of the worker.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.tracer.weave.WeaveTracer.trace_context","title":"<code>trace_context(name=None, *, rollout_id=None, attempt_id=None, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronous implementation of the tracing context.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional operation name.</p> </li> <li> <code>rollout_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional rollout ID.</p> </li> <li> <code>attempt_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional attempt ID.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If store, rollout_id, and attempt_id are inconsistently provided.</p> </li> <li> <code>RuntimeError</code>             \u2013            <p>If Weave is not installed or client is uninitialized.</p> </li> </ul>"},{"location":"reference/semconv/","title":"Semantic Conventions","text":""},{"location":"reference/semconv/#agentlightning.semconv","title":"<code>agentlightning.semconv</code>","text":"<p>Semantic conventions for Agent-lightning spans.</p> <p>Conventions in this file are added on demand. We generally DO NOT add new semantic conventions unless it's absolutely needed for certain algorithms or scenarios.</p>"},{"location":"reference/semconv/#agentlightning.semconv.AGL_ANNOTATION","title":"<code>AGL_ANNOTATION = 'agentlightning.annotation'</code>  <code>module-attribute</code>","text":"<p>Agent-lightning's standard span name for annotations.</p> <p>Annotations are minimal span units for rewards, tags, and metadatas. They are used to \"annotate\" a specific event or a part of rollout.</p>"},{"location":"reference/semconv/#agentlightning.semconv.AGL_EXCEPTION","title":"<code>AGL_EXCEPTION = 'agentlightning.exception'</code>  <code>module-attribute</code>","text":"<p>Agent-lightning's standard span name for exceptions.</p> <p>Used by the exception emitter to record exception details.</p>"},{"location":"reference/semconv/#agentlightning.semconv.AGL_MESSAGE","title":"<code>AGL_MESSAGE = 'agentlightning.message'</code>  <code>module-attribute</code>","text":"<p>Agent-lightning's standard span name for messages and logs.</p>"},{"location":"reference/semconv/#agentlightning.semconv.AGL_OBJECT","title":"<code>AGL_OBJECT = 'agentlightning.object'</code>  <code>module-attribute</code>","text":"<p>Agent-lightning's standard span name for customized objects.</p>"},{"location":"reference/semconv/#agentlightning.semconv.AGL_OPERATION","title":"<code>AGL_OPERATION = 'agentlightning.operation'</code>  <code>module-attribute</code>","text":"<p>Agent-lightning's standard span name for functions. Wrap function or code-blocks as operations.</p>"},{"location":"reference/semconv/#agentlightning.semconv.AGL_REWARD","title":"<code>AGL_REWARD = 'agentlightning.reward'</code>  <code>module-attribute</code>","text":"<p>Agent-lightning's standard span name for reward operations.</p>"},{"location":"reference/semconv/#agentlightning.semconv.AGL_VIRTUAL","title":"<code>AGL_VIRTUAL = 'agentlightning.virtual'</code>  <code>module-attribute</code>","text":"<p>Agent-lightning's standard span name for virtual operations.</p> <p>Mostly used in adapter when needing to represent the root or intermediate operations.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LightningResourceAttributes","title":"<code>LightningResourceAttributes</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Resource attribute names used in Agent-lightning spans.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LightningResourceAttributes.ATTEMPT_ID","title":"<code>ATTEMPT_ID = 'agentlightning.attempt_id'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Resource name for attempt ID in Agent-lightning spans.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LightningResourceAttributes.ROLLOUT_ID","title":"<code>ROLLOUT_ID = 'agentlightning.rollout_id'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Resource name for rollout ID in Agent-lightning spans.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LightningResourceAttributes.SPAN_SEQUENCE_ID","title":"<code>SPAN_SEQUENCE_ID = 'agentlightning.span_sequence_id'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Resource name for span sequence ID in Agent-lightning spans.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LightningResourceAttributes.TRACER_NAME","title":"<code>TRACER_NAME = 'agentlightning.tracer.name'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Which tracer is used to create this span.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LightningSpanAttributes","title":"<code>LightningSpanAttributes</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Attribute names that commonly appear in Agent-lightning spans.</p> <p>Exception types can't be found here because they are defined in OpenTelemetry's official semantic conventions.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LightningSpanAttributes.LINK","title":"<code>LINK = 'agentlightning.link'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Attribute name for linking the current span to another span or other objects like requests/responses.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LightningSpanAttributes.MESSAGE_BODY","title":"<code>MESSAGE_BODY = 'agentlightning.message.body'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Attribute name for message text in message spans.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LightningSpanAttributes.OBJECT_JSON","title":"<code>OBJECT_JSON = 'agentlightning.object.json'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Attribute name for object serialized value (JSON) in object spans.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LightningSpanAttributes.OBJECT_LITERAL","title":"<code>OBJECT_LITERAL = 'agentlightning.object.literal'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Attribute name for object literal value in object spans (for str, int, bool, ...).</p>"},{"location":"reference/semconv/#agentlightning.semconv.LightningSpanAttributes.OBJECT_TYPE","title":"<code>OBJECT_TYPE = 'agentlightning.object.type'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Attribute name for object type (full qualified name) in object spans.</p> <p>I think builtin types like str, int, bool, list, dict are self-explanatory and should also be qualified to use here.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LightningSpanAttributes.OPERATION_INPUT","title":"<code>OPERATION_INPUT = 'agentlightning.operation.input'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Attribute name for operation input in operation spans.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LightningSpanAttributes.OPERATION_NAME","title":"<code>OPERATION_NAME = 'agentlightning.operation.name'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Attribute name for operation name in operation spans, normally the function name.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LightningSpanAttributes.OPERATION_OUTPUT","title":"<code>OPERATION_OUTPUT = 'agentlightning.operation.output'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Attribute name for operation output in operation spans.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LightningSpanAttributes.REWARD","title":"<code>REWARD = 'agentlightning.reward'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Attribute prefix for rewards-related data in reward spans.</p> <p>It should be used as a prefix. For example, \"agentlightning.reward.0.value\" can be used to track a specific metric. See RewardAttributes.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LightningSpanAttributes.TAG","title":"<code>TAG = 'agentlightning.tag'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Attribute name for tagging spans with customized strings.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LinkAttributes","title":"<code>LinkAttributes</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Standard link types used in Agent-lightning spans.</p> <p>The link is more powerful than OpenTelemetry link in that it supports linking to a queryset of spans. It can even link to span object that hasn't been emitted yet.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LinkAttributes.KEY_MATCH","title":"<code>KEY_MATCH = 'key_match'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Linking to spans with matching attribute keys.</p> <p><code>trace_id</code> and <code>span_id</code> are reserved and will be used to link to specific spans directly.</p> <p>For example, it can be <code>gen_ai.response.id</code> if intended to be link to a chat completion response span. Or it can be <code>span_id</code> to link to a specific span by its ID.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LinkAttributes.VALUE_MATCH","title":"<code>VALUE_MATCH = 'value_match'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Linking to spans with corresponding attribute values on those keys.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LinkPydanticModel","title":"<code>LinkPydanticModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A stricter implementation of LinkAttributes used in otel helpers.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LinkPydanticModel.key_match","title":"<code>key_match</code>  <code>instance-attribute</code>","text":"<p>The attribute key to match on the target spans.</p>"},{"location":"reference/semconv/#agentlightning.semconv.LinkPydanticModel.value_match","title":"<code>value_match</code>  <code>instance-attribute</code>","text":"<p>The attribute value to match on the target spans.</p>"},{"location":"reference/semconv/#agentlightning.semconv.RewardAttributes","title":"<code>RewardAttributes</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Multi-dimensional reward attributes will look like:</p> <pre><code>{\"agentlightning.reward.0.name\": \"efficiency\", \"agentlightning.reward.0.value\": 0.75}\n</code></pre> <p>The first reward in the reward list will automatically be the primary reward. If the reward list has greater than 1, it shall be a multi-dimensional case.</p>"},{"location":"reference/semconv/#agentlightning.semconv.RewardAttributes.REWARD_NAME","title":"<code>REWARD_NAME = 'name'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Key for each dimension in multi-dimensional reward spans.</p>"},{"location":"reference/semconv/#agentlightning.semconv.RewardAttributes.REWARD_VALUE","title":"<code>REWARD_VALUE = 'value'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Value for each dimension in multi-dimensional reward spans.</p>"},{"location":"reference/semconv/#agentlightning.semconv.RewardPydanticModel","title":"<code>RewardPydanticModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A stricter implementation of RewardAttributes used in otel helpers.</p>"},{"location":"reference/semconv/#agentlightning.semconv.RewardPydanticModel.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>Name of the reward dimension.</p>"},{"location":"reference/semconv/#agentlightning.semconv.RewardPydanticModel.value","title":"<code>value</code>  <code>instance-attribute</code>","text":"<p>Value of the reward dimension.</p>"},{"location":"reference/store/","title":"Store References","text":""},{"location":"reference/store/#agentlightning.LightningStore","title":"<code>agentlightning.LightningStore</code>","text":"<p>Contract for the persistent control-plane that coordinates training rollouts.</p> <p>A <code>LightningStore</code> mediates every interaction between algorithms and runners:</p> <ul> <li>Rollout lifecycle: accept new rollouts, queue them for execution, create attempts,   and drive the rollout status machine (<code>\"queuing\"</code> \u2192 <code>\"preparing\"</code> \u2192 <code>\"running\"</code> \u2192   <code>{\"succeeded\",\"failed\",\"cancelled\"}</code> or <code>\"requeuing\"</code> when a retry is justified).</li> <li>Attempt tracking: record each execution attempt, including progress heartbeats,   retry sequencing, and terminal states such as <code>\"timeout\"</code> or <code>\"unresponsive\"</code>.</li> <li>Span ingest: capture structured telemetry emitted by runners (either as native   <code>Span</code> objects or as <code>opentelemetry.sdk.trace.ReadableSpan</code>   instances) so that algorithms can reconstruct trajectories and rewards.</li> <li>Resource versioning: manage immutable snapshots of named resources   (prompt templates, model checkpoints, proxy endpoints, \u2026) and expose a single   \"latest\" snapshot that runners can fetch just after claiming work.</li> </ul> <p>Implementations must provide thread-safe/async-safe semantics: each coroutine should appear atomic to callers even when multiple algorithms or runners call the API concurrently. Unless stated otherwise, missing identifiers should result in a <code>ValueError</code>.</p>"},{"location":"reference/store/#agentlightning.LightningStore.capabilities","title":"<code>capabilities</code>  <code>property</code>","text":"<p>Return the capabilities of the store.</p>"},{"location":"reference/store/#agentlightning.LightningStore.add_many_spans","title":"<code>add_many_spans(spans)</code>  <code>async</code>","text":"<p>Persist a sequence of pre-constructed spans emitted during rollout execution.</p> <p>Implementations can simply delegate to <code>add_span()</code> for each span. However, if the store supports bulk insertion, it can implement this method to improve performance.</p>"},{"location":"reference/store/#agentlightning.LightningStore.add_otel_span","title":"<code>add_otel_span(rollout_id, attempt_id, readable_span, sequence_id=None)</code>  <code>async</code>","text":"<p>Convert and persist an OpenTelemetry span for a particular attempt.</p> <p>Implementations must transform the <code>readable_span</code> into a <code>Span</code> (typically via <code>Span.from_opentelemetry()</code>), assign a strictly increasing <code>sequence_id</code> when one is not provided, and persist it using the same semantics as <code>add_span()</code>.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the rollout that produced the span.</p> </li> <li> <code>attempt_id</code>               (<code>str</code>)           \u2013            <p>Attempt identifier the span belongs to.</p> </li> <li> <code>readable_span</code>               (<code>ReadableSpan</code>)           \u2013            <p>OpenTelemetry span in SDK form.</p> </li> <li> <code>sequence_id</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional explicit ordering hint. When omitted, call <code>get_next_span_sequence_id()</code> automatically.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Span]</code>           \u2013            <p>The stored span record. Return <code>None</code> if the span was not added due to a duplicate.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement span persistence.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when the rollout or attempt is unknown.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.add_resources","title":"<code>add_resources(resources)</code>  <code>async</code>","text":"<p>Persist a new immutable snapshot of named resources and mark it as latest.</p> <p>Implementations must assign a fresh <code>resources_id</code> and ensure subsequent calls to <code>get_latest_resources()</code> return the snapshot produced here.</p> <p>Parameters:</p> <ul> <li> <code>resources</code>               (<code>NamedResources</code>)           \u2013            <p>Mapping of resource names to their serialized payloads.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ResourcesUpdate</code>           \u2013            <p>The stored <code>ResourcesUpdate</code> including its generated id.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement resource persistence.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.add_span","title":"<code>add_span(span)</code>  <code>async</code>","text":"<p>Persist a pre-constructed span emitted during rollout execution.</p> <p>The provided <code>Span</code> must already contain the <code>rollout_id</code>, <code>attempt_id</code>, and <code>sequence_id</code>. Implementations must:</p> <ul> <li>Verify that both rollout and attempt exist.</li> <li>Ensure span ordering remains strictly increasing per attempt (rejecting or keeping duplicates).</li> <li>Treat the span arrival as a heartbeat: update the attempt's <code>last_heartbeat_time</code>   and transition both attempt and rollout to <code>\"running\"</code> if they were still   <code>\"preparing\"</code> or <code>\"requeuing\"</code>.</li> </ul> <p>Parameters:</p> <ul> <li> <code>span</code>               (<code>Span</code>)           \u2013            <p>Fully populated span to persist.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Span]</code>           \u2013            <p>The stored span record (implementations may return a copy).</p> </li> <li> <code>Optional[Span]</code>           \u2013            <p>Return <code>None</code> if the span was not added due to a duplicate.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement span persistence.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when the referenced rollout or attempt is missing.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.dequeue_many_rollouts","title":"<code>dequeue_many_rollouts(*, limit=1, worker_id=None)</code>  <code>async</code>","text":"<p>Claim up to <code>limit</code> queued rollouts without blocking.</p> <p>The implementation can repeatedly invokes <code>dequeue_rollout()</code> until reaching the requested limit or the queue is empty. Subclasses can override it to fetch multiple rollouts atomically.</p> <p>Parameters:</p> <ul> <li> <code>limit</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Maximum number of rollouts to claim. Non-positive values return an empty list.</p> </li> <li> <code>worker_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional worker identifier passed through to each dequeue call.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[AttemptedRollout]</code>           \u2013            <p>Attempted rollouts claimed in FIFO order. May contain fewer than <code>limit</code> entries</p> </li> <li> <code>Sequence[AttemptedRollout]</code>           \u2013            <p>when the queue is exhausted.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.dequeue_rollout","title":"<code>dequeue_rollout(worker_id=None)</code>  <code>async</code>","text":"<p>Claim the oldest queued rollout and transition it to <code>preparing</code>.</p> <p>This function do not block.</p> <p>Retrieval must be FIFO across rollouts that remain in <code>queuing</code> or <code>requeuing</code> state. When a rollout is claimed, implementations must:</p> <ul> <li>Transition its status to <code>\"preparing\"</code>.</li> <li>Create a new attempt with <code>status=\"preparing\"</code> and <code>sequence_id</code> equal to   the number of attempts already registered for the rollout plus one.</li> <li>Return an <code>AttemptedRollout</code> snapshot so the   runner knows both rollout metadata and the attempt identifier.</li> <li>Optionally refresh the caller's <code>Worker</code> telemetry   (e.g., <code>last_dequeue_time</code>) when <code>worker_id</code> is provided.</li> </ul> <p>Parameters:</p> <ul> <li> <code>worker_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional worker identifier to associate the claimed attempt with.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[AttemptedRollout]</code>           \u2013            <p>The next attempt to execute, or <code>None</code> when no eligible rollouts are queued.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement queue retrieval.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.enqueue_many_rollouts","title":"<code>enqueue_many_rollouts(rollouts)</code>  <code>async</code>","text":"<p>Persist multiple rollouts in <code>queuing</code> state.</p> <p>The implementation can delegate to <code>enqueue_rollout()</code> per request and preserves the input ordering. Subclasses can override to provide more efficient bulk enqueue semantics.</p> <p>Parameters:</p> <ul> <li> <code>rollouts</code>               (<code>Sequence[EnqueueRolloutRequest]</code>)           \u2013            <p>Rollout submission payloads mirroring <code>enqueue_rollout()</code>'s parameters. Each entry requires <code>input</code> and can optionally include other fields.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[Rollout]</code>           \u2013            <p>Rollouts enqueued in the same order as <code>rollouts</code>.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.enqueue_rollout","title":"<code>enqueue_rollout(input, mode=None, resources_id=None, config=None, metadata=None)</code>  <code>async</code>","text":"<p>Persist a rollout in <code>queuing</code> state so runners can claim it later.</p> <p>Note</p> <p>Different from <code>start_rollout()</code>, this method is called when the caller only wants to submit work for later scheduling.</p> <p>Implementations must generate a unique <code>rollout_id</code>, stamp <code>start_time</code> with the current time, default <code>config</code> to a fresh <code>RolloutConfig</code>, and insert the rollout at the tail of the scheduling queue. No attempt is created yet.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>TaskInput</code>)           \u2013            <p>Arbitrary task payload supplied by an algorithm.</p> </li> <li> <code>mode</code>               (<code>Literal['train', 'val', 'test'] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional semantic mode indicator (<code>\"train\"</code>, <code>\"val\"</code>, <code>\"test\"</code>).</p> </li> <li> <code>resources_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Resource snapshot used when a runner eventually executes the rollout.</p> </li> <li> <code>config</code>               (<code>RolloutConfig | None</code>, default:                   <code>None</code> )           \u2013            <p>Fine-grained retry/timeout parameters to persist with the rollout.</p> </li> <li> <code>metadata</code>               (<code>Dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Free-form metadata stored verbatim with the rollout record.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Rollout</code>           \u2013            <p>The stored <code>Rollout</code> in <code>queuing</code> status.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must persist the rollout.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations should raise when <code>resources_id</code> does not exist.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.get_latest_attempt","title":"<code>get_latest_attempt(rollout_id)</code>  <code>async</code>","text":"<p>Fetch the attempt with the highest <code>sequence_id</code> for <code>rollout_id</code>.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier to inspect.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Attempt]</code>           \u2013            <p>The most recent attempt or <code>None</code> when no attempts exist yet.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement retrieval.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when the rollout does not exist.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.get_latest_resources","title":"<code>get_latest_resources()</code>  <code>async</code>","text":"<p>Fetch the latest resource snapshot marked as the global default.</p> <p>Returns:</p> <ul> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>The current latest <code>ResourcesUpdate</code>, or <code>None</code> when</p> </li> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>no resources have been registered yet.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement retrieval.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.get_many_span_sequence_ids","title":"<code>get_many_span_sequence_ids(rollout_attempt_ids)</code>  <code>async</code>","text":"<p>Bulk allocate the next strictly increasing sequence number used to order spans.</p> <p>Implementations may delegate to <code>get_next_span_sequence_id()</code> for each rollout and attempt.</p> <p>Parameters:</p> <ul> <li> <code>rollout_attempt_ids</code>               (<code>Sequence[Tuple[str, str]]</code>)           \u2013            <p>List of tuples of rollout and attempt identifiers.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[int]</code>           \u2013            <p>List of sequence numbers.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.get_next_span_sequence_id","title":"<code>get_next_span_sequence_id(rollout_id, attempt_id)</code>  <code>async</code>","text":"<p>Allocate the next strictly increasing sequence number used to order spans.</p> <p>Implementations must retain counters so repeated calls return <code>1, 2, ...</code> without gaps unless spans were explicitly inserted with a custom <code>sequence_id</code>. The counter may be scoped per rollout or per attempt, but the sequence must be strictly increasing for spans emitted by the specified attempt so traces remain totally ordered.</p> <p>See Distributed Tracing for detailed motivations.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the rollout emitting spans.</p> </li> <li> <code>attempt_id</code>               (<code>str</code>)           \u2013            <p>Attempt identifier for the upcoming span.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The next integer sequence identifier, unique within the attempt.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must provide the allocator.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when the rollout or attempt does not exist.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.get_resources_by_id","title":"<code>get_resources_by_id(resources_id)</code>  <code>async</code>","text":"<p>Return a specific named resource snapshot by identifier.</p> <p>Parameters:</p> <ul> <li> <code>resources_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the snapshot.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>The stored <code>ResourcesUpdate</code>, or <code>None</code> when missing.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement retrieval.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.get_rollout_by_id","title":"<code>get_rollout_by_id(rollout_id)</code>  <code>async</code>","text":"<p>Fetch a rollout by identifier without mutating its state.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Rollout]</code>           \u2013            <p>The rollout when found, otherwise <code>None</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement retrieval.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.get_worker_by_id","title":"<code>get_worker_by_id(worker_id)</code>  <code>async</code>","text":"<p>Retrieve a single worker by identifier.</p> <p>Parameters:</p> <ul> <li> <code>worker_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the worker.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Worker]</code>           \u2013            <p>The worker record if it exists, otherwise <code>None</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement lookup semantics.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.otlp_traces_endpoint","title":"<code>otlp_traces_endpoint()</code>","text":"<p>Return the OTLP/HTTP traces endpoint of the store.</p> <p>The traces can have rollout ID and attempt ID (and optionally sequence ID) saved in the \"resource\" of the spans. The store, if it supports OTLP, should be able to receive the traces and save them via <code>add_span</code> or <code>add_otel_span</code>.</p> <p>The endpoint should be compatible with OTLP HTTP protocol. It's not necessarily compatible with OTLP gRPC protocol.</p> <p>The returned endpoint will usually ends with <code>/v1/traces</code>.</p>"},{"location":"reference/store/#agentlightning.LightningStore.query_attempts","title":"<code>query_attempts(rollout_id, *, sort_by='sequence_id', sort_order='asc', limit=-1, offset=0)</code>  <code>async</code>","text":"<p>Return every attempt ever created for <code>rollout_id</code> in ascending sequence order.</p> <p>The parameters allow callers to re-order or paginate the attempts so that large retry histories can be streamed lazily.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the rollout being inspected.</p> </li> <li> <code>sort_by</code>               (<code>Optional[str]</code>, default:                   <code>'sequence_id'</code> )           \u2013            <p>Field to sort by. Must be a numeric or string field of <code>Attempt</code>. Defaults to <code>sequence_id</code> (oldest first).</p> </li> <li> <code>sort_order</code>               (<code>Literal['asc', 'desc']</code>, default:                   <code>'asc'</code> )           \u2013            <p>Order to sort by.</p> </li> <li> <code>limit</code>               (<code>int</code>, default:                   <code>-1</code> )           \u2013            <p>Limit on the number of results. <code>-1</code> for unlimited.</p> </li> <li> <code>offset</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Offset into the results.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[Attempt]</code>           \u2013            <p>Sequence of Attempts. Returns an empty sequence when none exist.</p> </li> <li> <code>Sequence[Attempt]</code>           \u2013            <p>The return value is not guaranteed to be a list.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement the query.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when the rollout does not exist.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.query_resources","title":"<code>query_resources(*, resources_id=None, resources_id_contains=None, sort_by=None, sort_order='asc', limit=-1, offset=0)</code>  <code>async</code>","text":"<p>List every stored resource snapshot in insertion order.</p> <p>Supports lightweight filtering, sorting, and pagination for embedding in dashboards.</p> <p>Parameters:</p> <ul> <li> <code>resources_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional identifier of the resources to include.</p> </li> <li> <code>resources_id_contains</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional substring match for resources identifiers.</p> </li> <li> <code>sort_by</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional field to sort by (must be numeric or string on <code>ResourcesUpdate</code>).</p> </li> <li> <code>sort_order</code>               (<code>Literal['asc', 'desc']</code>, default:                   <code>'asc'</code> )           \u2013            <p>Order to sort by.</p> </li> <li> <code>limit</code>               (<code>int</code>, default:                   <code>-1</code> )           \u2013            <p>Limit on the number of results. <code>-1</code> for unlimited.</p> </li> <li> <code>offset</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Offset into the results.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[ResourcesUpdate]</code>           \u2013            <p><code>ResourcesUpdate</code> objects.</p> </li> <li> <code>Sequence[ResourcesUpdate]</code>           \u2013            <p>By default, resources are sorted in a deterministic but undefined order.</p> </li> <li> <code>Sequence[ResourcesUpdate]</code>           \u2013            <p>The return value is not guaranteed to be a list.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement retrieval.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.query_rollouts","title":"<code>query_rollouts(*, status_in=None, rollout_id_in=None, rollout_id_contains=None, filter_logic='and', sort_by=None, sort_order='asc', limit=-1, offset=0, status=None, rollout_ids=None)</code>  <code>async</code>","text":"<p>Retrieve rollouts filtered by status and/or explicit identifiers.</p> <p>This interface supports structured filtering, sorting, and pagination so callers can build simple dashboards without copying data out of the store. The legacy parameters <code>status</code> and <code>rollout_ids</code> remain valid and are treated as aliases for <code>status_in</code> and <code>rollout_id_in</code> respectively\u2014when both the new and deprecated parameters are supplied the new parameters take precedence.</p> <p>Parameters:</p> <ul> <li> <code>status_in</code>               (<code>Optional[Sequence[RolloutStatus]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional whitelist of <code>RolloutStatus</code> values.</p> </li> <li> <code>rollout_id_in</code>               (<code>Optional[Sequence[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional whitelist of rollout identifiers to include.</p> </li> <li> <code>rollout_id_contains</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional substring match for rollout identifiers.</p> </li> <li> <code>filter_logic</code>               (<code>Literal['and', 'or']</code>, default:                   <code>'and'</code> )           \u2013            <p>Logical operator to combine filters.</p> </li> <li> <code>sort_by</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional field to sort by. Must reference a numeric or string field on <code>Rollout</code>.</p> </li> <li> <code>sort_order</code>               (<code>Literal['asc', 'desc']</code>, default:                   <code>'asc'</code> )           \u2013            <p>Direction to sort when <code>sort_by</code> is provided.</p> </li> <li> <code>limit</code>               (<code>int</code>, default:                   <code>-1</code> )           \u2013            <p>Maximum number of rows to return. Use <code>-1</code> for \"no limit\".</p> </li> <li> <code>offset</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of rows to skip before returning results.</p> </li> <li> <code>status</code>               (<code>Optional[Sequence[RolloutStatus]]</code>, default:                   <code>None</code> )           \u2013            <p>Deprecated field. Use <code>status_in</code> instead.</p> </li> <li> <code>rollout_ids</code>               (<code>Optional[Sequence[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Deprecated field. Use <code>rollout_id_in</code> instead.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[Rollout]</code>           \u2013            <p>A sequence of matching rollouts (or <code>AttemptedRollout</code></p> </li> <li> <code>Sequence[Rollout]</code>           \u2013            <p>when attempts exist). Ordering is deterministic when <code>sort_by</code> is set.</p> </li> <li> <code>Sequence[Rollout]</code>           \u2013            <p>The return value is not guaranteed to be a list.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement the query.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.query_spans","title":"<code>query_spans(rollout_id, attempt_id=None, *, trace_id=None, trace_id_contains=None, span_id=None, span_id_contains=None, parent_id=None, parent_id_contains=None, name=None, name_contains=None, filter_logic='and', limit=-1, offset=0, sort_by='sequence_id', sort_order='asc')</code>  <code>async</code>","text":"<p>Return the stored spans for a rollout, optionally scoped to one attempt.</p> <p>Supports a handful of filters that cover the most common debugging scenarios (matching <code>trace_id</code>/<code>span_id</code>/<code>parent_id</code> or substring matches on the span name). <code>attempt_id=\"latest\"</code> acts as a convenience that resolves the most recent attempt before evaluating filters. When <code>attempt_id=None</code>, spans across every attempt are eligible. By default results are sorted by <code>sequence_id</code> (oldest first). Implementations may raise a <code>RuntimeError</code> when spans were evicted or expired.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the rollout being inspected.</p> </li> <li> <code>attempt_id</code>               (<code>str | Literal['latest'] | None</code>, default:                   <code>None</code> )           \u2013            <p>Attempt identifier to filter by. Pass <code>\"latest\"</code> to retrieve only the most recent attempt, or <code>None</code> to return all spans across attempts.</p> </li> <li> <code>trace_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional trace ID to filter by.</p> </li> <li> <code>trace_id_contains</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional substring match for trace IDs.</p> </li> <li> <code>span_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional span ID to filter by.</p> </li> <li> <code>span_id_contains</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional substring match for span IDs.</p> </li> <li> <code>parent_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional parent span ID to filter by.</p> </li> <li> <code>parent_id_contains</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional substring match for parent span IDs.</p> </li> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional span name to filter by.</p> </li> <li> <code>name_contains</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional substring match for span names.</p> </li> <li> <code>filter_logic</code>               (<code>Literal['and', 'or']</code>, default:                   <code>'and'</code> )           \u2013            <p>Logical operator to combine the optional filters above. The <code>rollout_id</code> argument is always applied with AND semantics.</p> </li> <li> <code>limit</code>               (<code>int</code>, default:                   <code>-1</code> )           \u2013            <p>Limit on the number of results. <code>-1</code> for unlimited.</p> </li> <li> <code>offset</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Offset into the results.</p> </li> <li> <code>sort_by</code>               (<code>Optional[str]</code>, default:                   <code>'sequence_id'</code> )           \u2013            <p>Field to sort by. Must be a numeric or string field of <code>Span</code>.</p> </li> <li> <code>sort_order</code>               (<code>Literal['asc', 'desc']</code>, default:                   <code>'asc'</code> )           \u2013            <p>Order to sort by.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[Span]</code>           \u2013            <p>An ordered list of spans (possibly empty).</p> </li> <li> <code>Sequence[Span]</code>           \u2013            <p>The return value is not guaranteed to be a list.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement the query.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when the rollout or attempt is unknown.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.query_workers","title":"<code>query_workers(*, status_in=None, worker_id_contains=None, filter_logic='and', sort_by=None, sort_order='asc', limit=-1, offset=0)</code>  <code>async</code>","text":"<p>Query all workers in the system.</p> <p>Parameters:</p> <ul> <li> <code>status_in</code>               (<code>Optional[Sequence[WorkerStatus]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional whitelist of <code>WorkerStatus</code> values.</p> </li> <li> <code>worker_id_contains</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional substring match for worker identifiers.</p> </li> <li> <code>filter_logic</code>               (<code>Literal['and', 'or']</code>, default:                   <code>'and'</code> )           \u2013            <p>Logical operator to combine the optional filters above.</p> </li> <li> <code>sort_by</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Field to sort by. Must be a numeric or string field of <code>Worker</code>.</p> </li> <li> <code>sort_order</code>               (<code>Literal['asc', 'desc']</code>, default:                   <code>'asc'</code> )           \u2013            <p>Order to sort by.</p> </li> <li> <code>limit</code>               (<code>int</code>, default:                   <code>-1</code> )           \u2013            <p>Limit on the number of results. <code>-1</code> for unlimited.</p> </li> <li> <code>offset</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Offset into the results.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[Worker]</code>           \u2013            <p>Sequence of Workers. Returns an empty sequence when none exist.</p> </li> <li> <code>Sequence[Worker]</code>           \u2013            <p>The return value is not guaranteed to be a list.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.start_attempt","title":"<code>start_attempt(rollout_id, worker_id=None)</code>  <code>async</code>","text":"<p>Create a manual retry attempt for an existing rollout.</p> <p>This is typically invoked by runners that wish to retry outside of the normal queue flow (for example in an online RL setup). Implementations must validate that the rollout exists, allocate a fresh <code>attempt_id</code>, increment the <code>sequence_id</code> monotonically, stamp the new attempt with <code>status=\"preparing\"</code>, and return an up-to-date <code>AttemptedRollout</code>.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Unique identifier of the rollout receiving a new attempt.</p> </li> <li> <code>worker_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional worker identifier to associate the new attempt with.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AttemptedRollout</code>           \u2013            <p>The rollout paired with its newly-created attempt.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement attempt creation.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when <code>rollout_id</code> is unknown.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.start_rollout","title":"<code>start_rollout(input, mode=None, resources_id=None, config=None, metadata=None, worker_id=None)</code>  <code>async</code>","text":"<p>Register a rollout and immediately create its first attempt.</p> <p>Note</p> <p>Use <code>enqueue_rollout()</code> when the caller only wants to submit work for later scheduling.</p> <p>The rollout must be persisted with <code>status=\"preparing\"</code> and an initial attempt with <code>sequence_id == 1</code> so the caller can begin execution without visiting the public queue. Implementations are expected to:</p> <ol> <li>Generate a unique <code>rollout_id</code> and <code>attempt_id</code>.</li> <li>Record <code>start_time</code> for both rollout and attempt based on the current clock.</li> <li>Copy <code>config</code> and <code>metadata</code> so later mutations do not leak shared references.</li> <li>Resolve <code>resources_id</code> to the latest resource snapshot when <code>None</code> is supplied.</li> </ol> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>TaskInput</code>)           \u2013            <p>Arbitrary task payload supplied by an algorithm.</p> </li> <li> <code>mode</code>               (<code>RolloutMode | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional semantic mode for downstream analytics (<code>\"train\"</code>, <code>\"val\"</code>, <code>\"test\"</code>).</p> </li> <li> <code>resources_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Concrete resource snapshot to execute against; defaults to the latest stored snapshot.</p> </li> <li> <code>config</code>               (<code>RolloutConfig | None</code>, default:                   <code>None</code> )           \u2013            <p>Rollout retry/timeout policy. Should default to a fresh <code>RolloutConfig</code>.</p> </li> <li> <code>metadata</code>               (<code>Dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Free-form metadata persisted verbatim with the rollout.</p> </li> <li> <code>worker_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional worker identifier to associate the new attempt with.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AttemptedRollout</code>           \u2013            <p>The fully-populated <code>AttemptedRollout</code> including</p> </li> <li> <code>AttemptedRollout</code>           \u2013            <p>the just-created attempt.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must provide durable storage for the rollout.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations should raise when <code>resources_id</code> does not exist.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.statistics","title":"<code>statistics()</code>  <code>async</code>","text":"<p>Return the statistics of the store.</p>"},{"location":"reference/store/#agentlightning.LightningStore.update_attempt","title":"<code>update_attempt(rollout_id, attempt_id, status=UNSET, worker_id=UNSET, last_heartbeat_time=UNSET, metadata=UNSET)</code>  <code>async</code>","text":"<p>Update attempt bookkeeping such as status, worker ownership, and heartbeats.</p> <p>When <code>attempt_id</code> is <code>\"latest\"</code> the update must target the attempt with the highest <code>sequence_id</code>; otherwise it must target the specific attempt. Implementations should propagate status changes to the rollout (for example via <code>rollout_status_from_attempt()</code>) once the latest attempt transitions to a terminal state.</p> <p>Similar to <code>update_rollout()</code>, parameters also default to the sentinel <code>UNSET</code>.</p> <p>If <code>worker_id</code> is present, the worker status will be updated following the rules:</p> <ol> <li>If attempt status is \"succeeded\" or \"failed\", the corresponding worker status will be set to \"idle\".</li> <li>If attempt status is \"unresponsive\" or \"timeout\", the corresponding worker status will be set to \"unknown\".</li> <li>Otherwise, the worker status will be set to \"busy\".</li> </ol> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the rollout whose attempt will be updated.</p> </li> <li> <code>attempt_id</code>               (<code>str | Literal['latest']</code>)           \u2013            <p>Attempt identifier or <code>\"latest\"</code> as a convenience.</p> </li> <li> <code>status</code>               (<code>AttemptStatus | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Replacement attempt status. Terminal statuses must set <code>end_time</code>.</p> </li> <li> <code>worker_id</code>               (<code>str | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Identifier for the worker currently processing the attempt.</p> </li> <li> <code>last_heartbeat_time</code>               (<code>float | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Wall-clock timestamp (seconds) of the latest heartbeat/span.</p> </li> <li> <code>metadata</code>               (<code>Optional[Dict[str, Any]] | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Replacement metadata dictionary.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Attempt</code>           \u2013            <p>The updated attempt record.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement mutation logic.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when the rollout or attempt is unknown.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.update_resources","title":"<code>update_resources(resources_id, resources)</code>  <code>async</code>","text":"<p>Overwrite or extend an existing resource snapshot and mark it as latest.</p> <p>This API is typically used by algorithms that maintain mutable resources (e.g., model checkpoints) under a stable identifier.</p> <p>Parameters:</p> <ul> <li> <code>resources_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the snapshot to replace.</p> </li> <li> <code>resources</code>               (<code>NamedResources</code>)           \u2013            <p>Updated mapping of resource names to payloads.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ResourcesUpdate</code>           \u2013            <p>The persisted <code>ResourcesUpdate</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement resource persistence.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when <code>resources_id</code> does not exist.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.update_rollout","title":"<code>update_rollout(rollout_id, input=UNSET, mode=UNSET, resources_id=UNSET, status=UNSET, config=UNSET, metadata=UNSET)</code>  <code>async</code>","text":"<p>Update rollout metadata and, when provided, drive status transitions.</p> <p>Parameters default to the sentinel <code>UNSET</code> to distinguish omitted fields from explicit <code>None</code> assignments. Implementations must:</p> <ul> <li>Validate the rollout exists before mutating it.</li> <li>Replace each property when a concrete value (including <code>None</code>) is supplied.</li> <li>When the status switches into a terminal state, set <code>end_time</code> and signal any waiters.</li> <li>When the status re-enters a queueing state, ensure the rollout is enqueued exactly once.</li> </ul> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the rollout to update.</p> </li> <li> <code>input</code>               (<code>TaskInput | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Replacement task payload; pass <code>None</code> to explicitly clear the input.</p> </li> <li> <code>mode</code>               (<code>Optional[Literal['train', 'val', 'test']] | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Replacement rollout mode.</p> </li> <li> <code>resources_id</code>               (<code>Optional[str] | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Replacement resources snapshot reference.</p> </li> <li> <code>status</code>               (<code>RolloutStatus | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Target rollout status.</p> </li> <li> <code>config</code>               (<code>RolloutConfig | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Replacement retry/timeout configuration.</p> </li> <li> <code>metadata</code>               (<code>Optional[Dict[str, Any]] | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Replacement metadata dictionary.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Rollout</code>           \u2013            <p>The updated rollout record.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement mutation logic.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when the rollout is unknown or the update is invalid.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.update_worker","title":"<code>update_worker(worker_id, heartbeat_stats=UNSET)</code>  <code>async</code>","text":"<p>Record a heartbeat for <code>worker_id</code> and refresh telemetry.</p> <p>Implementations must treat this API as heartbeat-only: it should snapshot the latest stats when provided, stamp <code>last_heartbeat_time</code> with the current wall clock, and rely on other store mutations (<code>dequeue_rollout</code>, <code>update_attempt</code>, etc.) to drive the worker's busy/idle status, assignment, and activity timestamps.</p> <p>Parameters:</p> <ul> <li> <code>worker_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the worker to update.</p> </li> <li> <code>heartbeat_stats</code>               (<code>Dict[str, Any] | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Replacement worker heartbeat statistics (non-null when provided).</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.wait_for_rollouts","title":"<code>wait_for_rollouts(*, rollout_ids, timeout=None)</code>  <code>async</code>","text":"<p>Block until the targeted rollouts reach a terminal status or the timeout expires.</p> <p>Terminal statuses are <code>\"succeeded\"</code>, <code>\"failed\"</code>, and <code>\"cancelled\"</code>. When the timeout elapses, implementations should return the subset of rollouts that are already terminal and omit the rest.</p> <p>Warning</p> <p>It's dangerous and might be event-loop blocking to call this function with a long timeout. It's a good idea to poll for the method to check if new completed rollouts can coming. Be careful in implementing the sleep logic to avoid busy-waiting.</p> <p>Parameters:</p> <ul> <li> <code>rollout_ids</code>               (<code>List[str]</code>)           \u2013            <p>Identifiers of rollouts to watch.</p> </li> <li> <code>timeout</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Maximum time in seconds to wait. <code>None</code> waits indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Rollout]</code>           \u2013            <p>Rollouts that finished before the deadline, in arbitrary order.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement waiting semantics.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when a rollout identifier is unknown.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStoreCapabilities","title":"<code>agentlightning.LightningStoreCapabilities</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Capability of a LightningStore implementation.</p> <p>All keys are optional and false by default.</p>"},{"location":"reference/store/#agentlightning.LightningStoreCapabilities.async_safe","title":"<code>async_safe</code>  <code>instance-attribute</code>","text":"<p>Whether the store is async-safe.</p>"},{"location":"reference/store/#agentlightning.LightningStoreCapabilities.otlp_traces","title":"<code>otlp_traces</code>  <code>instance-attribute</code>","text":"<p>Whether the store supports OTLP/HTTP traces.</p>"},{"location":"reference/store/#agentlightning.LightningStoreCapabilities.thread_safe","title":"<code>thread_safe</code>  <code>instance-attribute</code>","text":"<p>Whether the store is thread-safe.</p>"},{"location":"reference/store/#agentlightning.LightningStoreCapabilities.zero_copy","title":"<code>zero_copy</code>  <code>instance-attribute</code>","text":"<p>Whether the store has only one copy across all threads/processes.</p>"},{"location":"reference/store/#store-implementations","title":"Store Implementations","text":""},{"location":"reference/store/#agentlightning.InMemoryLightningStore","title":"<code>agentlightning.InMemoryLightningStore</code>","text":"<p>               Bases: <code>CollectionBasedLightningStore[InMemoryLightningCollections]</code></p> <p>In-memory implementation of LightningStore using Python data structures. Thread-safe and async-compatible but data is not persistent.</p> <p>Parameters:</p> <ul> <li> <code>thread_safe</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether the store is thread-safe.</p> </li> <li> <code>eviction_memory_threshold</code>               (<code>float | int | None</code>, default:                   <code>None</code> )           \u2013            <p>The threshold for evicting spans in bytes. By default, it's 70% of the total VRAM available.</p> </li> <li> <code>safe_memory_threshold</code>               (<code>float | int | None</code>, default:                   <code>None</code> )           \u2013            <p>The threshold for safe memory usage in bytes. By default, it's 80% of the eviction threshold.</p> </li> <li> <code>span_size_estimator</code>               (<code>Callable[[Span], int] | None</code>, default:                   <code>None</code> )           \u2013            <p>A function to estimate the size of a span in bytes. By default, it's a simple size estimator that uses sys.getsizeof.</p> </li> <li> <code>tracker</code>               (<code>MetricsBackend | None</code>, default:                   <code>None</code> )           \u2013            <p>The metrics tracker to use.</p> </li> <li> <code>scan_debounce_seconds</code>               (<code>float</code>, default:                   <code>10.0</code> )           \u2013            <p>The debounce time for the scan for unhealthy rollouts. Set to 0 to disable debouncing.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.capabilities","title":"<code>capabilities</code>  <code>property</code>","text":"<p>Return the capabilities of the store.</p>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.statistics","title":"<code>statistics()</code>  <code>async</code>","text":"<p>Return the statistics of the store.</p>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.wait_for_rollout","title":"<code>wait_for_rollout(rollout_id, timeout=None)</code>  <code>async</code>","text":"<p>Wait for a specific rollout to complete with a timeout.</p>"},{"location":"reference/store/#agentlightning.store.mongo.MongoLightningStore","title":"<code>agentlightning.store.mongo.MongoLightningStore</code>","text":"<p>               Bases: <code>CollectionBasedLightningStore[MongoLightningCollections]</code></p> <p>MongoDB implementation of LightningStore using MongoDB collections. Data is persistent and can be shared between multiple processes.</p> <p>Parameters:</p> <ul> <li> <code>mongo_uri</code>               (<code>str</code>, default:                   <code>'mongodb://localhost:27017/?replicaSet=rs0'</code> )           \u2013            <p>MongoDB connection string (defaults to local replica set).</p> </li> <li> <code>mongo_client_kwargs</code>               (<code>Mapping[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Extra keyword arguments forwarded to <code>AsyncMongoClient</code>.</p> </li> <li> <code>database_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The MongoDB database name. Defaults to <code>agentlightning</code>.</p> </li> <li> <code>partition_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The partition id. Useful when sharing the database among multiple Agent-lightning trainers.</p> </li> <li> <code>tracker</code>               (<code>MetricsBackend | None</code>, default:                   <code>None</code> )           \u2013            <p>The metrics tracker to use.</p> </li> <li> <code>scan_debounce_seconds</code>               (<code>float</code>, default:                   <code>10.0</code> )           \u2013            <p>The debounce time for the scan for unhealthy rollouts. Set to 0 to disable debouncing.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.store.mongo.MongoLightningStore.capabilities","title":"<code>capabilities</code>  <code>property</code>","text":"<p>Return the capabilities of the store.</p>"},{"location":"reference/store/#agentlightning.store.mongo.MongoLightningStore.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the store by closing the client pool.</p>"},{"location":"reference/store/#agentlightning.store.mongo.MongoLightningStore.wait_for_rollouts","title":"<code>wait_for_rollouts(*, rollout_ids, timeout=None)</code>  <code>async</code>","text":"<p>Wait for specified rollouts to complete with a timeout.</p> <p>Concurrently wait for all rollouts to complete with a timeout.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore","title":"<code>agentlightning.CollectionBasedLightningStore</code>","text":"<p>               Bases: <code>LightningStore</code>, <code>Generic[T_collections]</code></p> <p>It's the standard implementation of LightningStore that uses collections to store data.</p> <p>If the store implementation is to use the store's default behavior, it's recommended to inherit from this class and override the methods if needed. Bring your own collection implementation by using a different <code>collections</code> argument.</p> <p>The methods in this class should generally not call each other, especially those that are locked.</p> <p>Parameters:</p> <ul> <li> <code>collections</code>               (<code>T_collections</code>)           \u2013            <p>The collections to use for storage.</p> </li> <li> <code>read_snapshot</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Make sure read operations are atomic. If set to true, all read operations like <code>query_rollouts</code> will have better consistency. It may use an isolated snapshot that supports repeatable reads.</p> </li> <li> <code>tracker</code>               (<code>MetricsBackend | None</code>, default:                   <code>None</code> )           \u2013            <p>Enable metrics tracking.</p> </li> <li> <code>scan_debounce_seconds</code>               (<code>float</code>, default:                   <code>10.0</code> )           \u2013            <p>The debounce time for the scan for unhealthy rollouts. Set to 0 to disable debouncing. The debounce is a non-perfect traffic control. It's isolated for each store instance if there are multiple worker replicas.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.capabilities","title":"<code>capabilities</code>  <code>property</code>","text":"<p>Return the capabilities of the store.</p> <p>This store supports no capability. The capability depends on the underlying collections.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.add_many_spans","title":"<code>add_many_spans(spans)</code>  <code>async</code>","text":"<p>Persist a sequence of pre-converted spans.</p> <p>See <code>LightningStore.add_many_spans()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.add_otel_span","title":"<code>add_otel_span(rollout_id, attempt_id, readable_span, sequence_id=None)</code>  <code>async</code>","text":"<p>Add an opentelemetry span to the store.</p> <p>See <code>LightningStore.add_otel_span()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.add_resources","title":"<code>add_resources(resources)</code>  <code>async</code>","text":"<p>Stores a new version of named resources and sets it as the latest.</p> <p>See <code>LightningStore.add_resources()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.add_span","title":"<code>add_span(span)</code>  <code>async</code>","text":"<p>Persist a pre-converted span.</p> <p>See <code>LightningStore.add_span()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.dequeue_many_rollouts","title":"<code>dequeue_many_rollouts(*, limit=1, worker_id=None)</code>  <code>async</code>","text":"<p>Retrieves up to <code>limit</code> tasks from the queue without blocking.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.dequeue_rollout","title":"<code>dequeue_rollout(worker_id=None)</code>  <code>async</code>","text":"<p>Retrieves the next task from the queue without blocking. Returns <code>None</code> if the queue is empty.</p> <p>Will set the rollout status to preparing and create a new attempt.</p> <p>See <code>LightningStore.dequeue_rollout()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.enqueue_many_rollouts","title":"<code>enqueue_many_rollouts(rollouts)</code>  <code>async</code>","text":"<p>Adds many rollouts in a batch.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.enqueue_rollout","title":"<code>enqueue_rollout(input, mode=None, resources_id=None, config=None, metadata=None)</code>  <code>async</code>","text":"<p>Adds a new task to the queue with specific metadata and returns the rollout.</p> <p>See <code>LightningStore.enqueue_rollout()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.get_latest_attempt","title":"<code>get_latest_attempt(rollout_id)</code>  <code>async</code>","text":"<p>Retrieves the latest attempt for a given rollout ID.</p> <p>See <code>LightningStore.get_latest_attempt()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.get_latest_resources","title":"<code>get_latest_resources()</code>  <code>async</code>","text":"<p>Retrieves the latest version of named resources.</p> <p>See <code>LightningStore.get_latest_resources()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.get_many_span_sequence_ids","title":"<code>get_many_span_sequence_ids(rollout_attempt_ids)</code>  <code>async</code>","text":"<p>Get the next span sequence IDs for a given list of rollout and attempt identifiers.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.get_next_span_sequence_id","title":"<code>get_next_span_sequence_id(rollout_id, attempt_id)</code>  <code>async</code>","text":"<p>Get the next span sequence ID for a given rollout and attempt. The number is strictly increasing for each rollout. The store will not issue the same sequence ID twice.</p> <p>See <code>LightningStore.get_next_span_sequence_id()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.get_resources_by_id","title":"<code>get_resources_by_id(resources_id)</code>  <code>async</code>","text":"<p>Retrieves a specific version of named resources by its ID.</p> <p>See <code>LightningStore.get_resources_by_id()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.get_rollout_by_id","title":"<code>get_rollout_by_id(rollout_id)</code>  <code>async</code>","text":"<p>Retrieves a specific rollout by its ID.</p> <p>See <code>LightningStore.get_rollout_by_id()</code> for semantics.</p> <p>If the rollout has been attempted, the latest attempt will also be returned.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.query_attempts","title":"<code>query_attempts(rollout_id, *, sort_by='sequence_id', sort_order='asc', limit=-1, offset=0)</code>  <code>async</code>","text":"<p>Retrieve attempts for a rollout with optional ordering/pagination.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.query_resources","title":"<code>query_resources(*, resources_id=None, resources_id_contains=None, sort_by=None, sort_order='asc', limit=-1, offset=0)</code>  <code>async</code>","text":"<p>Return every stored resource snapshot in insertion order.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.query_rollouts","title":"<code>query_rollouts(*, status_in=None, rollout_id_in=None, rollout_id_contains=None, filter_logic='and', sort_by=None, sort_order='asc', limit=-1, offset=0, status=None, rollout_ids=None)</code>  <code>async</code>","text":"<p>Retrieve rollouts with filtering and pagination.</p> <p>See <code>LightningStore.query_rollouts()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.query_spans","title":"<code>query_spans(rollout_id, attempt_id=None, *, trace_id=None, trace_id_contains=None, span_id=None, span_id_contains=None, parent_id=None, parent_id_contains=None, name=None, name_contains=None, filter_logic='and', limit=-1, offset=0, sort_by='sequence_id', sort_order='asc')</code>  <code>async</code>","text":"<p>Query and retrieve spans associated with a specific rollout ID. Returns an empty list if no spans are found.</p> <p>See <code>LightningStore.query_spans()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.query_workers","title":"<code>query_workers(*, status_in=None, worker_id_contains=None, filter_logic='and', sort_by=None, sort_order='asc', limit=-1, offset=0)</code>  <code>async</code>","text":"<p>Return the current snapshot of all workers.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.start_attempt","title":"<code>start_attempt(rollout_id, worker_id=None)</code>  <code>async</code>","text":"<p>Creates a new attempt for a given rollout ID and return the attempt details.</p> <p>See <code>LightningStore.start_attempt()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.start_rollout","title":"<code>start_rollout(input, mode=None, resources_id=None, config=None, metadata=None, worker_id=None)</code>  <code>async</code>","text":"<p>Notify the store that I'm about to run a rollout.</p> <p>See <code>LightningStore.start_rollout()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.statistics","title":"<code>statistics()</code>  <code>async</code>","text":"<p>Return the statistics of the store.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.update_attempt","title":"<code>update_attempt(rollout_id, attempt_id, status=UNSET, worker_id=UNSET, last_heartbeat_time=UNSET, metadata=UNSET)</code>  <code>async</code>","text":"<p>Update a specific or latest attempt for a given rollout.</p> <p>See <code>LightningStore.update_attempt()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.update_resources","title":"<code>update_resources(collections, resources_id, resources)</code>  <code>async</code>","text":"<p>Safely stores a new version of named resources and sets it as the latest.</p> <p>See <code>LightningStore.update_resources()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.update_rollout","title":"<code>update_rollout(rollout_id, input=UNSET, mode=UNSET, resources_id=UNSET, status=UNSET, config=UNSET, metadata=UNSET)</code>  <code>async</code>","text":"<p>Update the rollout status and related metadata.</p> <p>See <code>LightningStore.update_rollout()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.update_worker","title":"<code>update_worker(worker_id, heartbeat_stats=UNSET)</code>  <code>async</code>","text":"<p>Create or update a worker entry.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.wait_for_rollout","title":"<code>wait_for_rollout(rollout_id, timeout=None)</code>  <code>async</code>","text":"<p>Wait for a specific rollout to complete with a timeout.</p> <p>Subclass may use advanced mechanisms like events to accelerate this.</p> <p>Returns the completed rollout, or None if timeout is reached.</p>"},{"location":"reference/store/#agentlightning.CollectionBasedLightningStore.wait_for_rollouts","title":"<code>wait_for_rollouts(*, rollout_ids, timeout=None)</code>  <code>async</code>","text":"<p>Wait for specified rollouts to complete with a timeout. Returns the completed rollouts, potentially incomplete if timeout is reached.</p> <p>This method does not change the state of the store.</p> <p>See <code>LightningStore.wait_for_rollouts()</code> for semantics.</p>"},{"location":"reference/store/#client-server-and-thread-safe-wrappers","title":"Client-Server and Thread-safe Wrappers","text":""},{"location":"reference/store/#agentlightning.LightningStoreServer","title":"<code>agentlightning.LightningStoreServer</code>","text":"<p>               Bases: <code>LightningStore</code></p> <p>Server wrapper that exposes a LightningStore via HTTP API. Delegates all operations to an underlying store implementation.</p> <p>Healthcheck and watchdog relies on the underlying store.</p> <p><code>agl store</code> is a convenient CLI to start a store server.</p> <p>When the server is executed in a subprocess, the store will discover itself having a different PID and automatically delegate to an HTTP client instead of using the local store. This ensures one single copy of the store will be shared across all processes.</p> <p>This server exporting OTLP-compatible traces via the <code>/v1/traces</code> endpoint.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>LightningStore</code>)           \u2013            <p>The underlying store to delegate operations to.</p> </li> <li> <code>host</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The hostname or IP address to bind the server to.</p> </li> <li> <code>port</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The TCP port to listen on.</p> </li> <li> <code>cors_allow_origins</code>               (<code>Sequence[str] | str | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of CORS origins to allow. Use '*' to allow all origins.</p> </li> <li> <code>launch_mode</code>               (<code>LaunchMode</code>, default:                   <code>'thread'</code> )           \u2013            <p>The launch mode to use for the server. Defaults to \"thread\", which runs the server in a separate thread.</p> </li> <li> <code>launcher_args</code>               (<code>PythonServerLauncherArgs | None</code>, default:                   <code>None</code> )           \u2013            <p>The arguments to use for the server launcher. It's not allowed to set <code>host</code>, <code>port</code>, <code>launch_mode</code> together with <code>launcher_args</code>.</p> </li> <li> <code>n_workers</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of workers to run in the server. Only applicable for <code>mp</code> launch mode.</p> </li> <li> <code>tracker</code>               (<code>MetricsBackend | None</code>, default:                   <code>None</code> )           \u2013            <p>The metrics tracker to use for the server.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStoreServer.capabilities","title":"<code>capabilities</code>  <code>property</code>","text":"<p>Return the capabilities of the store.</p>"},{"location":"reference/store/#agentlightning.LightningStoreServer.endpoint","title":"<code>endpoint</code>  <code>property</code>","text":"<p>Endpoint is the address that the client will use to connect to the server.</p>"},{"location":"reference/store/#agentlightning.LightningStoreServer.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Control pickling to prevent server state from being sent to subprocesses.</p> <p>When LightningStoreServer is pickled (e.g., passed to a subprocess), we only serialize the underlying store and connection details. The client instance and process-awareness state are excluded as they should not be transferred between processes.</p> <p>The subprocess should create its own server instance if needed.</p>"},{"location":"reference/store/#agentlightning.LightningStoreServer.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Restore from pickle by reconstructing only the essential attributes.</p> <p>Note: This creates a new server instance without FastAPI/uvicorn initialized. Call init() pattern or create a new LightningStoreServer if you need a fully functional server in the subprocess. The unpickled server will also have no app and store attributes, this is to make sure there is only one copy of the server in the whole system.</p>"},{"location":"reference/store/#agentlightning.LightningStoreServer.otlp_traces_endpoint","title":"<code>otlp_traces_endpoint()</code>","text":"<p>Return the OTLP/HTTP traces endpoint of the store.</p>"},{"location":"reference/store/#agentlightning.LightningStoreServer.run_forever","title":"<code>run_forever()</code>  <code>async</code>","text":"<p>Runs the FastAPI server indefinitely.</p>"},{"location":"reference/store/#agentlightning.LightningStoreServer.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Starts the FastAPI server in the background.</p> <p>You need to call this method in the same process as the server was created in.</p>"},{"location":"reference/store/#agentlightning.LightningStoreServer.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Gracefully stops the running FastAPI server.</p> <p>You need to call this method in the same process as the server was created in.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient","title":"<code>agentlightning.LightningStoreClient</code>","text":"<p>               Bases: <code>LightningStore</code></p> <p>HTTP client that talks to a remote LightningStoreServer.</p> <p>Parameters:</p> <ul> <li> <code>server_address</code>               (<code>str</code>)           \u2013            <p>The address of the LightningStoreServer to connect to.</p> </li> <li> <code>retry_delays</code>               (<code>Sequence[float]</code>, default:                   <code>(1.0, 2.0, 5.0)</code> )           \u2013            <p>Backoff schedule (seconds) used when the initial request fails for a non-application reason. Each entry is a retry attempt. Setting to an empty sequence to disable retries.</p> </li> <li> <code>health_retry_delays</code>               (<code>Sequence[float]</code>, default:                   <code>(0.1, 0.2, 0.5)</code> )           \u2013            <p>Delays between /health probes while waiting for the server to come back. Setting to an empty sequence to disable health checks.</p> </li> <li> <code>request_timeout</code>               (<code>float</code>, default:                   <code>30.0</code> )           \u2013            <p>Timeout (seconds) for each request.</p> </li> <li> <code>connection_timeout</code>               (<code>float</code>, default:                   <code>5.0</code> )           \u2013            <p>Timeout (seconds) for establishing connection.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStoreClient.capabilities","title":"<code>capabilities</code>  <code>property</code>","text":"<p>Return the capabilities of the store.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient.__getstate__","title":"<code>__getstate__()</code>","text":"<p>When LightningStoreClient is pickled (e.g., passed to a subprocess), we only serialize the server address and retry configurations. The ClientSessions are excluded as they should not be transferred between processes.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Restore from pickle by reconstructing only the essential attributes.</p> <p>Replicating <code>__init__</code> logic to create another client instance in the subprocess.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the HTTP session.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient.dequeue_rollout","title":"<code>dequeue_rollout(worker_id=None)</code>  <code>async</code>","text":"<p>Dequeue a rollout from the server queue.</p> <p>Returns:</p> <ul> <li> <code>Optional[AttemptedRollout]</code>           \u2013            <p>AttemptedRollout if a rollout is available, None if queue is empty.</p> </li> </ul> Note <p>This method does NOT retry on failures. If any exception occurs (network error, server error, etc.), it logs the error and returns None immediately.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient.get_latest_attempt","title":"<code>get_latest_attempt(rollout_id)</code>  <code>async</code>","text":"<p>Get the latest attempt for a rollout.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>ID of the rollout to query.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Attempt]</code>           \u2013            <p>Attempt if found, None if not found or if all retries are exhausted.</p> </li> </ul> Note <p>This method retries on transient failures (network errors, 5xx status codes). If all retries fail, it logs the error and returns None instead of raising an exception.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient.get_latest_resources","title":"<code>get_latest_resources()</code>  <code>async</code>","text":"<p>Get the latest resources.</p> <p>Returns:</p> <ul> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>ResourcesUpdate if found, None if not found or if all retries are exhausted.</p> </li> </ul> Note <p>This method retries on transient failures (network errors, 5xx status codes). If all retries fail, it logs the error and returns None instead of raising an exception.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient.get_resources_by_id","title":"<code>get_resources_by_id(resources_id)</code>  <code>async</code>","text":"<p>Get resources by their ID.</p> <p>Parameters:</p> <ul> <li> <code>resources_id</code>               (<code>str</code>)           \u2013            <p>ID of the resources to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>ResourcesUpdate if found, None if not found or if all retries are exhausted.</p> </li> </ul> Note <p>This method retries on transient failures (network errors, 5xx status codes). If all retries fail, it logs the error and returns None instead of raising an exception.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient.get_rollout_by_id","title":"<code>get_rollout_by_id(rollout_id)</code>  <code>async</code>","text":"<p>Get a rollout by its ID.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>ID of the rollout to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Rollout]</code>           \u2013            <p>Rollout if found, None if not found or if all retries are exhausted.</p> </li> </ul> Note <p>This method retries on transient failures (network errors, 5xx status codes). If all retries fail, it logs the error and returns None instead of raising an exception.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient.otlp_traces_endpoint","title":"<code>otlp_traces_endpoint()</code>","text":"<p>Return the OTLP/HTTP traces endpoint of the store.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient.query_resources","title":"<code>query_resources(*, resources_id=None, resources_id_contains=None, sort_by=None, sort_order='asc', limit=-1, offset=0)</code>  <code>async</code>","text":"<p>List all resource snapshots stored on the server.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient.wait_for_rollouts","title":"<code>wait_for_rollouts(*, rollout_ids, timeout=None)</code>  <code>async</code>","text":"<p>Wait for rollouts to complete.</p> <p>Parameters:</p> <ul> <li> <code>rollout_ids</code>               (<code>List[str]</code>)           \u2013            <p>List of rollout IDs to wait for.</p> </li> <li> <code>timeout</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Timeout in seconds. If not None, the method will raise a ValueError if the timeout is greater than 0.1 seconds.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Rollout]</code>           \u2013            <p>List of rollouts that are completed.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStoreThreaded","title":"<code>agentlightning.LightningStoreThreaded</code>","text":"<p>               Bases: <code>LightningStore</code></p> <p>Facade that delegates all store operations to a underlying store instance.</p> <p>The operations are guaranteed to be thread-safe. Make sure the threaded stores are instantiated before initializing the threads.</p>"},{"location":"reference/store/#agentlightning.LightningStoreThreaded.capabilities","title":"<code>capabilities</code>  <code>property</code>","text":"<p>Return the capabilities of the store.</p>"},{"location":"reference/store/#agentlightning.LightningStoreThreaded.statistics","title":"<code>statistics()</code>  <code>async</code>","text":"<p>Return the statistics of the store.</p>"},{"location":"reference/store/#collections-and-collection-implementations","title":"Collections and Collection Implementations","text":""},{"location":"reference/store/#agentlightning.store.collection.AtomicMode","title":"<code>agentlightning.store.collection.AtomicMode = Literal['r', 'w', 'rw']</code>  <code>module-attribute</code>","text":"<p>What is expected within the atomic context. Can be \"read\", \"write\", or \"read-write\".</p>"},{"location":"reference/store/#agentlightning.store.collection.AtomicLabels","title":"<code>agentlightning.store.collection.AtomicLabels = Literal['rollouts', 'attempts', 'spans', 'resources', 'workers', 'rollout_queue', 'span_sequence_ids', 'generic']</code>  <code>module-attribute</code>","text":"<p>Labels for atomic operations.</p> <p>These labels are used to identify the collections that are affected by the atomic operation.</p> <p>The <code>generic</code> label is used to identify atomic operations that are not associated with any specific collection.</p>"},{"location":"reference/store/#agentlightning.store.collection.Collection","title":"<code>agentlightning.store.collection.Collection</code>","text":"<p>               Bases: <code>TrackedCollection</code>, <code>Generic[T]</code></p> <p>Standard collection interface. Behaves like a list of items. Supporting addition, updating, and deletion of items.</p>"},{"location":"reference/store/#agentlightning.store.collection.Collection.delete","title":"<code>delete(items)</code>  <code>async</code>","text":"<p>Delete the given items from the collection.</p> <p>Parameters:</p> <ul> <li> <code>items</code>               (<code>Sequence[T]</code>)           \u2013            <p>The items to delete from the collection.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the items with the primary keys to be deleted do not exist.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.store.collection.Collection.get","title":"<code>get(filter=None, sort=None)</code>  <code>async</code>","text":"<p>Get the first item that matches the given filters.</p> <p>Parameters:</p> <ul> <li> <code>filter</code>               (<code>Optional[FilterOptions]</code>, default:                   <code>None</code> )           \u2013            <p>The filters to apply to the collection. See <code>FilterOptions</code>.</p> </li> <li> <code>sort</code>               (<code>Optional[SortOptions]</code>, default:                   <code>None</code> )           \u2013            <p>Sort options. See <code>SortOptions</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[T]</code>           \u2013            <p>The first item that matches the given filters, or None if no item matches.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.store.collection.Collection.insert","title":"<code>insert(items)</code>  <code>async</code>","text":"<p>Add the given items to the collection.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an item with the same primary key already exists.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.store.collection.Collection.item_type","title":"<code>item_type()</code>","text":"<p>Get the type of the items in the collection.</p>"},{"location":"reference/store/#agentlightning.store.collection.Collection.primary_keys","title":"<code>primary_keys()</code>","text":"<p>Get the primary keys of the collection.</p>"},{"location":"reference/store/#agentlightning.store.collection.Collection.query","title":"<code>query(filter=None, sort=None, limit=-1, offset=0)</code>  <code>async</code>","text":"<p>Query the collection with the given filters, sort order, and pagination.</p> <p>Parameters:</p> <ul> <li> <code>filter</code>               (<code>Optional[FilterOptions]</code>, default:                   <code>None</code> )           \u2013            <p>The filters to apply to the collection. See <code>FilterOptions</code>.</p> </li> <li> <code>sort</code>               (<code>Optional[SortOptions]</code>, default:                   <code>None</code> )           \u2013            <p>The options for sorting the collection. See <code>SortOptions</code>. The field must exist in the model. If field might contain null values, in which case the behavior is undefined (i.e., depending on the implementation).</p> </li> <li> <code>limit</code>               (<code>int</code>, default:                   <code>-1</code> )           \u2013            <p>Max number of items to return. Use -1 for \"no limit\".</p> </li> <li> <code>offset</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of items to skip from the start of the matching items.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PaginatedResult[T]</code>           \u2013            <p>PaginatedResult with items, limit, offset, and total matched items.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.store.collection.Collection.size","title":"<code>size()</code>  <code>async</code>","text":"<p>Get the number of items in the collection.</p>"},{"location":"reference/store/#agentlightning.store.collection.Collection.update","title":"<code>update(items, update_fields=None)</code>  <code>async</code>","text":"<p>Update the given items in the collection.</p> <p>Parameters:</p> <ul> <li> <code>items</code>               (<code>Sequence[T]</code>)           \u2013            <p>The items to update in the collection.</p> </li> <li> <code>update_fields</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>The fields to update. If not provided, all fields in the type will be updated. Only applicable if the item type is a Pydantic BaseModel.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an item with the primary keys does not exist.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[T]</code>           \u2013            <p>The items that were updated.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.store.collection.Collection.upsert","title":"<code>upsert(items, update_fields=None)</code>  <code>async</code>","text":"<p>Upsert the given items into the collection.</p> <p>If the items with the same primary keys already exist, they will be updated. Otherwise, they will be inserted.</p> <p>The operation has three semantics configurable via <code>update_fields</code>:</p> <ul> <li><code>update_or_insert</code> via <code>collection.upsert(items, update_fields=[\"status\", \"updated_at\"])</code>.   If the item with the same primary keys already exists, only the specified fields will be updated.   Otherwise, the item will be inserted.</li> <li><code>get_or_insert</code> via <code>collection.upsert(items, update_fields=[])</code>.   If the item with the same primary keys already exists, the item will be left unchanged.   Otherwise, the item will be inserted.</li> <li><code>replace_ish</code> via <code>collection.upsert(items)</code>.   If the item with the same primary keys already exists, all fields from the item will be set.   Otherwise, the item will be inserted.</li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[T]</code>           \u2013            <p>The items that were upserted.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.store.collection.Queue","title":"<code>agentlightning.store.collection.Queue</code>","text":"<p>               Bases: <code>TrackedCollection</code>, <code>Generic[T]</code></p> <p>Behaves like a deque. Supporting appending items to the end and popping items from the front.</p>"},{"location":"reference/store/#agentlightning.store.collection.Queue.dequeue","title":"<code>dequeue(limit=1)</code>  <code>async</code>","text":"<p>Pop the given number of items from the front of the queue.</p> <p>Parameters:</p> <ul> <li> <code>limit</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of items to pop from the front of the queue.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[T]</code>           \u2013            <p>The items that were popped from the front of the queue.</p> </li> <li> <code>Sequence[T]</code>           \u2013            <p>If there are less than <code>limit</code> items in the queue, the remaining items will be returned.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.store.collection.Queue.enqueue","title":"<code>enqueue(items)</code>  <code>async</code>","text":"<p>Append the given items to the end of the queue.</p> <p>Parameters:</p> <ul> <li> <code>items</code>               (<code>Sequence[T]</code>)           \u2013            <p>The items to append to the end of the queue.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[T]</code>           \u2013            <p>The items that were appended to the end of the queue.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.store.collection.Queue.has","title":"<code>has(item)</code>  <code>async</code>","text":"<p>Check if the given item is in the queue.</p>"},{"location":"reference/store/#agentlightning.store.collection.Queue.item_type","title":"<code>item_type()</code>","text":"<p>Get the type of the items in the queue.</p>"},{"location":"reference/store/#agentlightning.store.collection.Queue.peek","title":"<code>peek(limit=1)</code>  <code>async</code>","text":"<p>Peek the given number of items from the front of the queue.</p> <p>Parameters:</p> <ul> <li> <code>limit</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of items to peek from the front of the queue.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[T]</code>           \u2013            <p>The items that were peeked from the front of the queue.</p> </li> <li> <code>Sequence[T]</code>           \u2013            <p>If there are less than <code>limit</code> items in the queue, the remaining items will be returned.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.store.collection.Queue.size","title":"<code>size()</code>  <code>async</code>","text":"<p>Get the number of items in the queue.</p>"},{"location":"reference/store/#agentlightning.store.collection.KeyValue","title":"<code>agentlightning.store.collection.KeyValue</code>","text":"<p>               Bases: <code>TrackedCollection</code>, <code>Generic[K, V]</code></p> <p>Behaves like a dictionary. Supporting addition, updating, and deletion of items.</p>"},{"location":"reference/store/#agentlightning.store.collection.KeyValue.chmax","title":"<code>chmax(key, value)</code>  <code>async</code>","text":"<p>Set the value for the given key to the maximum of the current and new value.</p> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the existing value or <code>value</code> is not numeric.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.store.collection.KeyValue.get","title":"<code>get(key, default=None)</code>  <code>async</code>","text":"<p>Get the value for the given key, or the default value if the key is not found.</p>"},{"location":"reference/store/#agentlightning.store.collection.KeyValue.has","title":"<code>has(key)</code>  <code>async</code>","text":"<p>Check if the given key is in the dictionary.</p>"},{"location":"reference/store/#agentlightning.store.collection.KeyValue.inc","title":"<code>inc(key, amount)</code>  <code>async</code>","text":"<p>Increase the numeric value for the given key by <code>amount</code> and return the new value.</p> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the existing value or <code>amount</code> is not numeric.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.store.collection.KeyValue.pop","title":"<code>pop(key, default=None)</code>  <code>async</code>","text":"<p>Pop the value for the given key, or the default value if the key is not found.</p>"},{"location":"reference/store/#agentlightning.store.collection.KeyValue.set","title":"<code>set(key, value)</code>  <code>async</code>","text":"<p>Set the value for the given key.</p>"},{"location":"reference/store/#agentlightning.store.collection.KeyValue.size","title":"<code>size()</code>  <code>async</code>","text":"<p>Get the number of items in the dictionary.</p>"},{"location":"reference/store/#agentlightning.store.collection.LightningCollections","title":"<code>agentlightning.store.collection.LightningCollections</code>","text":"<p>               Bases: <code>TrackedCollection</code></p> <p>Collections of rollouts, attempts, spans, resources, and workers.</p> <p>LightningStore implementations can use this as a storage base to implement the store API.</p>"},{"location":"reference/store/#agentlightning.store.collection.LightningCollections.attempts","title":"<code>attempts</code>  <code>property</code>","text":"<p>Collections of attempts.</p>"},{"location":"reference/store/#agentlightning.store.collection.LightningCollections.resources","title":"<code>resources</code>  <code>property</code>","text":"<p>Collections of resources.</p>"},{"location":"reference/store/#agentlightning.store.collection.LightningCollections.rollout_queue","title":"<code>rollout_queue</code>  <code>property</code>","text":"<p>Queue of rollouts (tasks).</p>"},{"location":"reference/store/#agentlightning.store.collection.LightningCollections.rollouts","title":"<code>rollouts</code>  <code>property</code>","text":"<p>Collections of rollouts.</p>"},{"location":"reference/store/#agentlightning.store.collection.LightningCollections.span_sequence_ids","title":"<code>span_sequence_ids</code>  <code>property</code>","text":"<p>Dictionary (counter) of span sequence IDs.</p>"},{"location":"reference/store/#agentlightning.store.collection.LightningCollections.spans","title":"<code>spans</code>  <code>property</code>","text":"<p>Collections of spans.</p>"},{"location":"reference/store/#agentlightning.store.collection.LightningCollections.workers","title":"<code>workers</code>  <code>property</code>","text":"<p>Collections of workers.</p>"},{"location":"reference/store/#agentlightning.store.collection.LightningCollections.atomic","title":"<code>atomic(*, mode='rw', snapshot=False, commit=False, labels=None, **kwargs)</code>","text":"<p>Perform a atomic operation on the collections.</p> <p>Subclass may use args and kwargs to support multiple levels of atomicity. The arguments can be seen as tags. They only imply the behavior of the operation, not the implementation.</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>AtomicMode</code>, default:                   <code>'rw'</code> )           \u2013            <p>The mode of atomicity. See <code>AtomicMode</code>.</p> </li> <li> <code>snapshot</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable read snapshot for repeatable reads. Data consistency is guaranteed. The real behavior is implementation-dependent.</p> </li> <li> <code>commit</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable commitment for write operations. Unsuccessful operations will be rolled back depending on the implementation. Recommend to use <code>execute()</code> for this level to enable automatic retries. Remember that the real behavior is implementation-dependent.</p> </li> <li> <code>labels</code>               (<code>Optional[Sequence[AtomicLabels]]</code>, default:                   <code>None</code> )           \u2013            <p>Labels to add to the atomic operation (commonly used as lock names or collection names).</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments to pass to the operation.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.store.collection.LightningCollections.execute","title":"<code>execute(callback, *, mode='rw', snapshot=False, commit=False, labels=None, **kwargs)</code>  <code>async</code>","text":"<p>Execute the given callback within an atomic operation. Retry on transient errors is implied.</p> <p>See <code>atomic()</code> for more details.</p>"},{"location":"reference/store/#agentlightning.store.collection.ListBasedCollection","title":"<code>agentlightning.store.collection.ListBasedCollection</code>","text":"<p>               Bases: <code>Collection[T]</code></p> <p>In-memory implementation of Collection using a nested dict for O(1) primary-key lookup.</p> <p>The internal structure is:</p> <pre><code>{\n    pk1_value: {\n        pk2_value: {\n            ...\n                pkN_value: item\n        }\n    }\n}\n</code></pre> <p>where the nesting depth equals the number of primary keys.</p> <p>Sorting behavior:</p> <ol> <li>If no sort_by is provided, the items are returned in the order of insertion.</li> <li>If sort_by is provided, the items are sorted by the value of the sort_by field.</li> <li>If the sort_by field is a timestamp, the null values are treated as infinity.</li> <li>If the sort_by field is not a timestamp, the null values are treated as empty string    if the field is str-like, 0 if the field is int-like, 0.0 if the field is float-like.</li> </ol>"},{"location":"reference/store/#agentlightning.store.collection.ListBasedCollection.delete","title":"<code>delete(items)</code>  <code>async</code>","text":"<p>Delete the given items.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If any item with the given primary keys does not exist.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.store.collection.ListBasedCollection.get","title":"<code>get(filter=None, sort=None)</code>  <code>async</code>","text":"<p>Return the first (or best-sorted) item that matches the given filters, or None.</p>"},{"location":"reference/store/#agentlightning.store.collection.ListBasedCollection.insert","title":"<code>insert(items)</code>  <code>async</code>","text":"<p>Insert the given items.</p> <p>Raises:</p> <ul> <li> <code>DuplicatedPrimaryKeyError</code>             \u2013            <p>If any item with the same primary keys already exists.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.store.collection.ListBasedCollection.item_type","title":"<code>item_type()</code>","text":"<p>Return the Pydantic model type of items stored in this collection.</p>"},{"location":"reference/store/#agentlightning.store.collection.ListBasedCollection.primary_keys","title":"<code>primary_keys()</code>","text":"<p>Return the primary key field names for this collection.</p>"},{"location":"reference/store/#agentlightning.store.collection.ListBasedCollection.query","title":"<code>query(filter=None, sort=None, limit=-1, offset=0)</code>  <code>async</code>","text":"<p>Query the collection with filters, sort order, and pagination.</p> <p>Parameters:</p> <ul> <li> <code>filter</code>               (<code>Optional[FilterOptions]</code>, default:                   <code>None</code> )           \u2013            <p>Mapping of field name to operator dict along with the optional <code>_aggregate</code> logic.</p> </li> <li> <code>sort</code>               (<code>Optional[SortOptions]</code>, default:                   <code>None</code> )           \u2013            <p>Options describing which field to sort by and in which order.</p> </li> <li> <code>limit</code>               (<code>int</code>, default:                   <code>-1</code> )           \u2013            <p>Max number of items to return. Use -1 for \"no limit\".</p> </li> <li> <code>offset</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of items to skip from the start of the matching items.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.store.collection.ListBasedCollection.size","title":"<code>size()</code>  <code>async</code>","text":"<p>Return the number of items stored in the collection.</p>"},{"location":"reference/store/#agentlightning.store.collection.ListBasedCollection.update","title":"<code>update(items, update_fields=None)</code>  <code>async</code>","text":"<p>Update the given items.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If any item with the given primary keys does not exist.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.store.collection.ListBasedCollection.upsert","title":"<code>upsert(items, update_fields=None)</code>  <code>async</code>","text":"<p>Upsert the given items (insert if missing, otherwise update).</p>"},{"location":"reference/store/#agentlightning.store.collection.DequeBasedQueue","title":"<code>agentlightning.store.collection.DequeBasedQueue</code>","text":"<p>               Bases: <code>Queue[T]</code></p> <p>Queue implementation backed by collections.deque.</p> <p>Provides O(1) amortized enqueue (append) and dequeue (popleft).</p>"},{"location":"reference/store/#agentlightning.store.collection.DictBasedKeyValue","title":"<code>agentlightning.store.collection.DictBasedKeyValue</code>","text":"<p>               Bases: <code>KeyValue[K, V]</code></p> <p>KeyValue implementation backed by a plain dictionary.</p>"},{"location":"reference/store/#agentlightning.store.collection.InMemoryLightningCollections","title":"<code>agentlightning.store.collection.InMemoryLightningCollections</code>","text":"<p>               Bases: <code>LightningCollections</code></p> <p>In-memory implementation of LightningCollections using Python data structures.</p> <p>Serves as the storage base for <code>InMemoryLightningStore</code>.</p>"},{"location":"reference/store/#agentlightning.store.collection.InMemoryLightningCollections.atomic","title":"<code>atomic(*, mode='rw', snapshot=False, labels=None, **kwargs)</code>  <code>async</code>","text":"<p>In-memory collections apply a lock outside. It doesn't need to manipulate the collections inside.</p> <p>Skip the locking if mode is \"r\" and snapshot is False.</p> <p>This collection implementation does NOT support rollback / commit.</p>"},{"location":"reference/store/#agentlightning.store.collection.InMemoryLightningCollections.evict_spans_for_rollout","title":"<code>evict_spans_for_rollout(rollout_id)</code>  <code>async</code>","text":"<p>Evict all spans for a given rollout ID.</p> <p>Uses private API for efficiency.</p>"},{"location":"reference/store/#agentlightning.store.collection.mongo.MongoBasedCollection","title":"<code>agentlightning.store.collection.mongo.MongoBasedCollection</code>","text":"<p>               Bases: <code>Collection[T_model]</code></p> <p>Mongo-based implementation of Collection.</p> <p>Parameters:</p> <ul> <li> <code>client_pool</code>               (<code>MongoClientPool[Mapping[str, Any]]</code>)           \u2013            <p>The pool of MongoDB clients.</p> </li> <li> <code>database_name</code>               (<code>str</code>)           \u2013            <p>The name of the database.</p> </li> <li> <code>collection_name</code>               (<code>str</code>)           \u2013            <p>The name of the collection.</p> </li> <li> <code>partition_id</code>               (<code>str</code>)           \u2013            <p>The partition ID. Used to partition the collection into multiple collections.</p> </li> <li> <code>primary_keys</code>               (<code>Sequence[str]</code>)           \u2013            <p>The primary keys of the collection.</p> </li> <li> <code>item_type</code>               (<code>Type[T_model]</code>)           \u2013            <p>The type of the items in the collection.</p> </li> <li> <code>extra_indexes</code>               (<code>Sequence[Sequence[str]]</code>, default:                   <code>[]</code> )           \u2013            <p>The extra indexes to create on the collection.</p> </li> <li> <code>tracker</code>               (<code>MetricsBackend | None</code>, default:                   <code>None</code> )           \u2013            <p>The metrics tracker to use.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.store.collection.mongo.MongoBasedCollection.ensure_collection","title":"<code>ensure_collection()</code>  <code>async</code>","text":"<p>Ensure the backing MongoDB collection exists (and optionally its indexes).</p> <p>This method is idempotent and safe to call multiple times.</p> <p>It will also create a unique index across the configured primary key fields.</p>"},{"location":"reference/store/#agentlightning.store.collection.mongo.MongoBasedCollection.insert","title":"<code>insert(items)</code>  <code>async</code>","text":"<p>Insert items into the collection.</p> <p>The implementation does NOT do checks for duplicate primary keys, neither within the same insert call nor across different insert calls. It relies on the database to enforce uniqueness via indexes.</p>"},{"location":"reference/store/#agentlightning.store.collection.mongo.MongoBasedCollection.primary_keys","title":"<code>primary_keys()</code>","text":"<p>Return the primary key field names for this collection.</p>"},{"location":"reference/store/#agentlightning.store.collection.mongo.MongoBasedCollection.query","title":"<code>query(filter=None, sort=None, limit=-1, offset=0)</code>  <code>async</code>","text":"<p>Mongo-based implementation of Collection.query.</p> <p>The handling of null-values in sorting is different from memory-based implementation. In MongoDB, null values are treated as less than non-null values.</p>"},{"location":"reference/store/#agentlightning.store.collection.mongo.MongoBasedCollection.with_session","title":"<code>with_session(session)</code>","text":"<p>Create a new collection with the same configuration but a new session.</p>"},{"location":"reference/store/#agentlightning.store.collection.mongo.MongoBasedQueue","title":"<code>agentlightning.store.collection.mongo.MongoBasedQueue</code>","text":"<p>               Bases: <code>Queue[T_generic]</code>, <code>Generic[T_generic]</code></p> <p>Mongo-based implementation of Queue backed by a MongoDB collection.</p> <p>Items are stored append-only; dequeue marks items as consumed instead of deleting them.</p>"},{"location":"reference/store/#agentlightning.store.collection.mongo.MongoBasedQueue.__init__","title":"<code>__init__(client_pool, database_name, collection_name, partition_id, item_type, tracker=None)</code>","text":"<p>Parameters:</p> <ul> <li> <code>client_pool</code>               (<code>MongoClientPool[Mapping[str, Any]]</code>)           \u2013            <p>The pool of MongoDB clients.</p> </li> <li> <code>database_name</code>               (<code>str</code>)           \u2013            <p>The name of the database.</p> </li> <li> <code>collection_name</code>               (<code>str</code>)           \u2013            <p>The name of the collection backing the queue.</p> </li> <li> <code>partition_id</code>               (<code>str</code>)           \u2013            <p>Partition identifier; allows multiple logical queues in one collection.</p> </li> <li> <code>item_type</code>               (<code>Type[T_generic]</code>)           \u2013            <p>The Python type of queue items (primitive or BaseModel subclass).</p> </li> </ul>"},{"location":"reference/store/#agentlightning.store.collection.mongo.MongoBasedQueue.ensure_collection","title":"<code>ensure_collection()</code>  <code>async</code>","text":"<p>Ensure the backing collection exists.</p> <p>If it already exists, it returns the existing collection.</p>"},{"location":"reference/store/#agentlightning.store.collection.mongo.MongoBasedKeyValue","title":"<code>agentlightning.store.collection.mongo.MongoBasedKeyValue</code>","text":"<p>               Bases: <code>KeyValue[K, V]</code>, <code>Generic[K, V]</code></p> <p>Mongo-based implementation of KeyValue.</p>"},{"location":"reference/store/#agentlightning.store.collection.mongo.MongoBasedKeyValue.__init__","title":"<code>__init__(client_pool, database_name, collection_name, partition_id, key_type, value_type, tracker=None)</code>","text":"<p>Parameters:</p> <ul> <li> <code>client_pool</code>               (<code>MongoClientPool[Mapping[str, Any]]</code>)           \u2013            <p>The pool of MongoDB clients.</p> </li> <li> <code>database_name</code>               (<code>str</code>)           \u2013            <p>The name of the database.</p> </li> <li> <code>collection_name</code>               (<code>str</code>)           \u2013            <p>The name of the collection backing the key-value store.</p> </li> <li> <code>partition_id</code>               (<code>str</code>)           \u2013            <p>Partition identifier; allows multiple logical maps in one collection.</p> </li> <li> <code>key_type</code>               (<code>Type[K]</code>)           \u2013            <p>The Python type of keys (primitive or BaseModel).</p> </li> <li> <code>value_type</code>               (<code>Type[V]</code>)           \u2013            <p>The Python type of values (primitive or BaseModel).</p> </li> <li> <code>tracker</code>               (<code>MetricsBackend | None</code>, default:                   <code>None</code> )           \u2013            <p>The metrics tracker to use.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.store.collection.mongo.MongoBasedKeyValue.ensure_collection","title":"<code>ensure_collection(*, create_indexes=True)</code>  <code>async</code>","text":"<p>Ensure the backing collection exists (and optionally its indexes).</p>"},{"location":"reference/store/#agentlightning.store.collection.mongo.MongoClientPool","title":"<code>agentlightning.store.collection.mongo.MongoClientPool</code>","text":"<p>               Bases: <code>Generic[T_mapping]</code></p> <p>A pool of MongoDB clients, each bound to a specific event loop.</p> <p>The pool lazily creates <code>AsyncMongoClient</code> instances per event loop using the provided connection parameters, ensuring we never try to reuse a client across loops.</p>"},{"location":"reference/store/#agentlightning.store.collection.mongo.MongoClientPool.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close all clients currently tracked by the pool.</p>"},{"location":"reference/store/#agentlightning.store.collection.mongo.MongoLightningCollections","title":"<code>agentlightning.store.collection.mongo.MongoLightningCollections</code>","text":"<p>               Bases: <code>LightningCollections</code></p> <p>Mongo implementation of LightningCollections using MongoDB collections.</p> <p>Serves as the storage base for <code>MongoLightningStore</code>.</p>"},{"location":"reference/store/#agentlightning.store.collection.mongo.MongoLightningCollections.atomic","title":"<code>atomic(mode='rw', snapshot=False, commit=False, labels=None, *args, **kwargs)</code>  <code>async</code>","text":"<p>Perform a atomic operation on the collections.</p>"},{"location":"reference/store/#agentlightning.store.collection.mongo.MongoLightningCollections.execute","title":"<code>execute(callback, *, mode='rw', snapshot=False, commit=False, labels=None, **kwargs)</code>  <code>async</code>","text":"<p>Execute the given callback within an atomic operation, and with retries on transient errors.</p>"},{"location":"reference/trainer/","title":"Agent-lightning Trainer","text":""},{"location":"reference/trainer/#agentlightning.Trainer","title":"<code>agentlightning.Trainer</code>","text":"<p>               Bases: <code>TrainerLegacy</code></p> <p>High-level orchestration layer that wires Algorithm &lt;-&gt; Runner &lt;-&gt; Store.</p> <p>A <code>Trainer</code> packages the moving parts of Agent-Lightning's training loop into a single entry point:</p> <ul> <li>Algorithm lifecycle: Instantiates or accepts an <code>Algorithm</code>,   attaches the current <code>LightningStore</code>, adapter, and   initial resources, then executes the algorithm role inside the configured execution strategy.</li> <li>Runner fleet: Spawns one or more <code>Runner</code> instances (defaulting   to <code>LitAgentRunner</code>) that hydrate a <code>LitAgent</code>,   claim rollouts, stream spans, and respect graceful termination signals from the execution strategy.</li> <li>Execution strategy: Delegates process management to an   <code>ExecutionStrategy</code> (shared memory, client/server, etc.),   so advanced users can swap orchestration backends without changing trainer code.</li> <li>Telemetry plumbing: Ensures tracers, adapters, and optional <code>LLMProxy</code>   are wired into both algorithm and runners so telemetry flows back into the store.</li> </ul> <p>The trainer exposes two convenience entry points: <code>fit()</code> for full training and <code>dev()</code> for fast, reproducible dry-runs. See the Train the First Agent and Write the First Algorithm tutorials for the broader context.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.adapter","title":"<code>adapter = self._make_adapter(adapter_spec)</code>  <code>instance-attribute</code>","text":"<p>An instance of <code>TraceAdapter</code> to export data consumble by algorithms from traces.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.algorithm","title":"<code>algorithm = self._make_algorithm(algorithm)</code>  <code>instance-attribute</code>","text":"<p>An instance of <code>Algorithm</code> to use for training.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.daemon","title":"<code>daemon = daemon</code>  <code>instance-attribute</code>","text":"<p>Whether worker processes should be daemons. Daemon processes are terminated automatically when the main process exits. Deprecated. Only have effect with <code>fit_v0</code>.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.hooks","title":"<code>hooks = self._normalize_hooks(hooks)</code>  <code>instance-attribute</code>","text":"<p>A sequence of <code>Hook</code> instances to be called at various lifecycle stages (e.g., <code>on_trace_start</code>, <code>on_trace_end</code>, <code>on_rollout_start</code>, <code>on_rollout_end</code>).</p>"},{"location":"reference/trainer/#agentlightning.Trainer.initial_resources","title":"<code>initial_resources = initial_resources</code>  <code>instance-attribute</code>","text":"<p>An instance of <code>NamedResources</code> to use for bootstrapping the fit/dev process.</p> <p>The resources will be handed over to the algorithm. Note that not all algorithms support seeding resources.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.llm_proxy","title":"<code>llm_proxy = self._make_llm_proxy(llm_proxy, store=(self.store))</code>  <code>instance-attribute</code>","text":"<p>An instance of <code>LLMProxy</code> to use for intercepting the LLM calls. If not provided, algorithm may create one on its own.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.max_rollouts","title":"<code>max_rollouts = max_rollouts</code>  <code>instance-attribute</code>","text":"<p>Maximum number of rollouts to process per runner. If None, workers run until no more rollouts are available.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.max_tasks","title":"<code>max_tasks = max_tasks if max_tasks is not None else max_rollouts</code>  <code>instance-attribute</code>","text":"<p>Maximum number of tasks to process per runner. Deprecated in favor of <code>max_rollouts</code>.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.n_runners","title":"<code>n_runners = n_runners</code>  <code>instance-attribute</code>","text":"<p>Number of agent runners to run in parallel.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.n_workers","title":"<code>n_workers = n_runners</code>  <code>instance-attribute</code>","text":"<p>Number of agent workers to run in parallel. Deprecated in favor of <code>n_runners</code>.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.port","title":"<code>port = port</code>  <code>instance-attribute</code>","text":"<p>Port forwarded to <code>ClientServerExecutionStrategy</code>.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.runner","title":"<code>runner = self._make_runner(runner)</code>  <code>instance-attribute</code>","text":"<p>An instance of <code>Runner</code> to use for running the agent.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.store","title":"<code>store = self._make_store(store, self.strategy)</code>  <code>instance-attribute</code>","text":"<p>An instance of <code>LightningStore</code> to use for storing tasks and traces.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.strategy","title":"<code>strategy = self._make_strategy(strategy, n_runners=(self.n_runners), port=port)</code>  <code>instance-attribute</code>","text":"<p>An instance of <code>ExecutionStrategy</code> to use for spawning the algorithm and runners.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.tracer","title":"<code>tracer = self._make_tracer(tracer)</code>  <code>instance-attribute</code>","text":"<p>A tracer instance, or a string pointing to the class full name or a dictionary with a 'type' key that specifies the class full name and other initialization parameters. If None, a default <code>AgentOpsTracer</code> will be created with the current settings.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.triplet_exporter","title":"<code>triplet_exporter = self.adapter</code>  <code>instance-attribute</code>","text":"<p>An instance of <code>TracerTraceToTriplet</code> to export triplets from traces, or a dictionary with the initialization parameters for the exporter. Deprecated. Use <code>adapter</code> instead.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.__init__","title":"<code>__init__(*, dev=False, n_runners=None, max_rollouts=None, initial_resources=None, tracer=None, adapter=None, store=None, runner=None, strategy=None, port=None, algorithm=None, llm_proxy=None, n_workers=None, max_tasks=None, daemon=True, triplet_exporter=None, hooks=None)</code>","text":"<p>Configure the trainer and resolve user-provided component specifications.</p> <p>Each keyword accepts either a concrete instance, a class, a callable factory, a registry string, or a lightweight configuration dictionary (see <code>build_component()</code>).</p> <p>When <code>port</code> is provided it is forwarded to <code>ClientServerExecutionStrategy</code> instances constructed (or supplied) for the trainer.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.dev","title":"<code>dev(agent, train_dataset=None, *, val_dataset=None)</code>","text":"<p>Exercise the infrastructure using a fast, synchronous algorithm.</p> <p><code>Trainer.dev</code> mirrors <code>fit()</code> but insists on an <code>Algorithm</code> subtype that also derives from <code>FastAlgorithm</code>. This keeps the loop responsive for debugging while still touching the same store, runners, hooks, and tracer plumbing.</p> <p>If no algorithm is provided, a default <code>Baseline</code> algorithm will be used.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>LitAgent[T_co]</code>)           \u2013            <p><code>LitAgent</code> implementation to execute.</p> </li> <li> <code>train_dataset</code>               (<code>Optional[Dataset[T_co]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional iterable passed to the algorithm.</p> </li> <li> <code>val_dataset</code>               (<code>Optional[Dataset[T_co]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional iterable passed to the algorithm.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the configured algorithm does not inherit from <code>FastAlgorithm</code>.</p> </li> </ul>"},{"location":"reference/trainer/#agentlightning.Trainer.fit","title":"<code>fit(agent, train_dataset=None, *, val_dataset=None)</code>","text":"<p>Execute the full algorithm/runner training loop.</p> <p><code>Trainer.fit</code> packages the algorithm and runner bundles, then hands them to the active <code>ExecutionStrategy</code>. The strategy rarely returns until:</p> <ul> <li>The algorithm exhausts the dataset(s) and stops enqueuing rollouts.</li> <li><code>max_rollouts</code> causes individual runners to exit.</li> <li>An exception or interrupt cancels the shared <code>ExecutionEvent</code>.</li> </ul> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>LitAgent[T_co]</code>)           \u2013            <p><code>LitAgent</code> implementation executed by runners.</p> </li> <li> <code>train_dataset</code>               (<code>Optional[Dataset[T_co]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional iterable of rollout inputs consumed by the algorithm.</p> </li> <li> <code>val_dataset</code>               (<code>Optional[Dataset[T_co]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional iterable consumed by validation passes.</p> </li> </ul>"},{"location":"reference/trainer/#agentlightning.build_component","title":"<code>agentlightning.build_component(spec, *, expected_type, spec_name, default_factory=None, allow_none=False, optional_defaults=None, dict_requires_type=True, dict_default_cls=None, type_error_fmt=None, invalid_spec_error_fmt=None, registry=None)</code>","text":"<pre><code>build_component(\n    spec: Union[\n        T,\n        str,\n        Dict[str, Any],\n        type[T],\n        Callable[[], T],\n        None,\n    ],\n    *,\n    expected_type: type[T],\n    spec_name: str,\n    default_factory: Callable[[], T],\n    allow_none: bool = ...,\n    optional_defaults: Optional[OptionalDefaults] = ...,\n    dict_requires_type: bool = ...,\n    dict_default_cls: type[T] | None = ...,\n    type_error_fmt: str | None = ...,\n    invalid_spec_error_fmt: str | None = ...,\n    registry: Optional[Dict[str, str]] = ...\n) -&gt; T\n</code></pre><pre><code>build_component(\n    spec: Union[\n        T,\n        str,\n        Dict[str, Any],\n        type[T],\n        Callable[[], T],\n        None,\n    ],\n    *,\n    expected_type: type[T],\n    spec_name: str,\n    default_factory: None = ...,\n    allow_none: bool,\n    optional_defaults: Optional[OptionalDefaults] = ...,\n    dict_requires_type: bool = ...,\n    dict_default_cls: type[T] | None = ...,\n    type_error_fmt: str | None = ...,\n    invalid_spec_error_fmt: str | None = ...,\n    registry: Optional[Dict[str, str]] = ...\n) -&gt; T | None\n</code></pre><pre><code>build_component(\n    spec: Union[\n        T,\n        str,\n        Dict[str, Any],\n        type[T],\n        Callable[[], T],\n        None,\n    ],\n    *,\n    expected_type: type[T],\n    spec_name: str,\n    default_factory: None = ...,\n    allow_none: bool = ...,\n    optional_defaults: Optional[OptionalDefaults] = ...,\n    dict_requires_type: bool = ...,\n    dict_default_cls: type[T] | None = ...,\n    type_error_fmt: str | None = ...,\n    invalid_spec_error_fmt: str | None = ...,\n    registry: Optional[Dict[str, str]] = ...\n) -&gt; T | None\n</code></pre> <p>Build and return a component instance from a flexible specification.</p> <p>This function provides a flexible way to create component instances from various input formats including direct instances, class types, factory functions, import paths, or configuration dictionaries.</p> <p>Parameters:</p> <ul> <li> <code>spec</code>               (<code>Union[T, str, Dict[str, Any], type[T], Callable[[], T], None]</code>)           \u2013            <p>The component specification. Can be: - An instance of expected_type (returned as-is) - A string import path (e.g., 'module.Class') or registry key - A dict with 'type' key (import path or registry key) and constructor kwargs - A class type (will be instantiated) - A factory function (will be called) - None (uses default_factory or returns None if allow_none=True)</p> </li> <li> <code>expected_type</code>               (<code>type[T]</code>)           \u2013            <p>The type that the resulting instance must be or inherit from.</p> </li> <li> <code>spec_name</code>               (<code>str</code>)           \u2013            <p>Descriptive name for the spec, used in error messages.</p> </li> <li> <code>default_factory</code>               (<code>Callable[[], T] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional factory function called when spec is None.</p> </li> <li> <code>allow_none</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, allows None to be returned when spec is None and no default_factory is provided.</p> </li> <li> <code>optional_defaults</code>               (<code>Optional[OptionalDefaults]</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping parameter names to default values or factory functions that will be injected if the constructor accepts them.</p> </li> <li> <code>dict_requires_type</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, dict specs must include a 'type' key.</p> </li> <li> <code>dict_default_cls</code>               (<code>type[T] | None</code>, default:                   <code>None</code> )           \u2013            <p>Default class to use for dict specs without a 'type' key (only used when dict_requires_type=False).</p> </li> <li> <code>type_error_fmt</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Custom format string for type validation errors. Should include {type_name} and {expected_type} placeholders.</p> </li> <li> <code>invalid_spec_error_fmt</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Custom format string for invalid spec type errors. Should include {actual_type} and {expected_type} placeholders.</p> </li> <li> <code>registry</code>               (<code>Optional[Dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional mapping of short names to fully qualified import paths. When provided, string specs or dict 'type'/'name' entries are first resolved through this registry before attempting to import.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>T | None</code>           \u2013            <p>An instance of expected_type, or None if allow_none=True and spec is None</p> </li> <li> <code>T | None</code>           \u2013            <p>without a default_factory.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the instantiated object is not an instance of expected_type.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If spec is None and neither default_factory nor allow_none is set, or if spec type is invalid, or if dict spec is invalid.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Direct instance\n&gt;&gt;&gt; optimizer = build_component(AdamW(), expected_type=Optimizer, spec_name='optimizer')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # String import path\n&gt;&gt;&gt; optimizer = build_component('torch.optim.AdamW', expected_type=Optimizer, spec_name='optimizer')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Dict with type and kwargs\n&gt;&gt;&gt; spec = {'type': 'torch.optim.AdamW', 'lr': 0.001}\n&gt;&gt;&gt; optimizer = build_component(spec, expected_type=Optimizer, spec_name='optimizer')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Class type\n&gt;&gt;&gt; optimizer = build_component(AdamW, expected_type=Optimizer, spec_name='optimizer')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Factory function\n&gt;&gt;&gt; optimizer = build_component(lambda: AdamW(lr=0.001), expected_type=Optimizer,\n...                            spec_name='optimizer')\n</code></pre>"},{"location":"reference/trainer/#execution-strategy","title":"Execution Strategy","text":""},{"location":"reference/trainer/#agentlightning.ExecutionStrategy","title":"<code>agentlightning.ExecutionStrategy</code>","text":"<p>Coordinate algorithm and runner bundles within a single process abstraction.</p> <p>Strategies decide how many worker bundles to launch, whether to communicate through shared memory or an HTTP boundary, and how to react to shutdown signals. They intentionally avoid inspecting the bundle internals; instead, each bundle remains responsible for its own scheduling semantics.</p> <p>Note</p> <p>Implementations must honor the execute() contract by propagating <code>KeyboardInterrupt</code> and ensuring resources are released when an error occurs on either side of the algorithm/runner pair.</p>"},{"location":"reference/trainer/#agentlightning.ExecutionStrategy.execute","title":"<code>execute(algorithm, runner, store)</code>","text":"<p>Run the provided bundles using the configured orchestration model.</p> <p>Parameters:</p> <ul> <li> <code>algorithm</code>               (<code>AlgorithmBundle</code>)           \u2013            <p>Callable bundle responsible for algorithm execution.</p> </li> <li> <code>runner</code>               (<code>RunnerBundle</code>)           \u2013            <p>Callable bundle for runner workers.</p> </li> <li> <code>store</code>               (<code>LightningStore</code>)           \u2013            <p>Concrete <code>LightningStore</code> shared across bundles.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must provide the orchestration implementation.</p> </li> </ul>"},{"location":"reference/trainer/#agentlightning.ClientServerExecutionStrategy","title":"<code>agentlightning.ClientServerExecutionStrategy</code>","text":"<p>               Bases: <code>ExecutionStrategy</code></p> <p>Run algorithm and runner bundles as separate processes over HTTP.</p> <p>Execution Roles:</p> <ul> <li><code>\"algorithm\"</code>: Start <code>LightningStoreServer</code>   in-process and execute the algorithm bundle against it.</li> <li><code>\"runner\"</code>: Connect to an existing server with   <code>LightningStoreClient</code> and run the   runner bundle locally (spawning multiple processes when requested).</li> <li><code>\"both\"</code>: Spawn runner processes first, then execute the algorithm and   server on the same machine. This mode orchestrates the full loop locally.</li> </ul> <p>When <code>role == \"both\"</code> you may choose which side runs on the main process via <code>main_process</code>. The runner-on-main option is limited to <code>n_runners == 1</code> because each additional runner requires its own event loop and process.</p> <p>Warning</p> <p>When <code>main_process == \"runner\"</code> the algorithm and HTTP server execute in a child process. Store mutations remain isolated inside that process, so the original store instance passed to execute() is not updated.</p> <p>Abort Model (four-step escalation):</p> <ol> <li>Cooperative stop. Every bundle receives a shared    <code>MultiprocessingEvent</code> (<code>stop_evt</code>).    Any failure flips the event so peers can exit cleanly. Ctrl+C on the main    process also sets the flag.</li> <li>KeyboardInterrupt synthesis. Remaining subprocesses receive <code>SIGINT</code> to    trigger <code>KeyboardInterrupt</code> handlers.</li> <li>Termination. Stubborn processes are asked to <code>terminate()</code>    (<code>SIGTERM</code> on POSIX).</li> <li>Kill. As a last resort <code>kill()</code> is invoked (<code>SIGKILL</code> on POSIX).</li> </ol> <p>This mirrors the semantics implemented in <code>SharedMemoryExecutionStrategy</code> but adapts them to multiple processes and the HTTP client/server boundary.</p>"},{"location":"reference/trainer/#agentlightning.ClientServerExecutionStrategy.__init__","title":"<code>__init__(role=None, server_host=None, server_port=None, n_runners=1, graceful_timeout=10.0, terminate_timeout=10.0, main_process='algorithm', managed_store=None, allowed_exit_codes=(0, -15))</code>","text":"<p>Configure the strategy.</p> <p>Parameters:</p> <ul> <li> <code>role</code>               (<code>Literal['algorithm', 'runner', 'both'] | None</code>, default:                   <code>None</code> )           \u2013            <p>Which side(s) to run in this process. When omitted, the <code>AGL_CURRENT_ROLE</code> environment variable is used.</p> </li> <li> <code>server_host</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Interface the HTTP server binds to when running the algorithm bundle locally. Defaults to <code>AGL_SERVER_HOST</code> or <code>\"localhost\"</code> if unset.</p> </li> <li> <code>server_port</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Port for the HTTP server in \"algorithm\"/\"both\" modes. Defaults to <code>AGL_SERVER_PORT</code> or <code>4747</code> if unset.</p> </li> <li> <code>n_runners</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of runner processes to spawn in \"runner\"/\"both\".</p> </li> <li> <code>graceful_timeout</code>               (<code>float</code>, default:                   <code>10.0</code> )           \u2013            <p>How long to wait (seconds) after setting the stop event before escalating to signals.</p> </li> <li> <code>terminate_timeout</code>               (<code>float</code>, default:                   <code>10.0</code> )           \u2013            <p>How long to wait between escalation steps beyond the cooperative phase (re-used for SIGINT, terminate, and kill).</p> </li> <li> <code>main_process</code>               (<code>Literal['algorithm', 'runner']</code>, default:                   <code>'algorithm'</code> )           \u2013            <p>Which bundle runs on the main process when <code>role == \"both\"</code>. <code>\"runner\"</code> requires <code>n_runners == 1</code> and is primarily intended for debugging.</p> </li> <li> <code>managed_store</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>When <code>True</code> (default) the strategy constructs LightningStore client/server wrappers automatically. When <code>False</code> the provided <code>store</code> is passed directly to the bundles, allowing callers to manage store wrappers manually.</p> </li> <li> <code>allowed_exit_codes</code>               (<code>Iterable[int]</code>, default:                   <code>(0, -15)</code> )           \u2013            <p>Allowed exit codes for subprocesses. By default, runner can exit gracefully with code 0 or terminated by SIGTERM (-15).</p> </li> </ul>"},{"location":"reference/trainer/#agentlightning.SharedMemoryExecutionStrategy","title":"<code>agentlightning.SharedMemoryExecutionStrategy</code>","text":"<p>               Bases: <code>ExecutionStrategy</code></p> <p>Execute bundles in a single process with cooperative worker threads.</p> <p>Stop Model:</p> <ul> <li>All bundles share one <code>ThreadingEvent</code>   named <code>stop_evt</code>.</li> <li>Only the main thread receives <code>KeyboardInterrupt</code>. When Ctrl+C occurs we   set <code>stop_evt</code>.</li> <li>Any exception raised inside a bundle sets <code>stop_evt</code> so other threads can   unwind cooperatively.</li> <li>Once the bundle running on the main thread exits successfully the   treatment depends on <code>main_thread</code>:<ul> <li><code>\"algorithm\"</code>: the runners are asked to stop by setting <code>stop_evt</code>.</li> <li><code>\"runner\"</code>: the algorithm keeps running until it exits naturally.</li> </ul> </li> <li>Background threads are marked as daemons. We join them briefly and log any   stragglers before shutting down.</li> </ul> <p>Note</p> <p>Signals other than <code>SIGINT</code> (such as <code>SIGTERM</code>) are not intercepted; Python's default behavior for those signals is preserved.</p>"},{"location":"reference/trainer/#events","title":"Events","text":""},{"location":"reference/trainer/#agentlightning.ExecutionEvent","title":"<code>agentlightning.ExecutionEvent</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol capturing the cooperative stop contract shared by strategies.</p> <p>Implementations mirror the API of <code>threading.Event</code> and <code>multiprocessing.Event</code> so the rest of the execution layer can remain agnostic to the underlying concurrency primitive.</p> <p>Methods:</p> <pre><code>set: Signal cancellation. The call must be idempotent.\nclear: Reset the event to the unsignaled state.\nis_set: Return ``True`` when cancellation has been requested.\nwait: Block until the event is signaled or an optional timeout elapses.\n</code></pre>"},{"location":"reference/trainer/#agentlightning.ThreadingEvent","title":"<code>agentlightning.ThreadingEvent</code>","text":"<p>Thread-safe implementation of <code>ExecutionEvent</code>.</p>"},{"location":"reference/trainer/#agentlightning.MultiprocessingEvent","title":"<code>agentlightning.MultiprocessingEvent</code>","text":"<p>Process-safe implementation of <code>ExecutionEvent</code>.</p>"},{"location":"reference/trainer/#cli-builder","title":"CLI Builder","text":""},{"location":"reference/trainer/#agentlightning.lightning_cli","title":"<code>agentlightning.lightning_cli(*classes)</code>","text":"<pre><code>lightning_cli(cls1: Type[_C1]) -&gt; _C1\n</code></pre><pre><code>lightning_cli(\n    cls1: Type[_C1], cls2: Type[_C2]\n) -&gt; Tuple[_C1, _C2]\n</code></pre><pre><code>lightning_cli(\n    cls1: Type[_C1], cls2: Type[_C2], cls3: Type[_C3]\n) -&gt; Tuple[_C1, _C2, _C3]\n</code></pre><pre><code>lightning_cli(\n    cls1: Type[_C1],\n    cls2: Type[_C2],\n    cls3: Type[_C3],\n    cls4: Type[_C4],\n) -&gt; Tuple[_C1, _C2, _C3, _C4]\n</code></pre><pre><code>lightning_cli(\n    *classes: Type[CliConfigurable],\n) -&gt; Tuple[CliConfigurable, ...]\n</code></pre> <p>Parses command-line arguments to configure and instantiate provided CliConfigurable classes.</p> <p>Parameters:</p> <ul> <li> <code>*classes</code>               (<code>Type[CliConfigurable]</code>, default:                   <code>()</code> )           \u2013            <p>One or more classes that inherit from CliConfigurable. Each class's       init parameters will be exposed as command-line arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>CliConfigurable | Tuple[CliConfigurable, ...]</code>           \u2013            <p>A tuple of instantiated objects, corresponding to the input classes in order.</p> </li> </ul>"},{"location":"reference/trainer/#logging","title":"Logging","text":""},{"location":"reference/trainer/#agentlightning.configure_logger","title":"<code>agentlightning.configure_logger(level=logging.INFO, name='agentlightning')</code>","text":"<p>Create or reset a namespaced logger with a consistent console format.</p> <p>This helper clears any previously attached handlers before binding a single <code>StreamHandler</code> that writes to standard output. The resulting logger does not propagate to the root logger, preventing duplicate log emission when applications compose multiple logging configurations.</p> <p>Danger</p> <p>This function is deprecated in favor of <code>setup_logging</code>.</p> <p>Parameters:</p> <ul> <li> <code>level</code>               (<code>int</code>, default:                   <code>INFO</code> )           \u2013            <p>Logging level applied both to the logger and the installed handler. Defaults to <code>logging.INFO</code>.</p> </li> <li> <code>name</code>               (<code>str</code>, default:                   <code>'agentlightning'</code> )           \u2013            <p>Dotted path for the logger instance. Defaults to <code>\"agentlightning\"</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Logger</code>           \u2013            <p>Configured logger instance ready for immediate use.</p> </li> </ul> <p>Examples:</p> <pre><code>from agentlightning import configure_logger\n\nlogger = configure_logger(level=logging.INFO)\nlogger.info(\"agent-lightning is ready!\")\n</code></pre>"},{"location":"reference/trainer/#agentlightning.setup_module_logging","title":"<code>agentlightning.setup_module_logging(level='INFO', *, name='agentlightning', console=True, color=True, propagate=False, disable_existing_loggers=False)</code>","text":"<p>Initializes and returns the base logger for <code>agentlightning</code>.</p> <p>This function constructs and applies a <code>dictConfig</code> configuration for the logger hierarchy rooted at <code>name</code>. It supports either rich console formatting (via <code>RichHandler</code>) or plain text formatting, based on the <code>color</code> argument.</p> <p>Unlike <code>setup_logging</code>, this function configures only a single logger namespace and does not attach extra handlers or submodule levels. It is primarily used internally by <code>setup_logging</code> but is also suitable for direct integration in custom logging workflows.</p>"},{"location":"reference/trainer/#agentlightning.setup_logging","title":"<code>agentlightning.setup_logging(level='INFO', *, console=True, color=True, propagate=False, disable_existing_loggers=False, capture_warnings=False, submodule_levels=None, extra_handlers=None, formatter=None, apply_to=None, files=None)</code>","text":"<p>Configures logging for the <code>agentlightning</code> logger hierarchy.</p> <p>This function provides a one-stop setup utility for configuring the <code>agentlightning</code> root logger and optionally its submodules or external loggers. It supports console logging, colored rich output, per-submodule log levels, and optional handler/formatter injection.</p> <p>The setup is intentionally isolated: it does not modify the global root logger or loggers belonging to other libraries unless explicitly directed via <code>apply_to</code>.</p> <p>Parameters:</p> <ul> <li> <code>level</code>               (<code>int | str</code>, default:                   <code>'INFO'</code> )           \u2013            <p>Logging level for the base <code>agentlightning</code> logger. Accepts either an integer (e.g., <code>logging.DEBUG</code>) or a string level name (e.g., <code>\"INFO\"</code>). Defaults to <code>\"INFO\"</code>.</p> </li> <li> <code>console</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to attach a console handler to the logger. Defaults to <code>True</code>.</p> </li> <li> <code>color</code>               (<code>bool | Dict[str, Any]</code>, default:                   <code>True</code> )           \u2013            <p>Enables rich-formatted output using <code>RichHandler</code> when <code>True</code> or a configuration dict. If <code>False</code>, a plain text formatter is used instead. Defaults to <code>True</code>.</p> </li> <li> <code>propagate</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether <code>agentlightning</code> logs should propagate to ancestor loggers. Defaults to <code>False</code>.</p> </li> <li> <code>disable_existing_loggers</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Passed to <code>logging.config.dictConfig</code>. If <code>True</code>, disables all existing configured loggers before applying this configuration. Defaults to <code>False</code>.</p> </li> <li> <code>capture_warnings</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, redirects Python <code>warnings</code> emitted via the <code>warnings</code> module into the logging system. Defaults to <code>False</code>.</p> </li> <li> <code>submodule_levels</code>               (<code>Optional[dict[str, int | str]]</code>, default:                   <code>None</code> )           \u2013            <p>Mapping of submodule logger names to logging levels. If a specified submodule level is more verbose than the base level, a warning is emitted.</p> </li> <li> <code>extra_handlers</code>               (<code>Optional[list[Handler]]</code>, default:                   <code>None</code> )           \u2013            <p>A list of user-provided handlers to attach to the <code>agentlightning</code> logger. Handlers are added idempotently; duplicates are not reattached.</p> </li> <li> <code>formatter</code>               (<code>Optional[Formatter]</code>, default:                   <code>None</code> )           \u2013            <p>A formatter to apply to any handler under <code>agentlightning</code> that does not already have one assigned. Useful for customizing output without overwriting formatters on custom handlers.</p> </li> <li> <code>apply_to</code>               (<code>Optional[list[str]]</code>, default:                   <code>None</code> )           \u2013            <p>A list of additional logger names to configure identically to <code>agentlightning</code> base logger. Their handlers are replaced with copies of the base handlers, and propagation is disabled to avoid duplicate log emission.</p> </li> <li> <code>files</code>               (<code>Optional[str | dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>If a string, attach a FileHandler to the base <code>agentlightning</code> logger. If a dict, for each <code>(logger_name, filename)</code> pair, attach a FileHandler directly to that logger. Each file handler should use the logger's effective level at creation.</p> </li> </ul> Notes <ul> <li>On Windows, this function forces UTF-8 mode in the console to prevent   issues with rich output or special characters.</li> <li>Submodule loggers can generate records below the handler's emission   threshold. Whether such records appear depends on both the logger's   level and the handler's level.</li> <li><code>apply_to</code> loggers inherit the same handlers but do not propagate   upward, yielding isolated, consistent behavior.</li> </ul> <p>Examples:</p> <p>Basic setup:</p> <pre><code>&gt;&gt;&gt; setup()\n</code></pre> <p>Enabling debug mode with no color:</p> <pre><code>&gt;&gt;&gt; setup(level=\"DEBUG\", color=False)\n</code></pre> <p>Overriding specific submodule levels:</p> <pre><code>&gt;&gt;&gt; setup(submodule_levels={\"agentlightning.io\": \"DEBUG\"})\n</code></pre> <p>Attaching an additional file handler:</p> <pre><code>&gt;&gt;&gt; fh = logging.FileHandler(\"app.log\")\n&gt;&gt;&gt; setup(extra_handlers=[fh])\n</code></pre>"},{"location":"reference/types/","title":"Type References","text":""},{"location":"reference/types/#core-types","title":"Core Types","text":""},{"location":"reference/types/#agentlightning.Triplet","title":"<code>agentlightning.Triplet</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single interaction turn captured during reinforcement learning.</p>"},{"location":"reference/types/#agentlightning.RolloutRawResult","title":"<code>agentlightning.RolloutRawResult = Union[None, float, List[ReadableSpan], List[Span], List[SpanCoreFields]]</code>  <code>module-attribute</code>","text":"<p>Rollout result type.</p> <p>Possible return values of <code>rollout</code>.</p>"},{"location":"reference/types/#agentlightning.RolloutMode","title":"<code>agentlightning.RolloutMode = Literal['train', 'val', 'test']</code>  <code>module-attribute</code>","text":"<p>Possible rollout modes.</p>"},{"location":"reference/types/#agentlightning.GenericResponse","title":"<code>agentlightning.GenericResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Generic server response used by compatibility endpoints.</p> <p>Deprecated</p> <p>This response is no longer used by the new <code>LightningStore</code> APIs.</p> <p>Attributes:</p> <ul> <li> <code>status</code>               (<code>str</code>)           \u2013            <p>Status string describing the result of the request.</p> </li> <li> <code>message</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional human readable explanation.</p> </li> <li> <code>data</code>               (<code>Optional[Dict[str, Any]]</code>)           \u2013            <p>Arbitrary payload serialized as JSON.</p> </li> </ul>"},{"location":"reference/types/#agentlightning.ParallelWorkerBase","title":"<code>agentlightning.ParallelWorkerBase</code>","text":"<p>Base class for workloads executed across multiple worker processes.</p> <p>The lifecycle is orchestrated by the main process:</p> <ul> <li><code>init()</code> prepares shared state.</li> <li>Each worker calls <code>init_worker()</code> during start-up.</li> <li><code>run()</code> performs the parallel workload.</li> <li>Workers call <code>teardown_worker()</code> before exiting.</li> <li>The main process finalizes through <code>teardown()</code>.</li> </ul> <p>Subclasses must implement <code>run()</code> and can override other lifecycle hooks.</p>"},{"location":"reference/types/#agentlightning.ParallelWorkerBase.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the base class. This method can be overridden by subclasses.</p>"},{"location":"reference/types/#agentlightning.ParallelWorkerBase.init","title":"<code>init(*args, **kwargs)</code>","text":"<p>Initialize before spawning the workers. This method can be overridden by subclasses.</p>"},{"location":"reference/types/#agentlightning.ParallelWorkerBase.init_worker","title":"<code>init_worker(worker_id, *args, **kwargs)</code>","text":"<p>Initialize the worker. This method can be overridden by subclasses.</p>"},{"location":"reference/types/#agentlightning.ParallelWorkerBase.run","title":"<code>run(*args, **kwargs)</code>","text":"<p>Run the workload. This method can be overridden by subclasses.</p>"},{"location":"reference/types/#agentlightning.ParallelWorkerBase.teardown","title":"<code>teardown(*args, **kwargs)</code>","text":"<p>Teardown after the workers have exited. This method can be overridden by subclasses.</p>"},{"location":"reference/types/#agentlightning.ParallelWorkerBase.teardown_worker","title":"<code>teardown_worker(worker_id, *args, **kwargs)</code>","text":"<p>Teardown the worker. This method can be overridden by subclasses.</p>"},{"location":"reference/types/#agentlightning.Dataset","title":"<code>agentlightning.Dataset</code>","text":"<p>               Bases: <code>Protocol</code>, <code>Generic[T_co]</code></p> <p>The general interface for a dataset.</p> <p>It's currently implemented as a protocol, having a similar interface to <code>torch.utils.data.Dataset</code>. You don't have to inherit from this class; you can use a simple list if you want to.</p>"},{"location":"reference/types/#agentlightning.AttemptStatus","title":"<code>agentlightning.AttemptStatus = Literal['preparing', 'running', 'failed', 'succeeded', 'unresponsive', 'timeout']</code>  <code>module-attribute</code>","text":"<p>The status of an attempt.</p>"},{"location":"reference/types/#agentlightning.RolloutStatus","title":"<code>agentlightning.RolloutStatus = Literal['queuing', 'preparing', 'running', 'failed', 'succeeded', 'cancelled', 'requeuing']</code>  <code>module-attribute</code>","text":"<p>The status of a rollout.</p>"},{"location":"reference/types/#agentlightning.RolloutConfig","title":"<code>agentlightning.RolloutConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration controlling rollout retries and timeouts.</p>"},{"location":"reference/types/#agentlightning.RolloutConfig.max_attempts","title":"<code>max_attempts = Field(default=1, ge=1)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The maximum number of attempts for the rollout, including the first attempt.</p>"},{"location":"reference/types/#agentlightning.RolloutConfig.retry_condition","title":"<code>retry_condition = Field(default_factory=(cast(Callable[[], List[AttemptStatus]], list)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The list of statuses that should trigger a retry.</p>"},{"location":"reference/types/#agentlightning.RolloutConfig.timeout_seconds","title":"<code>timeout_seconds = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The timeout for the rollout, in seconds. None indicates no timeout.</p>"},{"location":"reference/types/#agentlightning.RolloutConfig.unresponsive_seconds","title":"<code>unresponsive_seconds = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The unresponsive timeout for the rollout, in seconds. None indicates no unresponsive timeout.</p>"},{"location":"reference/types/#agentlightning.Rollout","title":"<code>agentlightning.Rollout</code>","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/types/#agentlightning.Rollout.config","title":"<code>config = Field(default_factory=RolloutConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Retry and timeout configuration associated with the rollout.</p>"},{"location":"reference/types/#agentlightning.Rollout.end_time","title":"<code>end_time = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Timestamp when the rollout ended.</p>"},{"location":"reference/types/#agentlightning.Rollout.input","title":"<code>input</code>  <code>instance-attribute</code>","text":"<p>Task input used to generate the rollout.</p>"},{"location":"reference/types/#agentlightning.Rollout.metadata","title":"<code>metadata = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Additional metadata attached to the rollout.</p>"},{"location":"reference/types/#agentlightning.Rollout.mode","title":"<code>mode = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Execution mode such as <code>\"train\"</code>, <code>\"val\"</code> or <code>\"test\"</code>. See <code>RolloutMode</code>.</p>"},{"location":"reference/types/#agentlightning.Rollout.resources_id","title":"<code>resources_id = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Identifier of the resources required to execute the rollout.</p>"},{"location":"reference/types/#agentlightning.Rollout.rollout_id","title":"<code>rollout_id</code>  <code>instance-attribute</code>","text":"<p>Unique identifier for the rollout.</p>"},{"location":"reference/types/#agentlightning.Rollout.start_time","title":"<code>start_time</code>  <code>instance-attribute</code>","text":"<p>Timestamp when the rollout started.</p>"},{"location":"reference/types/#agentlightning.Rollout.status","title":"<code>status = 'queuing'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Latest status emitted by the controller.</p>"},{"location":"reference/types/#agentlightning.EnqueueRolloutRequest","title":"<code>agentlightning.EnqueueRolloutRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Payload describing a rollout to be queued via <code>enqueue_rollout</code>.</p> <p>A subset of fields from <code>Rollout</code> used for queuing new rollouts.</p>"},{"location":"reference/types/#agentlightning.EnqueueRolloutRequest.config","title":"<code>config = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Retry and timeout configuration associated with the rollout.</p>"},{"location":"reference/types/#agentlightning.EnqueueRolloutRequest.input","title":"<code>input</code>  <code>instance-attribute</code>","text":"<p>Task input used to generate the rollout.</p>"},{"location":"reference/types/#agentlightning.EnqueueRolloutRequest.metadata","title":"<code>metadata = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Additional metadata attached to the rollout.</p>"},{"location":"reference/types/#agentlightning.EnqueueRolloutRequest.mode","title":"<code>mode = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Execution mode such as <code>\"train\"</code>, <code>\"val\"</code> or <code>\"test\"</code>. See <code>RolloutMode</code>.</p>"},{"location":"reference/types/#agentlightning.EnqueueRolloutRequest.resources_id","title":"<code>resources_id = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Identifier of the resources required to execute the rollout.</p>"},{"location":"reference/types/#agentlightning.Attempt","title":"<code>agentlightning.Attempt</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Execution attempt for a rollout, including metadata for retries.</p>"},{"location":"reference/types/#agentlightning.Attempt.attempt_id","title":"<code>attempt_id</code>  <code>instance-attribute</code>","text":"<p>The universal id for current attempt.</p>"},{"location":"reference/types/#agentlightning.Attempt.end_time","title":"<code>end_time = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The time when the attempt has ended.</p>"},{"location":"reference/types/#agentlightning.Attempt.last_heartbeat_time","title":"<code>last_heartbeat_time = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The last time when the worker has reported progress (i.e., a span).</p>"},{"location":"reference/types/#agentlightning.Attempt.metadata","title":"<code>metadata = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A bucket for any other relevant information.</p>"},{"location":"reference/types/#agentlightning.Attempt.rollout_id","title":"<code>rollout_id</code>  <code>instance-attribute</code>","text":"<p>The rollout which this attempt belongs to.</p>"},{"location":"reference/types/#agentlightning.Attempt.sequence_id","title":"<code>sequence_id</code>  <code>instance-attribute</code>","text":"<p>The sequence number of the attempt, starting from 1.</p>"},{"location":"reference/types/#agentlightning.Attempt.start_time","title":"<code>start_time</code>  <code>instance-attribute</code>","text":"<p>The time when the attempt has started.</p>"},{"location":"reference/types/#agentlightning.Attempt.status","title":"<code>status = 'preparing'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The status of the attempt.</p>"},{"location":"reference/types/#agentlightning.Attempt.worker_id","title":"<code>worker_id = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The rollout worker which is executing this attempt.</p>"},{"location":"reference/types/#agentlightning.AttemptedRollout","title":"<code>agentlightning.AttemptedRollout</code>","text":"<p>               Bases: <code>Rollout</code></p> <p>Rollout paired with the currently active attempt.</p>"},{"location":"reference/types/#agentlightning.AttemptedRollout.attempt","title":"<code>attempt</code>  <code>instance-attribute</code>","text":"<p>The attempt that is currently processing the rollout.</p>"},{"location":"reference/types/#agentlightning.Worker","title":"<code>agentlightning.Worker</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Worker information. This is actually the same as Runner info.</p>"},{"location":"reference/types/#agentlightning.Worker.current_attempt_id","title":"<code>current_attempt_id = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The ID of the current attempt that the worker is processing.</p>"},{"location":"reference/types/#agentlightning.Worker.current_rollout_id","title":"<code>current_rollout_id = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The ID of the current rollout that the worker is processing.</p>"},{"location":"reference/types/#agentlightning.Worker.heartbeat_stats","title":"<code>heartbeat_stats = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Statistics about the worker's heartbeat.</p>"},{"location":"reference/types/#agentlightning.Worker.last_busy_time","title":"<code>last_busy_time = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The last time when the worker has started an attempt and became busy.</p>"},{"location":"reference/types/#agentlightning.Worker.last_dequeue_time","title":"<code>last_dequeue_time = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The last time when the worker has tried to dequeue a rollout.</p>"},{"location":"reference/types/#agentlightning.Worker.last_heartbeat_time","title":"<code>last_heartbeat_time = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The last time when the worker has reported the stats.</p>"},{"location":"reference/types/#agentlightning.Worker.last_idle_time","title":"<code>last_idle_time = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The last time when the worker has triggered the end of an attempt and became idle.</p>"},{"location":"reference/types/#agentlightning.Worker.status","title":"<code>status = 'unknown'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The status of the worker.</p>"},{"location":"reference/types/#agentlightning.Worker.worker_id","title":"<code>worker_id</code>  <code>instance-attribute</code>","text":"<p>The ID of the worker.</p>"},{"location":"reference/types/#agentlightning.WorkerStatus","title":"<code>agentlightning.WorkerStatus = Literal['idle', 'busy', 'unknown']</code>  <code>module-attribute</code>","text":""},{"location":"reference/types/#agentlightning.Hook","title":"<code>agentlightning.Hook</code>","text":"<p>               Bases: <code>ParallelWorkerBase</code></p> <p>Base class for defining hooks in the agent runner's lifecycle.</p>"},{"location":"reference/types/#agentlightning.Hook.on_rollout_end","title":"<code>on_rollout_end(*, agent, runner, rollout, spans)</code>  <code>async</code>","text":"<p>Hook called after a rollout attempt completes.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>LitAgent[Any]</code>)           \u2013            <p>The <code>LitAgent</code> instance associated with the runner.</p> </li> <li> <code>runner</code>               (<code>Runner[Any]</code>)           \u2013            <p>The <code>Runner</code> managing the rollout.</p> </li> <li> <code>rollout</code>               (<code>Rollout</code>)           \u2013            <p>The <code>Rollout</code> object that has been processed.</p> </li> <li> <code>spans</code>               (<code>Union[List[ReadableSpan], List[Span]]</code>)           \u2013            <p>The spans that have been added to the store.</p> </li> </ul> <p>Subclasses can override this method for cleanup or additional logging. By default, this is a no-op.</p>"},{"location":"reference/types/#agentlightning.Hook.on_rollout_start","title":"<code>on_rollout_start(*, agent, runner, rollout)</code>  <code>async</code>","text":"<p>Hook called immediately before a rollout attempt begins.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>LitAgent[Any]</code>)           \u2013            <p>The <code>LitAgent</code> instance associated with the runner.</p> </li> <li> <code>runner</code>               (<code>Runner[Any]</code>)           \u2013            <p>The <code>Runner</code> managing the rollout.</p> </li> <li> <code>rollout</code>               (<code>Rollout</code>)           \u2013            <p>The <code>Rollout</code> object that will be processed.</p> </li> </ul> <p>Subclasses can override this method to implement custom logic such as logging, metric collection, or resource setup. By default, this is a no-op.</p>"},{"location":"reference/types/#agentlightning.Hook.on_trace_end","title":"<code>on_trace_end(*, agent, runner, tracer, rollout)</code>  <code>async</code>","text":"<p>Hook called immediately after the rollout completes but before the tracer exits the trace context.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>LitAgent[Any]</code>)           \u2013            <p>The <code>LitAgent</code> instance associated with the runner.</p> </li> <li> <code>runner</code>               (<code>Runner[Any]</code>)           \u2013            <p>The <code>Runner</code> managing the rollout.</p> </li> <li> <code>tracer</code>               (<code>Tracer</code>)           \u2013            <p>The <code>Tracer</code> instance associated with the runner.</p> </li> <li> <code>rollout</code>               (<code>Rollout</code>)           \u2013            <p>The <code>Rollout</code> object that has been processed.</p> </li> </ul> <p>Subclasses can override this method to implement custom logic such as logging, metric collection, or resource cleanup. By default, this is a no-op.</p>"},{"location":"reference/types/#agentlightning.Hook.on_trace_start","title":"<code>on_trace_start(*, agent, runner, tracer, rollout)</code>  <code>async</code>","text":"<p>Hook called immediately after the tracer enters the trace context but before the rollout begins.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>LitAgent[Any]</code>)           \u2013            <p>The <code>LitAgent</code> instance associated with the runner.</p> </li> <li> <code>runner</code>               (<code>Runner[Any]</code>)           \u2013            <p>The <code>Runner</code> managing the rollout.</p> </li> <li> <code>tracer</code>               (<code>Tracer</code>)           \u2013            <p>The <code>Tracer</code> instance associated with the runner.</p> </li> <li> <code>rollout</code>               (<code>Rollout</code>)           \u2013            <p>The <code>Rollout</code> object that will be processed.</p> </li> </ul> <p>Subclasses can override this method to implement custom logic such as logging, metric collection, or resource setup. By default, this is a no-op.</p>"},{"location":"reference/types/#agentlightning.PaginatedResult","title":"<code>agentlightning.PaginatedResult</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Sequence[T_item]</code></p> <p>Result of a paginated query.</p> <p>Behaves like a sequence, but also carries pagination metadata (limit, offset, total).</p>"},{"location":"reference/types/#agentlightning.PaginatedResult.items","title":"<code>items</code>  <code>instance-attribute</code>","text":"<p>Items in the result.</p>"},{"location":"reference/types/#agentlightning.PaginatedResult.limit","title":"<code>limit</code>  <code>instance-attribute</code>","text":"<p>Limit of the result.</p>"},{"location":"reference/types/#agentlightning.PaginatedResult.offset","title":"<code>offset</code>  <code>instance-attribute</code>","text":"<p>Offset of the result.</p>"},{"location":"reference/types/#agentlightning.PaginatedResult.total","title":"<code>total</code>  <code>instance-attribute</code>","text":"<p>Total number of items in the collection.</p>"},{"location":"reference/types/#agentlightning.FilterOptions","title":"<code>agentlightning.FilterOptions = Mapping[Union[str, Literal['_aggregate', '_must']], Union[FilterField, Literal['and', 'or'], Mapping[str, FilterField]]]</code>  <code>module-attribute</code>","text":"<p>A mapping of field name -&gt; operator dict.</p> <p>Each operator dict can contain:</p> <ul> <li>\"exact\": value for exact equality.</li> <li>\"within\": iterable of allowed values.</li> <li>\"contains\": substring to search for in string fields.</li> </ul> <p>The filter can also have a special field called \"_aggregate\" that can be used to specify the logic to combine the results of the filters:</p> <ul> <li>\"and\": all conditions must match. This is the default value if not specified.</li> <li>\"or\": at least one condition must match.</li> </ul> <p>All conditions within a field and between different fields are stored in a unified pool and combined using <code>_aggregate</code>.</p> <p>The filter can also have a special group called \"_must\", which is a mapping of filters that must all match, no matter whether the aggregate logic is \"and\" or \"or\".</p> <p>Example:</p> <pre><code>{\n    \"_aggregate\": \"or\",\n    \"_must\": {\n        \"city\": {\"exact\": \"New York\"},\n        \"timezone\": {\"within\": [\"America/New_York\", \"America/Los_Angeles\"]},\n    },\n    \"status\": {\"exact\": \"active\"},\n    \"id\": {\"within\": [1, 2, 3]},\n    \"name\": {\"contains\": \"foo\"},\n}\n</code></pre>"},{"location":"reference/types/#agentlightning.SortOptions","title":"<code>agentlightning.SortOptions</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Options for sorting the collection.</p>"},{"location":"reference/types/#agentlightning.SortOptions.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>The name of the field to sort by.</p>"},{"location":"reference/types/#agentlightning.SortOptions.order","title":"<code>order</code>  <code>instance-attribute</code>","text":"<p>The order to sort by.</p>"},{"location":"reference/types/#agentlightning.FilterField","title":"<code>agentlightning.FilterField</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>An operator dict for a single field.</p>"},{"location":"reference/types/#resources","title":"Resources","text":""},{"location":"reference/types/#agentlightning.Resource","title":"<code>agentlightning.Resource</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for tunable resources distributed to executors.</p>"},{"location":"reference/types/#agentlightning.Resource.resource_type","title":"<code>resource_type</code>  <code>instance-attribute</code>","text":"<p>Alias of the resource type.</p>"},{"location":"reference/types/#agentlightning.LLM","title":"<code>agentlightning.LLM</code>","text":"<p>               Bases: <code>Resource</code></p> <p>Resource that identifies an LLM endpoint and its configuration.</p>"},{"location":"reference/types/#agentlightning.LLM.api_key","title":"<code>api_key = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optional secret used to authenticate requests.</p>"},{"location":"reference/types/#agentlightning.LLM.endpoint","title":"<code>endpoint</code>  <code>instance-attribute</code>","text":"<p>The URL of the LLM API endpoint.</p>"},{"location":"reference/types/#agentlightning.LLM.model","title":"<code>model</code>  <code>instance-attribute</code>","text":"<p>The identifier for the model to be used (e.g., 'gpt-4o').</p>"},{"location":"reference/types/#agentlightning.LLM.sampling_parameters","title":"<code>sampling_parameters = Field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A dictionary of hyperparameters for model inference, such as temperature, top_p, etc.</p>"},{"location":"reference/types/#agentlightning.LLM.get_base_url","title":"<code>get_base_url(*args, **kwargs)</code>","text":"<p>Return the base URL consumed by OpenAI-compatible clients.</p> <p>Users are encouraged to use <code>get_base_url(rollout_id, attempt_id)</code> to get the LLM endpoint instead of accessing <code>.endpoint</code> directly.</p>"},{"location":"reference/types/#agentlightning.ProxyLLM","title":"<code>agentlightning.ProxyLLM</code>","text":"<p>               Bases: <code>LLM</code></p> <p>LLM resource that rewrites endpoints through <code>LLMProxy</code>.</p> <p>The proxy injects rollout- and attempt-specific routing information into the endpoint so that downstream services can attribute requests correctly.</p>"},{"location":"reference/types/#agentlightning.ProxyLLM.__getattribute__","title":"<code>__getattribute__(name)</code>","text":"<p>Emit a warning when <code>endpoint</code> is accessed directly after initialization.</p>"},{"location":"reference/types/#agentlightning.ProxyLLM.get_base_url","title":"<code>get_base_url(rollout_id, attempt_id)</code>","text":"<p>Return the routed endpoint for a specific rollout/attempt pair.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>Optional[str]</code>)           \u2013            <p>Identifier of the rollout making the request.</p> </li> <li> <code>attempt_id</code>               (<code>Optional[str]</code>)           \u2013            <p>Identifier of the attempt within that rollout.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Fully qualified endpoint including rollout metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If exactly one of <code>rollout_id</code> or <code>attempt_id</code> is provided.</p> </li> </ul>"},{"location":"reference/types/#agentlightning.ProxyLLM.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Mark initialization as complete after Pydantic finishes setup.</p>"},{"location":"reference/types/#agentlightning.ProxyLLM.with_attempted_rollout","title":"<code>with_attempted_rollout(rollout)</code>","text":"<p>Bake rollout metadata into a concrete <code>LLM</code> instance.</p>"},{"location":"reference/types/#agentlightning.PromptTemplate","title":"<code>agentlightning.PromptTemplate</code>","text":"<p>               Bases: <code>Resource</code></p> <p>Resource describing a reusable prompt template.</p>"},{"location":"reference/types/#agentlightning.PromptTemplate.engine","title":"<code>engine</code>  <code>instance-attribute</code>","text":"<p>The templating engine to use for rendering the prompt.</p>"},{"location":"reference/types/#agentlightning.PromptTemplate.template","title":"<code>template</code>  <code>instance-attribute</code>","text":"<p>The template string. The format depends on the engine.</p>"},{"location":"reference/types/#agentlightning.PromptTemplate.format","title":"<code>format(**kwargs)</code>","text":"<p>Format the prompt using keyword arguments.</p> <p>Warning</p> <p>Only the <code>f-string</code> engine is supported for now.</p>"},{"location":"reference/types/#agentlightning.ResourceUnion","title":"<code>agentlightning.ResourceUnion = Annotated[Union[LLM, ProxyLLM, PromptTemplate], Field(discriminator='resource_type')]</code>  <code>module-attribute</code>","text":""},{"location":"reference/types/#agentlightning.NamedResources","title":"<code>agentlightning.NamedResources = Dict[str, ResourceUnion]</code>  <code>module-attribute</code>","text":"<p>Mapping from resource names to their configured instances.</p> <p>Examples:</p> <pre><code>resources: NamedResources = {\n    \"main_llm\": LLM(\n        endpoint=\"http://localhost:8080\",\n        model=\"llama3\",\n        sampling_parameters={\"temperature\": 0.7, \"max_tokens\": 100},\n    ),\n    \"system_prompt\": PromptTemplate(\n        template=\"You are a helpful assistant.\",\n        engine=\"f-string\",\n    ),\n}\n</code></pre>"},{"location":"reference/types/#agentlightning.ResourcesUpdate","title":"<code>agentlightning.ResourcesUpdate</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Update payload broadcast to clients when resources change.</p>"},{"location":"reference/types/#agentlightning.ResourcesUpdate.create_time","title":"<code>create_time</code>  <code>instance-attribute</code>","text":"<p>Timestamp of the creation time of the resources.</p>"},{"location":"reference/types/#agentlightning.ResourcesUpdate.resources","title":"<code>resources</code>  <code>instance-attribute</code>","text":"<p>Mapping of resource names to their definitions.</p>"},{"location":"reference/types/#agentlightning.ResourcesUpdate.resources_id","title":"<code>resources_id</code>  <code>instance-attribute</code>","text":"<p>Identifier used to version the resources.</p>"},{"location":"reference/types/#agentlightning.ResourcesUpdate.update_time","title":"<code>update_time</code>  <code>instance-attribute</code>","text":"<p>Timestamp of the last update time of the resources.</p>"},{"location":"reference/types/#agentlightning.ResourcesUpdate.version","title":"<code>version</code>  <code>instance-attribute</code>","text":"<p>Version of the resources.</p>"},{"location":"reference/types/#traces","title":"Traces","text":""},{"location":"reference/types/#agentlightning.AttributeValue","title":"<code>agentlightning.AttributeValue = Union[str, bool, int, float, Sequence[str], Sequence[bool], Sequence[int], Sequence[float]]</code>  <code>module-attribute</code>","text":"<p>Possible values for OpenTelemetry attributes.</p>"},{"location":"reference/types/#agentlightning.Attributes","title":"<code>agentlightning.Attributes = Dict[str, AttributeValue]</code>  <code>module-attribute</code>","text":"<p>Mapping from attribute names to their values. Same as OpenTelemetry <code>Attributes</code> type.</p>"},{"location":"reference/types/#agentlightning.TraceState","title":"<code>agentlightning.TraceState = Dict[str, str]</code>  <code>module-attribute</code>","text":"<p>Mapping from trace state key to its value. Same as OpenTelemetry <code>TraceState</code> type.</p>"},{"location":"reference/types/#agentlightning.SpanContext","title":"<code>agentlightning.SpanContext</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic representation of <code>opentelemetry.trace.SpanContext</code> values.</p>"},{"location":"reference/types/#agentlightning.SpanContext.is_remote","title":"<code>is_remote</code>  <code>instance-attribute</code>","text":"<p>Whether the span is remote.</p>"},{"location":"reference/types/#agentlightning.SpanContext.span_id","title":"<code>span_id</code>  <code>instance-attribute</code>","text":"<p>The span ID of the span.</p>"},{"location":"reference/types/#agentlightning.SpanContext.trace_id","title":"<code>trace_id</code>  <code>instance-attribute</code>","text":"<p>The trace ID of the span.</p>"},{"location":"reference/types/#agentlightning.SpanContext.trace_state","title":"<code>trace_state</code>  <code>instance-attribute</code>","text":"<p>Mapping from trace state key to its value.</p>"},{"location":"reference/types/#agentlightning.SpanContext.from_opentelemetry","title":"<code>from_opentelemetry(src)</code>  <code>classmethod</code>","text":"<p>Construct a <code>SpanContext</code> from OpenTelemetry data.</p>"},{"location":"reference/types/#agentlightning.TraceStatus","title":"<code>agentlightning.TraceStatus</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Serializable variant of <code>opentelemetry.trace.Status</code>.</p>"},{"location":"reference/types/#agentlightning.TraceStatus.description","title":"<code>description = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The description of the span. Same as OpenTelemetry <code>Status.description</code> type.</p>"},{"location":"reference/types/#agentlightning.TraceStatus.status_code","title":"<code>status_code</code>  <code>instance-attribute</code>","text":"<p>The status code of the span. Same as OpenTelemetry <code>Status.status_code</code> type.</p>"},{"location":"reference/types/#agentlightning.TraceStatus.from_opentelemetry","title":"<code>from_opentelemetry(src)</code>  <code>classmethod</code>","text":"<p>Create a <code>TraceStatus</code> from OpenTelemetry metadata.</p>"},{"location":"reference/types/#agentlightning.Event","title":"<code>agentlightning.Event</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Serializable representation of OpenTelemetry <code>Event</code> values.</p>"},{"location":"reference/types/#agentlightning.Event.attributes","title":"<code>attributes</code>  <code>instance-attribute</code>","text":"<p>Mapping from attribute names to their values. Same as OpenTelemetry <code>Attributes</code> type.</p>"},{"location":"reference/types/#agentlightning.Event.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>The name of the event.</p>"},{"location":"reference/types/#agentlightning.Event.timestamp","title":"<code>timestamp = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The timestamp of the event. Same as OpenTelemetry <code>Event.timestamp</code> type.</p>"},{"location":"reference/types/#agentlightning.Event.from_opentelemetry","title":"<code>from_opentelemetry(src)</code>  <code>classmethod</code>","text":"<p>Create an <code>Event</code> from an OpenTelemetry event.</p>"},{"location":"reference/types/#agentlightning.Link","title":"<code>agentlightning.Link</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Serializable representation of OpenTelemetry <code>Link</code> values.</p>"},{"location":"reference/types/#agentlightning.Link.attributes","title":"<code>attributes = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optional attributes.</p>"},{"location":"reference/types/#agentlightning.Link.context","title":"<code>context</code>  <code>instance-attribute</code>","text":"<p>The context of the link.</p>"},{"location":"reference/types/#agentlightning.Link.from_opentelemetry","title":"<code>from_opentelemetry(src)</code>  <code>classmethod</code>","text":"<p>Create a <code>Link</code> from an OpenTelemetry link.</p>"},{"location":"reference/types/#agentlightning.Resource","title":"<code>agentlightning.Resource</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for tunable resources distributed to executors.</p>"},{"location":"reference/types/#agentlightning.Resource.resource_type","title":"<code>resource_type</code>  <code>instance-attribute</code>","text":"<p>Alias of the resource type.</p>"},{"location":"reference/types/#agentlightning.Span","title":"<code>agentlightning.Span</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Agent Lightning's canonical span model used for persistence and analytics.</p> <p>The model captures the most relevant fields from <code>opentelemetry.sdk.trace.ReadableSpan</code> instances while preserving unmodeled attributes in Pydantic <code>BaseModel</code>'s extra storage. This keeps the serialized format stable even as upstream OpenTelemetry types evolve.</p>"},{"location":"reference/types/#agentlightning.Span.attempt_id","title":"<code>attempt_id</code>  <code>instance-attribute</code>","text":"<p>The attempt which this span belongs to.</p>"},{"location":"reference/types/#agentlightning.Span.attributes","title":"<code>attributes</code>  <code>instance-attribute</code>","text":"<p>The attributes of the span. See OpenTelemetry docs.</p>"},{"location":"reference/types/#agentlightning.Span.context","title":"<code>context</code>  <code>instance-attribute</code>","text":"<p>The context of the span. See OpenTelemetry docs.</p>"},{"location":"reference/types/#agentlightning.Span.end_time","title":"<code>end_time</code>  <code>instance-attribute</code>","text":"<p>The end time of the span. See OpenTelemetry docs.</p>"},{"location":"reference/types/#agentlightning.Span.events","title":"<code>events</code>  <code>instance-attribute</code>","text":"<p>The events of the span. See OpenTelemetry docs.</p>"},{"location":"reference/types/#agentlightning.Span.links","title":"<code>links</code>  <code>instance-attribute</code>","text":"<p>The links of the span. See OpenTelemetry docs.</p>"},{"location":"reference/types/#agentlightning.Span.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>The name of the span. See OpenTelemetry docs.</p>"},{"location":"reference/types/#agentlightning.Span.parent","title":"<code>parent</code>  <code>instance-attribute</code>","text":"<p>The parent context of the span. See OpenTelemetry docs.</p>"},{"location":"reference/types/#agentlightning.Span.parent_id","title":"<code>parent_id</code>  <code>instance-attribute</code>","text":"<p>The parent span ID of the span.</p>"},{"location":"reference/types/#agentlightning.Span.resource","title":"<code>resource</code>  <code>instance-attribute</code>","text":"<p>The resource of the span. See OpenTelemetry docs.</p>"},{"location":"reference/types/#agentlightning.Span.rollout_id","title":"<code>rollout_id</code>  <code>instance-attribute</code>","text":"<p>The rollout which this span belongs to.</p>"},{"location":"reference/types/#agentlightning.Span.sequence_id","title":"<code>sequence_id</code>  <code>instance-attribute</code>","text":"<p>The ID to make spans ordered within a single attempt.</p>"},{"location":"reference/types/#agentlightning.Span.span_id","title":"<code>span_id</code>  <code>instance-attribute</code>","text":"<p>The span ID of the span. This ID comes from the OpenTelemetry span ID generator.</p>"},{"location":"reference/types/#agentlightning.Span.start_time","title":"<code>start_time</code>  <code>instance-attribute</code>","text":"<p>The start time of the span. See OpenTelemetry docs.</p>"},{"location":"reference/types/#agentlightning.Span.status","title":"<code>status</code>  <code>instance-attribute</code>","text":"<p>The status of the span. See OpenTelemetry docs.</p>"},{"location":"reference/types/#agentlightning.Span.trace_id","title":"<code>trace_id</code>  <code>instance-attribute</code>","text":"<p>The trace ID of the span. One rollout/attempt can have multiple traces. This ID comes from the OpenTelemetry trace ID generator.</p>"},{"location":"reference/types/#agentlightning.Span.from_attributes","title":"<code>from_attributes(*, attributes, rollout_id=None, attempt_id=None, sequence_id=None, name=None, trace_id=None, span_id=None, parent_id=None, start_time=None, end_time=None, resource=None, status=None)</code>  <code>classmethod</code>","text":"<p>Build a synthetic span from raw attributes. Different from the <code>from_opentelemetry</code> method, all parameters other than <code>attributes</code> are optional and will be generated if not provided.</p> <p>Parameters:</p> <ul> <li> <code>attributes</code>               (<code>Attributes</code>)           \u2013            <p>Span attributes to persist.</p> </li> <li> <code>rollout_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional rollout identifier associated with the span.</p> </li> <li> <code>attempt_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional attempt identifier associated with the span.</p> </li> <li> <code>sequence_id</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Optional sequence number to preserve ordering.</p> </li> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional human-readable span name.</p> </li> <li> <code>trace_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Custom trace identifier. When omitted, a random identifier is generated.</p> </li> <li> <code>span_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Custom span identifier. When omitted, a random identifier is generated.</p> </li> <li> <code>parent_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional parent span identifier.</p> </li> <li> <code>start_time</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Span start timestamp in seconds.</p> </li> <li> <code>end_time</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Span end timestamp in seconds.</p> </li> <li> <code>resource</code>               (<code>Optional[OtelResource]</code>, default:                   <code>None</code> )           \u2013            <p>Explicit resource information to attach to the span.</p> </li> <li> <code>status</code>               (<code>Optional[TraceStatus]</code>, default:                   <code>None</code> )           \u2013            <p>Optional status of the span.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'Span'</code>           \u2013            <p><code>Span</code> populated with the provided attributes.</p> </li> </ul>"},{"location":"reference/types/#agentlightning.Span.from_core_fields","title":"<code>from_core_fields(core, *, rollout_id=None, attempt_id=None, sequence_id=None)</code>  <code>classmethod</code>","text":"<p>Build a span from a core span.</p> <p>Parameters:</p> <ul> <li> <code>core</code>               (<code>SpanCoreFields</code>)           \u2013            <p>Core span to build from.</p> </li> <li> <code>rollout_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional rollout identifier associated with the span.</p> </li> <li> <code>attempt_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional attempt identifier associated with the span.</p> </li> <li> <code>sequence_id</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Optional sequence number to preserve ordering.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Span</code>           \u2013            <p><code>Span</code> populated with the provided attributes.</p> </li> </ul>"},{"location":"reference/types/#agentlightning.Span.from_opentelemetry","title":"<code>from_opentelemetry(src, rollout_id, attempt_id, sequence_id)</code>  <code>classmethod</code>","text":"<p>Convert an OpenTelemetry span into the Agent Lightning data model.</p> <p>Parameters:</p> <ul> <li> <code>src</code>               (<code>ReadableSpan</code>)           \u2013            <p>Span captured by OpenTelemetry.</p> </li> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier for the rollout that produced the span.</p> </li> <li> <code>attempt_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the attempt within the rollout.</p> </li> <li> <code>sequence_id</code>               (<code>int</code>)           \u2013            <p>Monotonically increasing identifier assigned to the span.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'Span'</code>           \u2013            <p>Parsed <code>Span</code> instance suitable for persistence.</p> </li> </ul>"},{"location":"reference/types/#agentlightning.SpanAttributeNames","title":"<code>agentlightning.SpanAttributeNames</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Canonical attribute names written by Agent Lightning emitters. Deprecated in favor of semconv.</p>"},{"location":"reference/types/#agentlightning.SpanAttributeNames.MESSAGE","title":"<code>MESSAGE = 'message'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the message attribute.</p>"},{"location":"reference/types/#agentlightning.SpanAttributeNames.OBJECT","title":"<code>OBJECT = 'object'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the object attribute.</p>"},{"location":"reference/types/#agentlightning.SpanLike","title":"<code>agentlightning.SpanLike = Union[ReadableSpan, Span]</code>  <code>module-attribute</code>","text":"<p>Union type of OpenTelemetry <code>ReadableSpan</code> and Agent-lightning <code>Span</code>.</p>"},{"location":"reference/types/#agentlightning.SpanCoreFields","title":"<code>agentlightning.SpanCoreFields</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Core fields of a span. Used by span creators who don't care about the full span model.</p> <p>If the spans are managed by some OTel tracer provider, it's not advised to create spans via this path.</p>"},{"location":"reference/types/#agentlightning.SpanCoreFields.attributes","title":"<code>attributes</code>  <code>instance-attribute</code>","text":"<p>The attributes of the span.</p>"},{"location":"reference/types/#agentlightning.SpanCoreFields.end_time","title":"<code>end_time</code>  <code>instance-attribute</code>","text":"<p>The end time of the span.</p>"},{"location":"reference/types/#agentlightning.SpanCoreFields.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>The name of the span.</p>"},{"location":"reference/types/#agentlightning.SpanCoreFields.start_time","title":"<code>start_time</code>  <code>instance-attribute</code>","text":"<p>The start time of the span.</p>"},{"location":"reference/types/#agentlightning.SpanCoreFields.status","title":"<code>status</code>  <code>instance-attribute</code>","text":"<p>The status of the span.</p>"},{"location":"reference/types/#agentlightning.SpanRecordingContext","title":"<code>agentlightning.SpanRecordingContext</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Context for recording operations on a span. It doesn't have to finalize the span; the caller will do it.</p>"},{"location":"reference/types/#agentlightning.SpanRecordingContext.get_recorded_span","title":"<code>get_recorded_span()</code>","text":"<p>Get the recording of the span.</p>"},{"location":"reference/types/#agentlightning.SpanRecordingContext.record_attributes","title":"<code>record_attributes(attributes)</code>","text":"<p>Record attributes on the span.</p>"},{"location":"reference/types/#agentlightning.SpanRecordingContext.record_exception","title":"<code>record_exception(exception)</code>","text":"<p>Record an exception on the span.</p>"},{"location":"reference/types/#agentlightning.SpanRecordingContext.record_status","title":"<code>record_status(status_code, description=None)</code>","text":"<p>Record the status of the span.</p>"},{"location":"reference/types/#environment-variables","title":"Environment Variables","text":""},{"location":"reference/types/#agentlightning.LightningEnvVar","title":"<code>agentlightning.LightningEnvVar</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Environment variables for Agent Lightning.</p>"},{"location":"reference/types/#agentlightning.LightningEnvVar.AGL_CURRENT_ROLE","title":"<code>AGL_CURRENT_ROLE = 'AGL_CURRENT_ROLE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Which side(s) to run in this process. Used in <code>ClientServerExecutionStrategy</code>.</p>"},{"location":"reference/types/#agentlightning.LightningEnvVar.AGL_EMITTER_DEBUG","title":"<code>AGL_EMITTER_DEBUG = 'AGL_EMITTER_DEBUG'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Enable debug logging for the emitter.</p>"},{"location":"reference/types/#agentlightning.LightningEnvVar.AGL_MANAGED_STORE","title":"<code>AGL_MANAGED_STORE = 'AGL_MANAGED_STORE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If yes, the <code>ExecutionStrategy</code> constructs LightningStore wrappers automatically. When <code>False</code> the provided <code>store</code> is passed directly to the bundles, allowing callers to manage store wrappers manually.</p>"},{"location":"reference/types/#agentlightning.LightningEnvVar.AGL_SERVER_HOST","title":"<code>AGL_SERVER_HOST = 'AGL_SERVER_HOST'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Interface the <code>LightningStoreServer</code> binds to when running the algorithm bundle locally.</p>"},{"location":"reference/types/#agentlightning.LightningEnvVar.AGL_SERVER_PORT","title":"<code>AGL_SERVER_PORT = 'AGL_SERVER_PORT'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Port the <code>LightningStoreServer</code> listens to.</p>"},{"location":"reference/types/#agentlightning.resolve_bool_env_var","title":"<code>agentlightning.resolve_bool_env_var(env_var, override=None, fallback=None)</code>","text":"<pre><code>resolve_bool_env_var(\n    env_var: LightningEnvVar, override: bool, fallback: bool\n) -&gt; bool\n</code></pre><pre><code>resolve_bool_env_var(\n    env_var: LightningEnvVar, *, fallback: bool\n) -&gt; bool\n</code></pre><pre><code>resolve_bool_env_var(\n    env_var: LightningEnvVar,\n    override: bool | None = None,\n    fallback: bool | None = None,\n) -&gt; bool | None\n</code></pre> <p>Resolve a boolean environment variable.</p> <p>Parameters:</p> <ul> <li> <code>env_var</code>               (<code>LightningEnvVar</code>)           \u2013            <p>The environment variable to resolve.</p> </li> <li> <code>override</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional override supplied by the caller.</p> </li> <li> <code>fallback</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>Default value if the environment variable is not set.</p> </li> </ul>"},{"location":"reference/types/#agentlightning.resolve_int_env_var","title":"<code>agentlightning.resolve_int_env_var(env_var, override=None, fallback=None)</code>","text":"<pre><code>resolve_int_env_var(\n    env_var: LightningEnvVar, override: int, fallback: int\n) -&gt; int\n</code></pre><pre><code>resolve_int_env_var(\n    env_var: LightningEnvVar, *, fallback: int\n) -&gt; int\n</code></pre><pre><code>resolve_int_env_var(\n    env_var: LightningEnvVar,\n    override: int | None = None,\n    fallback: int | None = None,\n) -&gt; int | None\n</code></pre> <p>Resolve an integer environment variable.</p> <p>Parameters:</p> <ul> <li> <code>env_var</code>               (<code>LightningEnvVar</code>)           \u2013            <p>The environment variable to resolve.</p> </li> <li> <code>override</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional override supplied by the caller.</p> </li> <li> <code>fallback</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Default value if the environment variable is not set.</p> </li> </ul>"},{"location":"reference/types/#agentlightning.resolve_str_env_var","title":"<code>agentlightning.resolve_str_env_var(env_var, override=None, fallback=None)</code>","text":"<pre><code>resolve_str_env_var(\n    env_var: LightningEnvVar, override: str, fallback: str\n) -&gt; str\n</code></pre><pre><code>resolve_str_env_var(\n    env_var: LightningEnvVar, *, fallback: str\n) -&gt; str\n</code></pre><pre><code>resolve_str_env_var(\n    env_var: LightningEnvVar,\n    override: str | None = None,\n    fallback: str | None = None,\n) -&gt; str | None\n</code></pre> <p>Resolve a string environment variable.</p> <p>Parameters:</p> <ul> <li> <code>env_var</code>               (<code>LightningEnvVar</code>)           \u2013            <p>The environment variable to resolve.</p> </li> <li> <code>override</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional override supplied by the caller.</p> </li> <li> <code>fallback</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Default value if the environment variable is not set.</p> </li> </ul>"},{"location":"reference/utilities/","title":"Utility References","text":""},{"location":"reference/utilities/#id","title":"ID","text":""},{"location":"reference/utilities/#agentlightning.utils.id.generate_id","title":"<code>agentlightning.utils.id.generate_id(length)</code>","text":"<p>Generate a random ID of the given length.</p> <p>Parameters:</p> <ul> <li> <code>length</code>               (<code>int</code>)           \u2013            <p>The length of the ID to generate.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>A random ID of the given length.</p> </li> </ul>"},{"location":"reference/utilities/#metrics","title":"Metrics","text":""},{"location":"reference/utilities/#agentlightning.utils.metrics.MetricsBackend","title":"<code>agentlightning.utils.metrics.MetricsBackend</code>","text":"<p>Abstract base class for metrics backends.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.MetricsBackend.has_prometheus","title":"<code>has_prometheus()</code>","text":"<p>Check if the backend has prometheus support.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.MetricsBackend.inc_counter","title":"<code>inc_counter(name, amount=1.0, labels=None)</code>  <code>async</code>","text":"<p>Increments a registered counter.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Metric name (must be registered as a counter).</p> </li> <li> <code>amount</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Increment amount.</p> </li> <li> <code>labels</code>               (<code>Optional[LabelDict]</code>, default:                   <code>None</code> )           \u2013            <p>Label values.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the metric is not registered, has the wrong type, or label keys do not match the registered label names.</p> </li> </ul>"},{"location":"reference/utilities/#agentlightning.utils.metrics.MetricsBackend.observe_histogram","title":"<code>observe_histogram(name, value, labels=None)</code>  <code>async</code>","text":"<p>Records an observation for a registered histogram.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Metric name (must be registered as a histogram).</p> </li> <li> <code>value</code>               (<code>float</code>)           \u2013            <p>Observed value.</p> </li> <li> <code>labels</code>               (<code>Optional[LabelDict]</code>, default:                   <code>None</code> )           \u2013            <p>Label values.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the metric is not registered, has the wrong type, or label keys do not match the registered label names.</p> </li> </ul>"},{"location":"reference/utilities/#agentlightning.utils.metrics.MetricsBackend.register_counter","title":"<code>register_counter(name, label_names=None, group_level=None)</code>","text":"<p>Registers a counter metric.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Metric name.</p> </li> <li> <code>label_names</code>               (<code>Optional[Sequence[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of label names. Order determines the truncation priority for group-level logging.</p> </li> <li> <code>group_level</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Optional per-metric grouping depth for backends that support label grouping (Console). Global backend settings take precedence when provided.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the metric is already registered with a different type or label set.</p> </li> </ul>"},{"location":"reference/utilities/#agentlightning.utils.metrics.MetricsBackend.register_histogram","title":"<code>register_histogram(name, label_names=None, buckets=None, group_level=None)</code>","text":"<p>Registers a histogram metric.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Metric name.</p> </li> <li> <code>label_names</code>               (<code>Optional[Sequence[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of label names. Order determines the truncation priority for group-level logging.</p> </li> <li> <code>buckets</code>               (<code>Optional[Sequence[float]]</code>, default:                   <code>None</code> )           \u2013            <p>Bucket boundaries (exclusive upper bounds). If None, the backend may choose defaults.</p> </li> <li> <code>group_level</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Optional per-metric grouping depth for backends that support label grouping (Console). Global backend settings take precedence when provided.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the metric is already registered with a different type or label set.</p> </li> </ul>"},{"location":"reference/utilities/#agentlightning.utils.metrics.ConsoleMetricsBackend","title":"<code>agentlightning.utils.metrics.ConsoleMetricsBackend</code>","text":"<p>               Bases: <code>MetricsBackend</code></p> <p>Console backend with sliding-window aggregations and label grouping.</p> <p>This backend:</p> <ul> <li>Requires explicit metric registration.</li> <li>Stores timestamped events per (metric_name, labels) key.</li> <li>Computes rate and percentiles (P50, P95, P99) over a sliding time window.</li> <li>Uses a single global logging decision: when logging is triggered, it   logs all metric groups, not just the one being updated.</li> </ul> <p>Rate is always per second.</p> <p>Label grouping: When logging, label dictionaries are truncated to the first <code>group_level</code> label pairs (following the registered label order) and metrics with identical truncated labels are aggregated together. For example:</p> <pre><code>labels = {\"method\": \"GET\", \"path\": \"/\", \"status\": \"200\"}\ngroup_level = 2  # aggregated labels {\"method\": \"GET\", \"path\": \"/\"}\n</code></pre> <p>If <code>group_level</code> is None or &lt; 1, all label combinations for a metric are merged into a single log entry (equivalent to grouping by zero labels). Individual counters or histograms can set their own <code>group_level</code> during registration; those values apply only when the backend-level <code>group_level</code> is unset, allowing selective overrides.</p> <p>Thread-safety: Runtime updates and snapshotting use two aiologic locks: one for mutating shared state and another that serializes the global logging decision/snapshot capture so other tasks can continue writing. Metric registration happens during initialization, so it is intentionally left lock-free; this assumption is documented here to avoid blocking writes unnecessarily.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.ConsoleMetricsBackend.__init__","title":"<code>__init__(window_seconds=60.0, log_interval_seconds=10.0, group_level=None)</code>","text":"<p>Initializes ConsoleMetricsBackend.</p> <p>Parameters:</p> <ul> <li> <code>window_seconds</code>               (<code>Optional[float]</code>, default:                   <code>60.0</code> )           \u2013            <p>Sliding window size (in seconds) used when computing rate and percentiles. If None, all in-memory events are used.</p> </li> <li> <code>log_interval_seconds</code>               (<code>float</code>, default:                   <code>10.0</code> )           \u2013            <p>Minimum time (in seconds) between log bursts. When the interval elapses, the next metric event triggers a snapshot and logging of all metrics.</p> </li> <li> <code>group_level</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Label grouping depth. When logging, only the first <code>group_level</code> labels (following registered order) are retained and metric events sharing those labels are aggregated. If None or &lt; 1, all label combinations collapse into a single group per metric.</p> </li> </ul>"},{"location":"reference/utilities/#agentlightning.utils.metrics.ConsoleMetricsBackend.inc_counter","title":"<code>inc_counter(name, amount=1.0, labels=None)</code>  <code>async</code>","text":"<p>Increments a registered counter metric.</p> <p>See base class for behavior and error conditions.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.ConsoleMetricsBackend.observe_histogram","title":"<code>observe_histogram(name, value, labels=None)</code>  <code>async</code>","text":"<p>Records an observation for a registered histogram metric.</p> <p>See base class for behavior and error conditions.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.ConsoleMetricsBackend.register_counter","title":"<code>register_counter(name, label_names=None, group_level=None)</code>","text":"<p>Registers a counter metric.</p> <p>See base class for argument documentation.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.ConsoleMetricsBackend.register_histogram","title":"<code>register_histogram(name, label_names=None, buckets=None, group_level=None)</code>","text":"<p>Registers a histogram metric.</p> <p>See base class for argument documentation.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.PrometheusMetricsBackend","title":"<code>agentlightning.utils.metrics.PrometheusMetricsBackend</code>","text":"<p>               Bases: <code>MetricsBackend</code></p> <p>Metrics backend that forwards events to prometheus_client.</p> <p>All metrics must be registered before use. This backend does not compute any aggregations; it only updates Prometheus metrics.</p> <p>Thread-safety: Registration is protected by a lock. Metric updates assume metrics are registered during initialization and then remain stable.</p> <p>Due to the nature of Prometheus, this backend is only suitable for recording high-volume metrics. Low-volume metrics might be lost if the event has only appeared once.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.PrometheusMetricsBackend.__init__","title":"<code>__init__()</code>","text":"<p>Initializes PrometheusMetricsBackend.</p> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If prometheus_client is not installed.</p> </li> </ul>"},{"location":"reference/utilities/#agentlightning.utils.metrics.PrometheusMetricsBackend.has_prometheus","title":"<code>has_prometheus()</code>","text":"<p>Check if the backend has prometheus support.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.PrometheusMetricsBackend.inc_counter","title":"<code>inc_counter(name, amount=1.0, labels=None)</code>  <code>async</code>","text":"<p>Increments a registered Prometheus counter.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.PrometheusMetricsBackend.observe_histogram","title":"<code>observe_histogram(name, value, labels=None)</code>  <code>async</code>","text":"<p>Records an observation for a registered Prometheus histogram.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.PrometheusMetricsBackend.register_counter","title":"<code>register_counter(name, label_names=None, group_level=None)</code>","text":"<p>Registers a Prometheus counter metric.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.PrometheusMetricsBackend.register_histogram","title":"<code>register_histogram(name, label_names=None, buckets=None, group_level=None)</code>","text":"<p>Registers a Prometheus histogram metric.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.MultiMetricsBackend","title":"<code>agentlightning.utils.metrics.MultiMetricsBackend</code>","text":"<p>               Bases: <code>MetricsBackend</code></p> <p>Metrics backend that forwards calls to multiple underlying backends.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.MultiMetricsBackend.__init__","title":"<code>__init__(backends)</code>","text":"<p>Initializes MultiMetricsBackend.</p> <p>Parameters:</p> <ul> <li> <code>backends</code>               (<code>Sequence[MetricsBackend]</code>)           \u2013            <p>Sequence of underlying backends.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no backends are provided.</p> </li> </ul>"},{"location":"reference/utilities/#agentlightning.utils.metrics.MultiMetricsBackend.has_prometheus","title":"<code>has_prometheus()</code>","text":"<p>Check if the backend has prometheus support.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.MultiMetricsBackend.inc_counter","title":"<code>inc_counter(name, amount=1.0, labels=None)</code>  <code>async</code>","text":"<p>Increments a counter metric in all underlying backends.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.MultiMetricsBackend.observe_histogram","title":"<code>observe_histogram(name, value, labels=None)</code>  <code>async</code>","text":"<p>Records a histogram observation in all underlying backends.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.MultiMetricsBackend.register_counter","title":"<code>register_counter(name, label_names=None, group_level=None)</code>","text":"<p>Registers a counter metric in all underlying backends.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.MultiMetricsBackend.register_histogram","title":"<code>register_histogram(name, label_names=None, buckets=None, group_level=None)</code>","text":"<p>Registers a histogram metric in all underlying backends.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.setup_multiprocess_prometheus","title":"<code>agentlightning.utils.metrics.setup_multiprocess_prometheus()</code>","text":"<p>Set up prometheus multiprocessing directory if not already configured.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.get_prometheus_registry","title":"<code>agentlightning.utils.metrics.get_prometheus_registry()</code>","text":"<p>Get the appropriate prometheus registry based on multiprocessing configuration.</p>"},{"location":"reference/utilities/#agentlightning.utils.metrics.shutdown_metrics","title":"<code>agentlightning.utils.metrics.shutdown_metrics(server=None, worker=None, *args, **kwargs)</code>","text":"<p>Shutdown prometheus metrics.</p>"},{"location":"reference/utilities/#server-launcher","title":"Server Launcher","text":""},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncher","title":"<code>agentlightning.utils.server_launcher.PythonServerLauncher</code>","text":"<p>Unified launcher for FastAPI, using uvicorn or gunicorn per mode/worker count.</p> <p>See <code>PythonServerLauncherArgs</code> for configuration options.</p> <p>Parameters:</p> <ul> <li> <code>app</code>               (<code>FastAPI</code>)           \u2013            <p>The FastAPI app to launch.</p> </li> <li> <code>args</code>               (<code>PythonServerLauncherArgs</code>)           \u2013            <p>The configuration for the server.</p> </li> <li> <code>serve_context</code>               (<code>Optional[AsyncContextManager[Any]]</code>, default:                   <code>None</code> )           \u2013            <p>An optional context manager to apply around the server startup.</p> </li> </ul>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncher.access_endpoint","title":"<code>access_endpoint</code>  <code>property</code>","text":"<p>Return a loopback-friendly URL so health checks succeed even when binding to 0.0.0.0.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncher.endpoint","title":"<code>endpoint</code>  <code>property</code>","text":"<p>Return the externally advertised host:port pair regardless of accessibility.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncher.health_url","title":"<code>health_url</code>  <code>property</code>","text":"<p>Build the absolute health-check endpoint from args, if one is configured.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncher.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Control pickling to prevent server state from being sent to subprocesses.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncher.__init__","title":"<code>__init__(app, args, serve_context=None)</code>","text":"<p>Initialize the launcher with the FastAPI app, configuration, and optional serve context.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncher.is_running","title":"<code>is_running()</code>","text":"<p>Return True if the server has been started and not yet stopped.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncher.reload","title":"<code>reload()</code>  <code>async</code>","text":"<p>Restart the server by stopping it if necessary and invoking start again.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncher.run_forever","title":"<code>run_forever()</code>  <code>async</code>","text":"<p>Start the server and block the caller until it exits, respecting the configured mode.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncher.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Starts the server according to launch_mode and n_workers.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncher.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop the server using the inverse of whatever launch mode was used to start it.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncherArgs","title":"<code>agentlightning.utils.server_launcher.PythonServerLauncherArgs</code>  <code>dataclass</code>","text":""},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncherArgs.access_host","title":"<code>access_host = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The hostname or IP address to advertise to the client. If not provided, the server will use the default outbound IPv4 address for this machine.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncherArgs.access_log","title":"<code>access_log = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to turn on access logs.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncherArgs.healthcheck_url","title":"<code>healthcheck_url = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The health check URL to use. If not provided, the server will not be checked for healthiness after starting.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncherArgs.host","title":"<code>host = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The hostname or IP address to bind the server to.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncherArgs.kill_unhealthy_server","title":"<code>kill_unhealthy_server = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to kill the server if it is not healthy after startup. This setting is ignored when <code>launch_mode</code> is not <code>asyncio</code>.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncherArgs.launch_mode","title":"<code>launch_mode = 'asyncio'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The launch mode. <code>asyncio</code> is the default mode to runs the server in the current thread. <code>thread</code> runs the server in a separate thread. <code>mp</code> runs the server in a separate process.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncherArgs.log_level","title":"<code>log_level = logging.INFO</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The log level to use.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncherArgs.n_workers","title":"<code>n_workers = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The number of workers to run in the server. Only applicable for <code>mp</code> mode. When <code>n_workers &gt; 1</code>, the server will be run using Gunicorn.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncherArgs.port","title":"<code>port = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The TCP port to listen on. If not provided, the server will use a random available port.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncherArgs.process_join_timeout","title":"<code>process_join_timeout = 10.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The timeout to wait for the process to join.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncherArgs.startup_timeout","title":"<code>startup_timeout = 60.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The timeout to wait for the server to start up.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncherArgs.thread_join_timeout","title":"<code>thread_join_timeout = 10.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The timeout to wait for the thread to join.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.PythonServerLauncherArgs.timeout_keep_alive","title":"<code>timeout_keep_alive = 30</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The timeout to keep the connection alive.</p>"},{"location":"reference/utilities/#agentlightning.utils.server_launcher.LaunchMode","title":"<code>agentlightning.utils.server_launcher.LaunchMode = Literal['asyncio', 'thread', 'mp']</code>  <code>module-attribute</code>","text":"<p>The launch mode for the server.</p>"},{"location":"reference/utilities/#opentelemetry","title":"OpenTelemetry","text":""},{"location":"reference/utilities/#agentlightning.utils.otel.full_qualified_name","title":"<code>agentlightning.utils.otel.full_qualified_name(obj)</code>","text":""},{"location":"reference/utilities/#agentlightning.utils.otel.get_tracer_provider","title":"<code>agentlightning.utils.otel.get_tracer_provider(inspect=True)</code>","text":"<p>Get the OpenTelemetry tracer provider configured for Agent Lightning.</p> <p>Parameters:</p> <ul> <li> <code>inspect</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inspect the tracer provider and log its configuration. When it's on, make sure you also set the logger level to DEBUG to see the logs.</p> </li> </ul>"},{"location":"reference/utilities/#agentlightning.utils.otel.get_tracer","title":"<code>agentlightning.utils.otel.get_tracer(use_active_span_processor=True)</code>","text":"<p>Resolve the OpenTelemetry tracer configured for Agent Lightning.</p> <p>Parameters:</p> <ul> <li> <code>use_active_span_processor</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the active span processor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tracer</code>           \u2013            <p>OpenTelemetry tracer tagged with the <code>agentlightning</code> instrumentation name.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If OpenTelemetry was not initialized before calling this helper.</p> </li> </ul>"},{"location":"reference/utilities/#agentlightning.utils.otel.make_tag_attributes","title":"<code>agentlightning.utils.otel.make_tag_attributes(tags)</code>","text":"<p>Convert a list of tags into flattened attributes for span tagging.</p> <p>There is no syntax enforced for tags, they are just strings. For example:</p> <pre><code>[\"gen_ai.model:gpt-4\", \"reward.extrinsic\"]\n</code></pre>"},{"location":"reference/utilities/#agentlightning.utils.otel.extract_tags_from_attributes","title":"<code>agentlightning.utils.otel.extract_tags_from_attributes(attributes)</code>","text":"<p>Extract tag attributes from flattened span attributes.</p> <p>Parameters:</p> <ul> <li> <code>attributes</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>A dictionary of flattened span attributes.</p> </li> </ul>"},{"location":"reference/utilities/#agentlightning.utils.otel.make_link_attributes","title":"<code>agentlightning.utils.otel.make_link_attributes(links)</code>","text":"<p>Convert a dictionary of links into flattened attributes for span linking.</p> <p>Links example:</p> <pre><code>{\n    \"gen_ai.response.id\": \"response-123\",\n    \"span_id\": \"abcd-efgh-ijkl\",\n}\n</code></pre>"},{"location":"reference/utilities/#agentlightning.utils.otel.query_linked_spans","title":"<code>agentlightning.utils.otel.query_linked_spans(spans, links)</code>","text":"<p>Query spans that are linked by the given link attributes.</p> <p>Parameters:</p> <ul> <li> <code>spans</code>               (<code>Sequence[T_SpanLike]</code>)           \u2013            <p>A sequence of spans to search.</p> </li> <li> <code>links</code>               (<code>List[LinkPydanticModel]</code>)           \u2013            <p>A list of link attributes to match.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[T_SpanLike]</code>           \u2013            <p>A list of spans that match the given link attributes.</p> </li> </ul>"},{"location":"reference/utilities/#agentlightning.utils.otel.extract_links_from_attributes","title":"<code>agentlightning.utils.otel.extract_links_from_attributes(attributes)</code>","text":"<p>Extract link attributes from flattened span attributes.</p> <p>Parameters:</p> <ul> <li> <code>attributes</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>A dictionary of flattened span attributes.</p> </li> </ul>"},{"location":"reference/utilities/#agentlightning.utils.otel.filter_attributes","title":"<code>agentlightning.utils.otel.filter_attributes(attributes, prefix)</code>","text":"<p>Filter attributes that start with the given prefix.</p> <p>The attribute must start with <code>prefix.</code> or be exactly <code>prefix</code> to be included.</p> <p>Parameters:</p> <ul> <li> <code>attributes</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>A dictionary of span attributes.</p> </li> <li> <code>prefix</code>               (<code>str</code>)           \u2013            <p>The prefix to filter by.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>A dictionary of attributes that start with the given prefix.</p> </li> </ul>"},{"location":"reference/utilities/#agentlightning.utils.otel.filter_and_unflatten_attributes","title":"<code>agentlightning.utils.otel.filter_and_unflatten_attributes(attributes, prefix)</code>","text":"<p>Filter attributes that start with the given prefix and unflatten them. The prefix will be removed during unflattening.</p> <p>Parameters:</p> <ul> <li> <code>attributes</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>A dictionary of span attributes.</p> </li> <li> <code>prefix</code>               (<code>str</code>)           \u2013            <p>The prefix to filter by.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[Dict[str, Any], List[Any]]</code>           \u2013            <p>A nested dictionary or list of attributes that start with the given prefix.</p> </li> </ul>"},{"location":"reference/utilities/#agentlightning.utils.otel.flatten_attributes","title":"<code>agentlightning.utils.otel.flatten_attributes(nested_data, *, expand_leaf_lists=False)</code>","text":"<p>Flatten a nested dictionary or list into a flat dictionary with dotted keys.</p> <p>This function recursively traverses dictionaries and lists, producing a flat key-value mapping where nested paths are represented via dot-separated keys. Lists are indexed numerically.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; flatten_attributes({\"a\": {\"b\": 1, \"c\": [2, 3]}}, expand_leaf_lists=True)\n{\"a.b\": 1, \"a.c.0\": 2, \"a.c.1\": 3}\n</code></pre> <p>Parameters:</p> <ul> <li> <code>nested_data</code>               (<code>Union[Dict[str, Any], List[Any]]</code>)           \u2013            <p>A nested structure composed of dictionaries, lists, or primitive values.</p> </li> <li> <code>expand_leaf_lists</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to expand lists composed only of primitive values. When <code>False</code> (the default), lists of str/int/float/bool are treated as leaf values and stored without enumerating their indices.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>A flat dictionary mapping dotted-string paths to primitive values.</p> </li> </ul>"},{"location":"reference/utilities/#agentlightning.utils.otel.unflatten_attributes","title":"<code>agentlightning.utils.otel.unflatten_attributes(flat_data)</code>","text":"<p>Reconstruct a nested dictionary/list structure from a flat dictionary.</p> <p>Keys are dot-separated paths. Segments that are digit strings will only become list indices if all keys in that dict form a consecutive 0..n-1 range. Otherwise they remain dict keys.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; unflatten_attributes({\"a.b\": 1, \"a.c.0\": 2, \"a.c.1\": 3})\n{\"a\": {\"b\": 1, \"c\": [2, 3]}}\n</code></pre> <p>Parameters:</p> <ul> <li> <code>flat_data</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>A dictionary whose keys are dot-separated paths and whose values are primitive data elements.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[Dict[str, Any], List[Any]]</code>           \u2013            <p>A nested dictionary (and lists where appropriate) corresponding to</p> </li> <li> <code>Union[Dict[str, Any], List[Any]]</code>           \u2013            <p>the flattened structure.</p> </li> </ul>"},{"location":"reference/utilities/#agentlightning.utils.otel.sanitize_attribute_value","title":"<code>agentlightning.utils.otel.sanitize_attribute_value(object, force=True)</code>","text":"<p>Sanitize an attribute value to be a valid OpenTelemetry attribute value.</p>"},{"location":"reference/utilities/#agentlightning.utils.otel.sanitize_attributes","title":"<code>agentlightning.utils.otel.sanitize_attributes(attributes, force=True)</code>","text":"<p>Sanitize a dictionary of attributes to be a valid OpenTelemetry attributes.</p> <p>Parameters:</p> <ul> <li> <code>attributes</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>A dictionary of attributes to sanitize.</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to force sanitization even when the value is not JSON serializable.</p> </li> </ul>"},{"location":"reference/utilities/#agentlightning.utils.otel.sanitize_list_attribute_sanity","title":"<code>agentlightning.utils.otel.sanitize_list_attribute_sanity(maybe_list)</code>","text":"<p>Try to sanitize a list of attributes to be a valid OpenTelemetry attribute value.</p> <p>Raise error if the list contains multiple types of primitive values.</p>"},{"location":"reference/utilities/#agentlightning.utils.otel.check_attributes_sanity","title":"<code>agentlightning.utils.otel.check_attributes_sanity(attributes)</code>","text":"<p>Check if a dictionary of attributes is a valid OpenTelemetry attributes.</p>"},{"location":"reference/utilities/#agentlightning.utils.otel.format_exception_attributes","title":"<code>agentlightning.utils.otel.format_exception_attributes(exception)</code>","text":"<p>Format an exception into a dictionary of attributes.</p>"},{"location":"reference/utilities/#otlp","title":"OTLP","text":""},{"location":"reference/utilities/#agentlightning.utils.otlp.handle_otlp_export","title":"<code>agentlightning.utils.otlp.handle_otlp_export(request, request_message_cls, response_message_cls, message_callback, signal_name)</code>  <code>async</code>","text":"<p>Generic handler for /v1/traces, /v1/metrics, /v1/logs.</p> <p>Convert the OTLP Protobuf request to a JSON-like object.</p>"},{"location":"reference/utilities/#agentlightning.utils.otlp.spans_from_proto","title":"<code>agentlightning.utils.otlp.spans_from_proto(request, sequence_id_bulk_issuer)</code>  <code>async</code>","text":"<p>Parse an OTLP proto payload into List[Span].</p> <p>A store is needed here for generating a sequence ID for each span.</p>"},{"location":"reference/utilities/#system-snapshot","title":"System Snapshot","text":""},{"location":"reference/utilities/#agentlightning.utils.system_snapshot.system_snapshot","title":"<code>agentlightning.utils.system_snapshot.system_snapshot(include_gpu=False)</code>","text":"<p>Capture a snapshot of the system's hardware and software information.</p> <p>Parameters:</p> <ul> <li> <code>include_gpu</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to include GPU information.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>A dictionary containing the system's hardware and software information.</p> </li> </ul>"},{"location":"tutorials/debug/","title":"Debugging and Troubleshooting","text":"<p>When you train your own agent with Agent-lightning, most failures surface because the agent logic is brittle or simply incorrect. Debugging becomes easier when you peel back the stack: start by driving the rollout logic on its own, dry-run the trainer loop, and only then bring the full algorithm and runner topology online. The <code>examples/apo/apo_debug.py</code> script demonstrates these techniques; this guide expands on each approach and helps you decide when to reach for them.</p>"},{"location":"tutorials/debug/#debugging-with-dashboard","title":"Debugging with Dashboard","text":"<p>When you launch an experiment with <code>Trainer.fit</code> or start an isolated store via <code>agl store</code>, the terminal prints a message similar to:</p> <pre><code>INFO     Agent-lightning dashboard will be available at http://192.168.0.107:4747\n</code></pre> <p>Visit that URL, and you will see the Agent-lightning dashboard:</p> <p></p> <p>The dashboard surfaces everything stored inside the store. Because the store mediates interactions between algorithms and runners, inspecting it often reveals which side is causing issues such as stale rollouts, unresponsive workers, or empty traces.</p> <p>For example, the VERL algorithm may receive no token IDs and emit <code>cannot reshape tensor of 0 elements into shape [1, 0, -1, 128] because the unspecified dimension size -1 can be any value and is ambiguous</code> (Issue #50, Issue #76). Several scenarios can produce that error: the runner might not produce trace spans at all, it might produce spans without token IDs, or the IDs may be present but formatted incorrectly. Inspecting the dashboard traces helps you pinpoint which condition applies.</p> <p></p> <p>By checking whether the trace span is empty and whether token IDs appear in the span attributes, you can narrow the issue to either the runner (agent) side or the algorithm side. Then apply the techniques below to debug the faulty component.</p>"},{"location":"tutorials/debug/#debug-level-logging","title":"Debug-level Logging","text":"<p>Starting from v0.3, detailed signals such as store server access logs, runner lifecycle logs, and span payloads only appear when the log level is <code>DEBUG</code> so the default output stays readable. Enable debug-level logging by adding the following snippet near the top of your script:</p> <pre><code>import agentlightning as agl\n\nagl.setup_logging(\"DEBUG\")\n</code></pre> <p>Set the log level on every process if your setup involves multiple workers. For example, when running stores in isolation, configure the store process explicitly:</p> <pre><code>agl store --port 4747 --log-level DEBUG\n</code></pre>"},{"location":"tutorials/debug/#using-runner-in-isolation","title":"Using <code>Runner</code> in Isolation","text":"<p><code>Runner</code> is a long-lived worker that wraps your <code>LitAgent</code>, coordinates tracing, and talks to the <code>LightningStore</code>. In typical training flows the trainer manages runners for you, but being able to spin one up manually is invaluable while debugging.</p> <p>If you define rollout logic with <code>@rollout</code> or implement a <code>LitAgent</code> directly, you will get a <code>LitAgent</code> instance and you should be able to execute it with <code>LitAgentRunner</code>, which is a subclass of <code>Runner</code>. The runner needs but does not instantiate a <code>Tracer</code>, so supply one yourself. See Working with Traces for a walkthrough of tracer options.</p> <p><code>Runner.run_context</code> prepares the runner to execute a particular agent. Besides the agent and tracer you must provide a store that will collect spans and rollouts. <code>InMemoryLightningStore</code> keeps everything in-process, which is perfect for debugging sessions.</p> <pre><code>import agentlightning as agl\n\ntracer = agl.OtelTracer()\nrunner = agl.LitAgentRunner(tracer)\nstore = agl.InMemoryLightningStore()\n\nwith runner.run_context(agent=apo_rollout, store=store):\n    ...\n</code></pre> <p>Inside the <code>run_context</code> block you can call <code>runner.step(...)</code> to execute a single rollout. The payload includes the task input and any <code>NamedResources</code> the agent expects. Read introduction to Resources and NamedResources for more details. For example, if your agent references a <code>PromptTemplate</code>, pass it through the <code>resources</code> argument:</p> <pre><code>with runner.run_context(agent=apo_rollout, store=store):\n    resource = agl.PromptTemplate(template=\"You are a helpful assistant. {any_question}\", engine=\"f-string\")\n    rollout = await runner.step(\n        \"Explain why the sky appears blue using principles of light scattering in 100 words.\",\n        resources={\"main_prompt\": resource},\n    )\n</code></pre> <p>You can do as many things as you want within the <code>Runner.run_context</code> block. After the rollout finishes you can query the store to inspect what happened:</p> <pre><code>print(await store.query_rollouts())\nprint(await store.query_spans(rollout.rollout_id))\n</code></pre> <p>Example output (with a reward span captured):</p> <pre><code>[Rollout(rollout_id='ro-519769241af8', input='Explain why the sky appears blue using principles of light scattering in 100 words.', start_time=1760706315.6996238, ..., status='succeeded')]\n[Span(rollout_id='ro-519769241af8', attempt_id='at-a6b62caf', sequence_id=1, ..., name='agentlightning.annotation', attributes={'agentlightning.reward.0.value': 0.95}, ...)]\n</code></pre> <p>Swap in an <code>AgentOpsTracer</code> instead of <code>OtelTracer</code> to see the underlying LLM spans alongside reward information:</p> <pre><code>[\n    Span(rollout_id='ro-519769241af8', attempt_id='at-a6b62caf', sequence_id=1, ..., name='openai.chat.completion', attributes={..., 'gen_ai.prompt.0.role': 'user', 'gen_ai.prompt.0.content': 'You are a helpful assistant. Explain why the sky appears blue using principles of light scattering in 100 words.', ...}),\n    Span(rollout_id='ro-519769241af8', attempt_id='at-a6b62caf', sequence_id=2, ..., name='openai.chat.completion', attributes={..., 'gen_ai.prompt.0.role': 'user', 'gen_ai.prompt.0.content': 'Evaluate how well the output fulfills the task...', ...}),\n    Span(rollout_id='ro-519769241af8', attempt_id='at-a6b62caf', sequence_id=3, ..., name='agentlightning.annotation', attributes={'agentlightning.reward.0.value': 0.95}, ...)\n]\n</code></pre> <p>Tip</p> <p>Spans too difficult to read? Try using <code>Adapter</code> to convert them into a more readable format.</p> <p><code>Runner.step</code> executes a full rollout even though it is named \"step\". The companion method <code>Runner.iter</code> executes multiple \"steps\" by continuously pulling new rollout inputs from the store until a stop event is set. Use <code>iter</code> once you are confident the single-step path works and you have another worker <code>enqueue_rollout</code> to the store.</p> <p>Tip</p> <p>You can also call <code>Runner.step</code> to inject ad-hoc rollouts into a running store being used by another algorithm, so that the rollouts can be consumed by the algorithms. This is very recently known as the paradigm of \"online RL\". At the moment, no algorithm in the algorithm zoo consumes externally generated rollouts, but the data flow is available there if you need it.</p>"},{"location":"tutorials/debug/#debug-with-llm-proxy","title":"Debug with LLM Proxy","text":"<p>If you are dealing with LLM optimization like Reinforcement Learning, we generally recommend using an online stable LLM service for your debugging purposes, like <code>openai/gpt-4.1-nano</code>. After the debugging is done, you can switch to a local training endpoint.</p> <p>However, if you want to use a local LLM features like getting the token IDs, you can also manually start a local vLLM server by:</p> <pre><code>vllm serve Qwen/Qwen2.5-0.5B-Instruct --port 8080\n</code></pre> <p>Then start the LLM proxy via the following script:</p> <pre><code>import asyncio\nimport aiohttp\nimport agentlightning as agl\n\nasync def serve_llm_proxy():\n    store = agl.InMemoryLightningStore()\n    store_server = agl.LightningStoreServer(store, \"127.0.0.1\", 8081)\n    await store_server.start()\n\n    llm_proxy = agl.LLMProxy(\n        port=8082,\n        model_list=[\n            {\n                \"model_name\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n                \"litellm_params\": {\n                    \"model\": \"hosted_vllm/Qwen/Qwen2.5-0.5B-Instruct\",\n                    \"api_base\": \"http://localhost:8080/v1\",\n                },\n            }\n        ],\n        store=store_server,\n    )\n\n    await llm_proxy.start()\n    await asyncio.sleep(1000000)\n</code></pre> <p>Test the served LLM proxy with a client like:</p> <pre><code>async def test_llm_proxy():\n    async with aiohttp.ClientSession() as session:\n        async with session.post(\"http://localhost:8082/v1/chat/completions\", json={\n            \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Hello, world!\"}],\n        }) as response:\n            print(await response.json())\n</code></pre> <p>You can now use the LLM proxy by specifying environment variables:</p> <pre><code>export OPENAI_API_BASE=http://localhost:8081/v1\nexport OPENAI_API_KEY=dummy\n</code></pre> <p>You might see warnings about <code>Missing or invalid rollout_id, attempt_id, or sequence_id</code> in the LLM proxy logs. This is fine because you don't have a rollout and attempt yet when you are debugging. When you started the training, the algorithm will create the rollouts for you and the warnings will go away.</p>"},{"location":"tutorials/debug/#hook-into-runners-lifecycle","title":"Hook into Runner's Lifecycle","text":"<p><code>Runner.run_context</code> accepts a <code>hooks</code> argument so you can observe or augment lifecycle events without editing your agent. Hooks subclass <code>Hook</code> and can respond to four asynchronous callbacks: <code>on_trace_start</code>, <code>on_rollout_start</code>, <code>on_rollout_end</code>, and <code>on_trace_end</code>. This is useful for:</p> <ul> <li>Capturing raw OpenTelemetry spans before they hit the store and before the <code>LitAgentRunner</code> do postprocessing on the rollout</li> <li>Inspecting the tracer instance after they are activated</li> <li>Logging rollout inputs before they are processed by the agent</li> </ul> <p>The <code>hook</code> mode in <code>examples/apo/apo_debug.py</code> prints every span collected during a rollout:</p> <pre><code>import agentlightning as agl\n\n# ... Same as previous example\n\nclass DebugHook(agl.Hook):\n    async def on_trace_end(self, *, agent, runner, tracer, rollout):\n        trace = tracer.get_last_trace()\n        print(\"Trace spans collected during the rollout:\")\n        for span in trace:\n            print(f\"- {span.name} (status: {span.status}):\\n  {span.attributes}\")\n\nwith runner.run_context(\n    agent=apo_rollout,\n    store=store,\n    hooks=[DebugHook()],\n):\n    await runner.step(\n        \"Explain why the sky appears blue using principles of light scattering in 100 words.\",\n        resources={\"main_prompt\": resource},\n    )\n</code></pre> <p>Because hooks run inside the runner process you can also attach debuggers or breakpoints directly in the callback implementations.</p> <p>Note</p> <p>For a better understanding of where hooks are called, we show a pseudo code of Runner's working flow below:</p> <pre><code>resources = await store.get_latest_resources()\nrollout = ...\ntry:\n    # &lt;-- on_rollout_start\n    with tracer.trace_context(...):\n        # &lt;--- on_trace_start\n        result = await agent.rollout(...)\n        # &lt;--- on_trace_end\n    post_process_result(result)\nexcept Exception:\n    # &lt;-- on_rollout_end\n    await store.update_attempt(status=...)\n</code></pre>"},{"location":"tutorials/debug/#dry-run-the-trainer-loop","title":"Dry-Run the Trainer Loop","text":"<p>Once single rollouts behave, switch to the trainer\u2019s dry-run mode. <code>Trainer.dev</code> spins up a lightweight fast algorithm \u2014 <code>agentlightning.Baseline</code> by default \u2014 so you can exercise the same infrastructure as <code>Trainer.fit</code> without standing up complex stacks like RL or SFT.</p> <p>Warning</p> <p>When you enable multiple runners via <code>n_runners</code>, the trainer may execute them in separate worker processes. Attaching a debugger such as <code>pdb</code> is only practical when <code>n_runners=1</code>, and even then the runner might not live in the main process.</p> <pre><code>import agentlightning as agl\n\ndataset: agl.Dataset[str] = [\n    \"Explain why the sky appears blue using principles of light scattering in 100 words.\",\n    \"What's the capital of France?\",\n]\nresource = agl.PromptTemplate(template=\"You are a helpful assistant. {any_question}\", engine=\"f-string\")\n\ntrainer = agl.Trainer(\n    n_runners=1,\n    initial_resources={\"main_prompt\": resource},\n)\ntrainer.dev(apo_rollout, dataset)\n</code></pre> <p>Just like <code>Runner.run_context</code>, <code>Trainer.dev</code> requires the <code>NamedResources</code> your agent expects. The key difference is that resources are attached to the trainer rather than the runner.</p> <p><code>Trainer.dev</code> uses an almost switchable interface from <code>Trainer.fit</code>. It also needs a dataset to iterate over, similar to <code>fit</code>. Under the hood <code>dev</code> uses the same implementation as <code>fit</code>, which means you can spin up multiple runners, observe scheduler behavior, and validate how algorithms adapt rollouts. The default <code>Baseline</code> logs detailed traces so you can see each rollout as the algorithm perceives it:</p> <pre><code>21:20:30 Initial resources set: {'main_prompt': PromptTemplate(resource_type='prompt_template', template='You are a helpful assistant. {any_question}', engine='f-string')}\n21:20:30 Proceeding epoch 1/1.\n21:20:30 Enqueued rollout ro-302fb202bd85 in train mode with sample: Explain why the sky appears blue using principles of light scattering in 100 words.\n21:20:30 Enqueued rollout ro-e65a3ffaa540 in train mode with sample: What's the capital of France?\n21:20:30 Waiting for 2 harvest tasks to complete...\n21:20:30 [Rollout ro-302fb202bd85] Status is initialized to queuing.\n21:20:30 [Rollout ro-e65a3ffaa540] Status is initialized to queuing.\n21:20:35 [Rollout ro-302fb202bd85] Finished with status succeeded in 3.80 seconds.\n21:20:35 [Rollout ro-302fb202bd85 | Attempt 1] ID: at-f84ad21c. Status: succeeded. Worker: Worker-0\n21:20:35 [Rollout ro-302fb202bd85 | Attempt at-f84ad21c | Span 3a286a856af6bea8] #1 (openai.chat.completion) ... 1.95 seconds. Attribute keys: ['gen_ai.request.type', 'gen_ai.system', ...]\n21:20:35 [Rollout ro-302fb202bd85 | Attempt at-f84ad21c | Span e2f44b775e058dd6] #2 (openai.chat.completion) ... 1.24 seconds. Attribute keys: ['gen_ai.request.type', 'gen_ai.system', ...]\n21:20:35 [Rollout ro-302fb202bd85 | Attempt at-f84ad21c | Span 45ee3c94fa1070ec] #3 (agentlightning.annotation) ... 0.00 seconds. Attribute keys: ['agentlightning.reward.0.value']\n21:20:35 [Rollout ro-302fb202bd85] Adapted data: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': '...', 'agent_name': ''}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=0.95, metadata={'response_id': '...', 'agent_name': ''})]\n21:20:35 Finished 1 rollouts.\n21:20:35 [Rollout ro-e65a3ffaa540] Status changed to preparing.\n21:20:40 [Rollout ro-e65a3ffaa540] Finished with status succeeded in 6.39 seconds.\n21:20:40 [Rollout ro-e65a3ffaa540 | Attempt 1] ID: at-eaefa5d4. Status: succeeded. Worker: Worker-0\n21:20:40 [Rollout ro-e65a3ffaa540 | Attempt at-eaefa5d4 | Span 901dd6acc0f50147] #1 (openai.chat.completion) ... 1.30 seconds. Attribute keys: ['gen_ai.request.type', 'gen_ai.system', ...]\n21:20:40 [Rollout ro-e65a3ffaa540 | Attempt at-eaefa5d4 | Span 52e0aa63e02be611] #2 (openai.chat.completion) ... 1.26 seconds. Attribute keys: ['gen_ai.request.type', 'gen_ai.system', ...]\n21:20:40 [Rollout ro-e65a3ffaa540 | Attempt at-eaefa5d4 | Span 6c452de193fbffd3] #3 (agentlightning.annotation) ... 0.00 seconds. Attribute keys: ['agentlightning.reward.0.value']\n21:20:40 [Rollout ro-e65a3ffaa540] Adapted data: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': '...', 'agent_name': ''}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=1.0, metadata={'response_id': '...', 'agent_name': ''})]\n21:20:40 Finished 2 rollouts.\n</code></pre> <p>The only limitation is that resources remain static and components like <code>LLMProxy</code> are not wired in. For richer dry runs you can subclass <code>FastAlgorithm</code> and override the pieces you care about.</p>"},{"location":"tutorials/debug/#debug-the-algorithm-runner-boundary","title":"Debug the Algorithm-Runner Boundary","text":"<p>Debugging algorithms in Agent-Lightning is often more challenging than debugging agents. Algorithms are typically stateful and depend on several moving parts \u2014 runners, stores, and trainers \u2014 which makes it difficult to isolate and inspect their behavior. Even mocking an agent to cooperate with an algorithm can be costly and error-prone. To simplify this, Agent-Lightning provides a way to run algorithms in isolation so you can attach a debugger and inspect internal state without interference from other components.</p> <p>By default, <code>Trainer.fit</code> runs the algorithm in the main process and thread, but its logs are interleaved with those from the store and runners, making it hard to follow what\u2019s happening inside the algorithm itself. In Write Your First Algorithm, we covered how to stand up a store, algorithm, and runner in isolation for your own implementations. This section extends that approach to cover two common questions:</p> <ol> <li>How can I run built-in or class-based algorithms (inheriting from <code>Algorithm</code>) in isolation?</li> <li>How can I still use <code>Trainer</code> features like <code>n_runners</code>, <code>adapter</code>, or <code>llm_proxy</code> while debugging?</li> </ol> <p>The solution is to keep using a <code>Trainer</code> instance but manage the store yourself, running the algorithm and runner roles separately. This approach mirrors the internal process orchestration of <code>Trainer.fit</code>, but with more visibility and control. Below, we show a step-by-step guide to achieve this with the <code>calc_agent</code> example.</p> <p>1. Launch the store manually. In a separate terminal, start the store:</p> <pre><code>agl store --port 4747\n</code></pre> <p>Add <code>--log-level DEBUG</code> to the command to see the detailed logs.</p> <p>Then, in your training script, create a <code>LightningStoreClient</code> and pass it to the trainer:</p> <pre><code>client = agl.LightningStoreClient(\"http://localhost:4747\")\ntrainer = agl.Trainer(store=client, ...)\n</code></pre> <p>Set the environment variable <code>AGL_MANAGED_STORE=0</code> so the trainer doesn't attempt to manage the store automatically.</p> <p>2. Start the runner and algorithm processes separately. Each process should run the same training script, but with different environment variables specifying the current role. This setup faithfully mirrors how <code>Trainer.fit</code> orchestrates these components behind the scenes.</p> <pre><code># Terminal 2 \u2013 Runner process\nAGL_MANAGED_STORE=0 AGL_CURRENT_ROLE=runner \\\n    python train_calc_agent.py --external-store-address http://localhost:4747 --val-file data/test_mini.parquet\n\n# Terminal 3 \u2013 Algorithm process\nAGL_MANAGED_STORE=0 AGL_CURRENT_ROLE=algorithm \\\n    python train_calc_agent.py --external-store-address http://localhost:4747 --val-file data/test_mini.parquet\n</code></pre> <p>3. Reuse your existing trainer configuration. You can continue using the same datasets, adapters, and proxies as usual. Because the store is now external, you can:</p> <ul> <li>Attach debuggers to either the algorithm or runner process</li> <li>Add fine-grained logging or tracing</li> <li>Simulate partial failures or latency in individual components</li> </ul> <p>This setup provides a faithful reproduction of the algorithm\u2013runner interaction while keeping the store visible for inspection. Once you\u2019ve resolved the issue, simply set <code>AGL_MANAGED_STORE=1</code> (or omit it) to return to the standard managed training workflow.</p>"},{"location":"tutorials/emitter/","title":"Using Emitters","text":"<p>While returning a single float for the final reward is sufficient for many algorithm-agent combinations, some advanced scenarios require richer feedback. For instance, an algorithm might learn more effectively if it receives intermediate rewards throughout a multi-step task, or if the agent needs to emit additional spans for debugging or analysis.</p> <p>Agent-lightning provides an emitter module for recording custom spans inside your agent logic. Just as Tracer automatically instruments common operations (for example, LLM calls), each emitter helper sends a Span that captures Agent-lightning-specific work so downstream algorithms can query it later. See Working with Traces for more details.</p> <p>For multi-step routines such as function calls, tools, or adapters, wrap code with <code>operation</code> \u2014 either as a decorator or a context manager \u2014 to capture inputs, outputs, and metadata on a dedicated <code>operation</code> span. This makes it easier to correlate downstream annotations (like rewards or messages) with the higher-level work that produced them.</p> <p>You can find the emitter functions in <code>agentlightning.emitter</code>.</p>"},{"location":"tutorials/emitter/#emitting-rewards-messages-and-more","title":"Emitting Rewards, Messages, and More","text":"<p>Here are the primary emitter functions:</p> <ul> <li><code>emit_reward(value: float)</code>: Records an intermediate/final reward, which is a convenient wrapper of <code>emit_annotation</code>.</li> <li><code>emit_annotation(attributes: Dict[str, Any])</code>: Records arbitrary metadata as a span.</li> <li><code>emit_message(message: str)</code>: Records a simple log message as a span.</li> <li><code>emit_exception(exception: BaseException)</code>: Records a Python exception, including its type, message, and stack trace.</li> <li><code>emit_object(obj: Any)</code>: Records any JSON-serializable object, perfect for structured data.</li> </ul> <p>Let's first see an example of an agent using these emitters to provide detailed feedback.</p> <pre><code>import agentlightning as agl\n\n@agl.rollout\ndef multi_step_agent(task: dict, prompt_template: PromptTemplate) -&gt; float:\n    try:\n        # Step 1: Initial planning\n        agl.emit_message(\"Starting planning phase.\")\n        plan = generate_plan(task, prompt_template)\n        agl.emit_object({\"plan_steps\": len(plan), \"first_step\": plan[0]})\n\n        # Award a small reward for a valid plan\n        plan_reward = grade_plan(plan)\n        agl.emit_reward(plan_reward)\n\n        # Step 2: Execute the plan\n        agl.emit_message(f\"Executing {len(plan)}-step plan.\")\n        execution_result = execute_plan(plan)\n\n        # Step 3: Final evaluation\n        final_reward = custom_grade_final_result(execution_result, task[\"expected_output\"])\n\n        # The return value is treated as the final reward for the rollout\n        return final_reward\n\n    except ValueError as e:\n        # Record the specific error and return a failure reward\n        agl.emit_exception(e)\n        return 0.0\n</code></pre> <p>Each helper accepts nested <code>attributes</code> (or keyword arguments for <code>operation</code>) and automatically flattens/sanitizes them into dotted OpenTelemetry keys. This means you can pass ordinary dictionaries/lists without pre-processing and still get consistent attribute names such as <code>meta.any_attribute</code> across all emitter operations. Agent-lightning does not restrict the attributes you supply, but it is best to consult OpenTelemetry's semantic conventions for recommended names. Agent-lightning also defines specific semconv for its own use cases. The pattern looks like this:</p> <pre><code>from opentelemetry.semconv.attributes import server_attributes\nfrom agentlightning import emit_object\n\nemit_object({\n    \"name\": \"John Doe\",\n    \"age\": 30,\n    \"email\": \"john.doe@example.com\",\n}, attributes={\n    server_attributes.SERVER_ADDRESS: \"127.0.0.1\",\n    server_attributes.SERVER_PORT: 8080,\n})\n</code></pre> <p>Running the above code sends the following span to the backend if you have a tracer active:</p> <pre><code>Span(\n    name='agentlightning.object',\n    attributes={\n        'agentlightning.object.type': 'dict',\n        'agentlightning.object.json': '{\"name\": \"John Doe\", \"age\": 30, \"email\": \"john.doe@example.com\"}',\n        'server.address': '127.0.0.1',\n        'server.port': 8080\n    }\n)\n</code></pre> <p>Tip</p> <p>If you don't have a tracer active, the above code will raise the following error:</p> <pre><code>RuntimeError: No active tracer found. Cannot emit object span.\n</code></pre> <p>By default, emitter helpers delegate to the active tracer to create and export spans (specifically via <code>Tracer.create_span</code>). If you want to emit spans without an active tracer, set <code>propagate=False</code> to keep the span local \u2014 a useful option for offline tests. The default <code>True</code> streams spans through the active tracer/exporters.</p> <p>When working with agentlightning.semconv, you typically use utilities such as <code>make_tag_attributes</code> and <code>make_link_attributes</code> to build the attributes dictionary. For example:</p> <pre><code>from agentlightning.utils.otel import make_tag_attributes\n\nemit_annotation(make_tag_attributes([\"tool\", \"calculator\", \"fast\", \"good\"]))\n</code></pre> <p>The above code will send a span with the following attributes to the backend:</p> <pre><code>{\n    \"agentlightning.tag.0\": \"tool\",\n    \"agentlightning.tag.1\": \"calculator\",\n    \"agentlightning.tag.2\": \"fast\",\n    \"agentlightning.tag.3\": \"good\"\n}\n</code></pre> <p>A counterpart utility function <code>extract_tags_from_attributes</code> is also available to extract the tags from the attributes dictionary.</p>"},{"location":"tutorials/emitter/#operations","title":"Operations","text":"<p>The <code>operation</code> helper tracks logical units of work within your agent, capturing inputs, outputs, timing, and success/failure status. Unlike point-in-time emitters, operations create a span representing a time interval. Use operations for tool calls, multi-step workflows, debugging, and performance monitoring. <code>operation</code> works as either a decorator or a context manager.</p> <p>The decorator automatically captures function arguments as inputs and the return value as output:</p> <pre><code>import agentlightning as agl\n\n@agl.operation\ndef search_documents(query: str, max_results: int = 10) -&gt; list[dict]:\n    results = perform_search(query, max_results)\n    return results\n\n@agl.operation(category=\"tool\", priority=\"high\")\ndef execute_calculation(expression: str) -&gt; float:\n    return eval_safely(expression)\n</code></pre> <p>The example above emits a span with <code>{\"category\": \"tool\", \"priority\": \"high\"}</code> attributes. It also records the function input and output via OPERATION_INPUT and OPERATION_OUTPUT. It works with async functions too:</p> <pre><code>@agl.operation\nasync def async_api_call(endpoint: str, payload: dict) -&gt; dict:\n    response = await http_client.post(endpoint, json=payload)\n    return response.json()\n</code></pre> <p>Override the operation name if needed:</p> <pre><code>@agl.operation(name=\"custom-name\")\ndef any_weird_name_i_dont_want():\n    pass\n</code></pre> <p>For more control, <code>operation</code> can also be used as a context manager to explicitly record inputs and outputs:</p> <pre><code>with agl.operation(tool_name=\"web_search\") as op:\n    op.set_input(query=\"latest AI research\", filters={\"date\": \"2024\"})\n    results = search_web(\"latest AI research\", {\"date\": \"2024\"})\n    op.set_output({\"result_count\": len(results), \"top_result\": results[0]})\n</code></pre> <p>The <code>propagate=False</code> flag also applies to <code>operation</code> when you want to keep operations local without requiring an active tracer:</p> <pre><code>@agl.operation(propagate=False)\ndef local_test():\n    return \"Not sent to backend\"\n</code></pre>"},{"location":"tutorials/emitter/#linking-to-other-spans","title":"Linking to Other Spans","text":"<p>Sometimes a span should explicitly point back to another span that produced the input it is working on (for example, linking a reward annotation to the <code>agentlightning.operation</code> span that generated a response). Agent-lightning encodes these relationships through flattened link attributes. The helper <code>make_link_attributes</code> converts a dictionary of keys such as <code>trace_id</code>, <code>span_id</code>, or any custom attribute into the <code>\"agentlightning.link.*\"</code> (LightningSpanAttributes.LINK) fields expected by the backend. Later, <code>query_linked_spans</code> can recover the original span(s) from those link descriptors.</p> <pre><code>import opentelemetry.trace as trace_api\nfrom agentlightning import emit_annotation, operation\nfrom agentlightning.utils.otel import make_link_attributes, make_tag_attributes\n\nwith operation(conversation_id=\"chat-42\") as op:\n    # ... perform the work ...\n    link_attrs = make_link_attributes({\n        \"conversation_id\": \"chat-42\",\n    })\n\n    emit_annotation(\n        {\n            **link_attrs,\n            **make_tag_attributes([\"reward\", \"good\"]),\n        }\n    )\n</code></pre> <p>When analyzing in adapters, pass the extracted link models to <code>query_linked_spans</code> to retrieve the matching span(s):</p> <pre><code>from agentlightning.utils.otel import extract_links_from_attributes, query_linked_spans\n\nannotation_span = ...  # Span from your trace store\noperation_spans = [...]  # list of spans you want to search\n\nlink_models = extract_links_from_attributes(annotation_span.attributes)\nmatches = query_linked_spans(operation_spans, link_models)\nassert matches  # Contains the original operation span\n</code></pre> <p>Correlating Rewards with LLM Requests</p> <p>Tracer instruments each request/response as its own span. You can link to the <code>gen_ai.response.id</code> attribute, which comes from the LLM response ID.</p> <pre><code>from agentlightning import emit_reward\nfrom agentlightning.utils.otel import make_link_attributes\n\nresult = call_llm(prompt)\nreward_links = make_link_attributes({\"gen_ai.response.id\": result.id})\nemit_reward(0.9, attributes=reward_links)\n</code></pre> <p>Later, use the same <code>gen_ai.response.id</code> key inside <code>query_linked_spans</code> to find the reward(s) that reference that specific LLM request span.</p>"},{"location":"tutorials/installation/","title":"Installation Guide","text":"<p>This guide explains how to install Agent-Lightning. You can install it from PyPI (the Python Package Index) for general use or directly from the source code if you plan to contribute or need fine-grained control over dependencies.</p> <p>Platform and Hardware Requirements</p> <p>Agent-Lightning is officially supported on Linux distributions (Ubuntu 22.04 or later is recommended). At the moment macOS and Windows (outside of WSL2) are not supported.</p> <p>The Python runtime must be Python 3.10 or newer. We recommend using the latest patch release of Python 3.10, 3.11, or 3.12 to pick up performance and security updates.</p> <p>A GPU is optional\u2014you only need CUDA-capable hardware if you plan to fine-tune model weights or run GPU-accelerated workloads. CPU-only environments are fully supported for evaluation and inference.</p>"},{"location":"tutorials/installation/#installing-from-pypi","title":"Installing from PyPI","text":"<p>The easiest way to get started is by installing Agent-Lightning directly from PyPI. This ensures you get the latest stable release of the package, tested for compatibility and reliability.</p>"},{"location":"tutorials/installation/#install-the-stable-release","title":"Install the Stable Release","text":"<p>Run the following command in your terminal:</p> <pre><code>pip install --upgrade agentlightning\n</code></pre> <p>This installs or upgrades Agent-Lightning to the newest stable version.</p> <p>Tip</p> <p>If you intend to use Agent-Lightning with VERL or run any of its example scripts, you\u2019ll need to install some additional dependencies. See the sections on Algorithm-specific installation and Example-specific installation for details.</p>"},{"location":"tutorials/installation/#install-the-nightly-build-latest-features","title":"Install the Nightly Build (Latest Features)","text":"<p>Agent-Lightning also publishes nightly builds, which contain the latest experimental features and improvements from the main branch. These are available via Test PyPI.</p> <pre><code>pip install --upgrade --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ --pre agentlightning\n</code></pre> <p>Warning</p> <p>The nightly builds are cutting-edge but may include unstable or untested changes. Use them at your own risk, especially in production environments.</p>"},{"location":"tutorials/installation/#algorithm-specific-installation","title":"Algorithm-specific Installation","text":"<p>Agent-Lightning supports multiple learning algorithms. Some of them like APO or VERL require extra dependencies. You can install them automatically using optional extras or manually if you prefer finer control.</p>"},{"location":"tutorials/installation/#installing-apo","title":"Installing APO","text":"<p>APO is an algorithm module that depends on libraries such as POML. You can install Agent-Lightning with APO support by running:</p> <pre><code>pip install agentlightning[apo]\n</code></pre> <p>Warning</p> <p>APO also depends on the OpenAI Python SDK, version 2.0 or newer. Ensure your SDK version is up to date to avoid compatibility issues.</p>"},{"location":"tutorials/installation/#installing-verl","title":"Installing VERL","text":"<p>VERL integrates with libraries like PyTorch, vLLM, and VERL framework. Although you can install all dependencies automatically, we recommend doing it manually to avoid version conflicts.</p> <pre><code>pip install agentlightning[verl]\n</code></pre> <p>Recommended Manual Setup (More Stable)</p> <p>Automated installation may cause issues if you don\u2019t have a compatible PyTorch or CUDA version preinstalled. For a more stable setup, install dependencies step-by-step:</p> <pre><code>pip install torch==2.8.0 torchvision==0.23.0 --index-url https://download.pytorch.org/whl/cu128\npip install flash-attn --no-build-isolation\npip install vllm==0.10.2\npip install verl==0.5.0\n</code></pre> <p>This approach ensures compatibility with CUDA 12.8 and minimizes dependency conflicts.</p>"},{"location":"tutorials/installation/#example-specific-installation","title":"Example-specific Installation","text":"<p>Each example in the <code>examples/</code> directory may have its own additional dependencies. Please refer to the README file of each example for detailed setup instructions:</p> <p>See Example READMEs.</p>"},{"location":"tutorials/installation/#installing-from-source-for-developers-and-contributors","title":"Installing from Source (for Developers and Contributors)","text":"<p>If you plan to contribute to Agent-Lightning or prefer to work with the latest development code, install it directly from the source repository.</p>"},{"location":"tutorials/installation/#why-install-from-source","title":"Why Install from Source?","text":"<ul> <li>You want to modify or contribute to the project.</li> <li>You prefer an isolated development environment.</li> <li>You want to test unreleased features or fix bugs locally.</li> </ul>"},{"location":"tutorials/installation/#using-uv-for-dependency-management","title":"Using <code>uv</code> for Dependency Management","text":"<p>Starting with version 0.2, Agent-Lightning uses <code>uv</code> as its default dependency manager.</p> <p><code>uv</code> is a fast and safe alternative to <code>pip</code> that:</p> <ul> <li>Installs packages in seconds (instead of minutes),</li> <li>Prevents dependency conflicts,</li> <li>Supports grouped dependencies for optional features.</li> </ul> <p>Before proceeding, make sure <code>uv</code> is installed.</p>"},{"location":"tutorials/installation/#minimal-developer-installation","title":"Minimal Developer Installation","text":"<pre><code>git clone https://github.com/microsoft/agent-lightning\ncd agent-lightning\nuv sync --group dev\n</code></pre> <p>This command sets up a clean development environment with only the essential dependencies.</p>"},{"location":"tutorials/installation/#installing-all-extras-cpu-or-gpu","title":"Installing All Extras (CPU or GPU)","text":"<p><code>uv sync</code> can also handle algorithm-specific and example-specific dependencies in one step.</p> <p>For a CPU-only machine:</p> <pre><code>uv sync --frozen \\\n    --extra apo \\\n    --extra verl \\\n    --group dev \\\n    --group torch-cpu \\\n    --group torch-stable \\\n    --group trl \\\n    --group agents \\\n    --no-default-groups\n</code></pre> <p>For a GPU-equipped machine that is CUDA 12.8 compatible:</p> <pre><code>uv sync --frozen \\\n    --extra apo \\\n    --extra verl \\\n    --group dev \\\n    --group torch-gpu-stable \\\n    --group trl \\\n    --group agents \\\n    --no-default-groups\n</code></pre> <p>Read more about Agent-lightning managed dependency groups here.</p>"},{"location":"tutorials/installation/#building-the-dashboard","title":"Building the Dashboard","text":"<p>The Agent-Lightning dashboard is built using Vite. To build the dashboard, run the following command:</p> <pre><code>cd dashboard\nnpm ci\nnpm run build\n</code></pre> <p>Some HTML and JavaScript assets will be generated in the <code>agentlightning/dashboard</code> directory.</p>"},{"location":"tutorials/installation/#activating-your-environment","title":"Activating Your Environment","text":"<p>After syncing dependencies, <code>uv</code> automatically creates a virtual environment inside the <code>.venv/</code> directory.</p> <p>You can use it in two ways:</p> <pre><code># Option 1: Prefix commands with uv run\nuv run python your_script.py\n\n# Option 2: Activate the virtual environment\nsource .venv/bin/activate\npython your_script.py\n</code></pre> <p>Before Contributing</p> <p>Agent-Lightning enforces code style and linting rules via pre-commit hooks. Installing them early prevents many avoidable formatting issues.</p> <pre><code>uv run pre-commit install\nuv run pre-commit run --all-files --show-diff-on-failure --color=always\n</code></pre>"},{"location":"tutorials/parallelize/","title":"Scaling out Agent-lightning","text":"<p>Agent-lightning splits training into an algorithm bundle and a runner bundle that exchange work through the <code>LightningStore</code>. This tutorial shows how to increase rollout throughput, place bundles across processes or machines, and keep the algorithm side scalable with external frameworks.</p>"},{"location":"tutorials/parallelize/#parallelizing-rollouts-with-trainer","title":"Parallelizing Rollouts with <code>Trainer</code>","text":"<p>Before we dive into the details of the bundles and execution strategies, let's first revisit how to parallelize rollouts with <code>Trainer</code>.</p> <p><code>Trainer</code> is the quickest way to dial up parallelism. Even when <code>n_runners = 1</code>, calling <code>Trainer.fit</code> runs the algorithm and runners in parallel. The algorithm enqueues rollouts; runners dequeue them and execute your <code>LitAgent</code>, and the algorithm collects spans via its <code>Adapter</code> before scheduling the next batch.</p> <p>Note</p> <p>One of the most important features of <code>Trainer</code> is the ability to abort things gracefully. For example, if you press <code>Ctrl+C</code> in the terminal, the algorithm will abort and the runners will stop executing. If the algorithm crashes, the runners will also stop executing.</p> <p>Increase throughput by setting <code>n_runners</code> when constructing the trainer. The following example comes from train_calc_agent.py. Since backend LLMs usually use techniques like continuous batching to increase throughput, you do not have to worry about overwhelming the backend with too many requests.</p> <pre><code>import agentlightning as agl\nfrom datasets import Dataset as HFDataset\nfrom calc_agent import calc_agent\n\ntrain_dataset = HFDataset.from_parquet(\"data/train.parquet\").to_list()\nval_dataset = HFDataset.from_parquet(\"data/test.parquet\").to_list()\n\nalgorithm = agl.VERL(verl_config)\n\ntrainer = agl.Trainer(\n    algorithm=algorithm,\n    n_runners=8,  # launch eight rollout workers\n    tracer=agl.OtelTracer(),\n    adapter=agl.LlmProxyTraceToTriplet(),\n)\n\ntrainer.fit(calc_agent, train_dataset=train_dataset, val_dataset=val_dataset)\n</code></pre> <p>In <code>Trainer</code>, there are multiple other initialization parameters that you can use to customize the training process. For example, you can use <code>max_rollouts</code> to keep smoke tests short. Pass a concrete <code>LightningStore</code> instance when you need persistence or want to share the queue across multiple scripts.</p> <p>Tip</p> <p>Before scaling out, run <code>Trainer.dev()</code> with <code>n_runners=1</code> to verify the rollout logic and spans without burning GPU hours.</p>"},{"location":"tutorials/parallelize/#bundles-and-execution-strategies","title":"Bundles and Execution Strategies","text":"<p>When <code>Trainer</code> starts, it packages its configuration into two callable bundles:</p> <p></p> <p>The algorithm bundle wraps your <code>Algorithm</code>, adapter, and any LLM proxy into a single callable that can be aborted via a signal event.</p> <pre><code>async def algorithm_bundle(store: LightningStore, event: ExecutionEvent) -&gt; None:\n    ...\n</code></pre> <p>The runner bundle wraps the <code>Runner</code>, tracer, hooks, and agent into a single callable that can be aborted via a signal event. Unlike the algorithm bundle, the runner bundle is expected to be replicated.</p> <pre><code>async def runner_bundle(store: LightningStore, worker_id: int, event: ExecutionEvent) -&gt; None:\n    ...\n</code></pre> <p>An execution strategy then decides where those bundles are placed (threads vs processes vs multiple machines), how many runner replicas to launch, and how lifecycle events such as shutdown are coordinated.</p> <p>By default, the trainer builds an <code>InMemoryLightningStore</code> if you do not provide one. Because that store has no locking or cross-process transport, the execution strategy is the component that wraps it in thread-safe or HTTP-safe facades (<code>LightningStoreThreaded</code>, <code>LightningStoreServer</code>) before handing it to bundles. For a deeper look at these facades, see Understanding the Store and Birds' Eye View.</p> <p>Agent-lightning provides two built-in execution strategies: <code>SharedMemoryExecutionStrategy</code> and <code>ClientServerExecutionStrategy</code>. You can pass a string alias, a configuration dictionary, or a pre-built strategy instance:</p> <pre><code>import agentlightning as agl\n\nalgorithm = agl.Baseline()\n\n# Short alias for the shared-memory strategy.\n# Because the runner lives on the main thread in this mode,\n# n_runners must be 1 unless you move the algorithm to the main thread.\ntrainer = agl.Trainer(algorithm=algorithm, n_runners=1, strategy=\"shm\")\n\n# Dict with overrides; keep the algorithm on the main thread so multiple runner threads can spawn.\n# Specifying `n_runners` inside strategy is equivalent to passing `n_runners` to the trainer.\ntrainer = agl.Trainer(\n    algorithm=algorithm,\n    strategy={\n        \"type\": \"shm\",\n        \"n_runners\": 8,\n        \"main_thread\": \"algorithm\",\n    },\n)\n\n# Pass an existing strategy instance \u2013 Trainer respects the strategy's own `n_runners`.\nstrategy = agl.SharedMemoryExecutionStrategy(main_thread=\"algorithm\", n_runners=4)\ntrainer = agl.Trainer(algorithm=algorithm, strategy=strategy)\n</code></pre> <p>If you omit the strategy, the trainer defaults to <code>ClientServerExecutionStrategy(n_runners=trainer.n_runners)</code>. You can still re-specify the client-server strategy through aliases or configuration to tweak ports and other settings:</p> <pre><code>trainer = agl.Trainer(\n    algorithm=algorithm,\n    n_runners=8,\n    strategy={\"type\": \"cs\", \"server_port\": 9999},\n)\n</code></pre> <p>Environment variables give you another layer of control. For example:</p> <pre><code>import os\n\nos.environ[\"AGL_SERVER_PORT\"] = \"10000\"\nos.environ[\"AGL_CURRENT_ROLE\"] = \"algorithm\"\nos.environ[\"AGL_MANAGED_STORE\"] = \"0\"\n\ntrainer = agl.Trainer(algorithm=algorithm, n_runners=8, strategy=\"cs\")\n</code></pre> <p>The resulting <code>ClientServerExecutionStrategy</code> picks up the port, role, and managed-store flag from the environment.</p> <p>Tip</p> <p>The same configuration patterns apply to other trainer components. For example, <pre><code>trainer = agl.Trainer(algorithm=algorithm, tracer=agl.OtelTracer())\n</code></pre> wires in a custom tracer, while <pre><code>trainer = agl.Trainer(algorithm=algorithm, adapter=\"agentlightning.adapter.TraceToMessages\")\n</code></pre> swaps in a different adapter. Passing a dict lets you tweak the init parameters of defaults without naming the class explicitly:</p> <pre><code>trainer = agl.Trainer(\n    algorithm=algorithm,\n    adapter={\"agent_match\": \"plan_agent\", \"repair_hierarchy\": False},\n)\n</code></pre> <p>The next sections walk through the two built-in strategies and how they affect placement and store access.</p>"},{"location":"tutorials/parallelize/#client-server-architecture","title":"Client-server Architecture","text":"<p>The default <code>ClientServerExecutionStrategy</code> starts a <code>LightningStoreServer</code> alongside the algorithm and spawns runner processes that talk to it through <code>LightningStoreClient</code>. All runners share the HTTP endpoint, so the queue and spans stay consistent across processes or machines.</p> <p>If you simply instantiate <code>Trainer</code> (as above), it will send the algorithm bundle and runner bundle to <code>ClientServerExecutionStrategy</code>, which will then:</p> <ol> <li>Launch \\(N+1\\) processes: \\(N\\) runner processes and 1 algorithm process (one of them could live in the main process).</li> <li>The algorithm process will take the store received from <code>Trainer</code>, wrap it in a <code>LightningStoreServer</code>, and start serving it over HTTP.</li> <li>The runner processes discard the store and create a new store, which is a client that connects to the algorithm process through <code>LightningStoreClient</code>, and start executing the runner bundle.</li> <li>The strategy automatically escalates shutdown (cooperative stop \u2192 <code>SIGINT</code> \u2192 <code>terminate()</code> \u2192 <code>kill()</code>) so long-running runners do not linger.</li> </ol> <p>You can override server placement or ports, and whether to automatically wrap the store, through constructor arguments or environment variables:</p> <pre><code>trainer = agl.Trainer(\n    algorithm=algorithm,\n    n_runners=1,\n    strategy={\n        \"type\": \"cs\",\n        \"server_host\": \"0.0.0.0\",\n        \"server_port\": 9999,\n        \"main_process\": \"runner\",\n    },\n)\n</code></pre> <p>Set <code>AGL_SERVER_HOST</code> and <code>AGL_SERVER_PORT</code> if you prefer environment-based configuration. You can also use <code>AGL_MANAGED_STORE</code> if you do not want the execution strategy to wrap the store for you. An example is shown in Debugging with External Store.</p> <p>Algorithms sometimes require heterogeneous computation resources, such as GPU accelerators, while runners sometimes require a specific environment to run because many agent frameworks are fragile in their dependencies. A role-based launch pattern helps you place the algorithm on a dedicated machine with more GPU memory, while runners can live on another machine with more flexible dependencies. This is possible via <code>AGL_CURRENT_ROLE=\"algorithm\"</code> or <code>AGL_CURRENT_ROLE=\"runner\"</code> environment variables. When running on different machines, you also need to set <code>AGL_SERVER_HOST</code> and <code>AGL_SERVER_PORT</code> to the IP address and port of the algorithm machine. You might recognize that this convention is very similar to <code>MASTER_ADDR</code> and <code>MASTER_PORT</code> in PyTorch distributed training.</p>"},{"location":"tutorials/parallelize/#launching-algorithm-and-runner-roles-on-separate-machines","title":"Launching Algorithm and Runner Roles on Separate Machines","text":"<p>When you want to stretch the algorithm onto a GPU-rich machine and keep rollout workers close to the data source (or on machines with a more permissive dependency stack), launch the same training script in different terminals with role-specific environment variables. The client\u2013server strategy will route each process to the right side of the queue as long as they share the same <code>AGL_SERVER_HOST</code>/<code>AGL_SERVER_PORT</code> pair.</p> <p>1. Pick an address and port for the store. Decide which machine will host the algorithm. Choose a TCP port that can be reached by the runner machines (for example, open it in your firewall configuration). In this example we will use <code>10.0.0.4:4747</code>.</p> <p>2. Start the algorithm process. On the machine that should run the algorithm, expose the store by binding to all network interfaces and mark the role as <code>algorithm</code>.</p> <pre><code>export AGL_SERVER_HOST=0.0.0.0\nexport AGL_SERVER_PORT=4747\nexport AGL_CURRENT_ROLE=algorithm\n\npython train_calc_agent.py\n</code></pre> <p>Leaving <code>AGL_MANAGED_STORE</code> unset (or setting it to <code>1</code>) lets the strategy create the <code>LightningStoreServer</code> for you. Otherwise, you can use the method in the previous section to create a store on your own.</p> <p>3. Start rollout workers on remote machines. Every runner machine should point to the algorithm host and declare itself as the <code>runner</code> role. You can start multiple processes per machine or repeat the command on additional hosts.</p> <pre><code>export AGL_SERVER_HOST=10.0.0.4\nexport AGL_SERVER_PORT=4747\nexport AGL_CURRENT_ROLE=runner\npython train_calc_agent.py --n-runners 4\n</code></pre> <p>The runner process automatically connects via <code>LightningStoreClient</code>. Adjust <code>--n-runners</code> to spawn the desired number of worker processes on that machine.</p> <p>4. Scale out as needed. Repeat step 3 on as many machines as you need. When you are done, stop the algorithm process. However, since the runners are on different machines, the strategy WILL NOT send a cooperative stop signal to the connected runners. So you need to kill the runners on your own.</p> <p>This role-based launch mirrors what <code>Trainer.fit</code> does inside a single machine while letting you spread work across a fleet. Because every process shares the same training script, you keep a single source of truth for dataset loading, adapters, and tracers, but you can tune compute resources independently for the algorithm and rollout workers.</p>"},{"location":"tutorials/parallelize/#shared-memory-strategy","title":"Shared-memory Strategy","text":"<p><code>SharedMemoryExecutionStrategy</code> keeps everything inside one process. The runner runs on the main thread (by default) while the algorithm lives on a Python thread guarded by <code>LightningStoreThreaded</code>.</p> <p>Use it when you want easier debugging with shared breakpoints and no serialization overhead, or minimal startup time for unit tests. It's not a good choice for many algorithms that require heavy model training because <code>LightningStoreThreaded</code> does not work for multiprocessing. Using it with multiprocessing algorithms will lead to undefined behavior.</p> <p>Sample configuration:</p> <pre><code>trainer = agl.Trainer(\n    algorithm=algorithm,\n    strategy=\"shm\",\n)\n</code></pre> <p>You can further customize the init parameters of <code>SharedMemoryExecutionStrategy</code>. With <code>main_thread=\"runner\"</code>, the runner occupies the main thread and <code>n_runners</code> must be <code>1</code>. The strategy respects <code>AGL_MANAGED_STORE</code>; set it to <code>0</code> to opt out of the <code>LightningStoreThreaded</code> wrapper.</p>"},{"location":"tutorials/parallelize/#parallelizing-algorithms","title":"Parallelizing Algorithms","text":"<p>Runner parallelism scales rollout throughput, but the algorithm loop remains a single-process loop inside the execution strategy. We understand that many algorithms have parallelization built in, but that's outside the parallelization scope of Agent-lightning.</p> <p>Agent-lightning strives to make algorithms\u2019 own parallelization work well under our execution strategies. The biggest challenge turns out to come from the store. For example, <code>VERL</code> uses Ray and launches FSDP and vLLM components internally. <code>ClientServerExecutionStrategy</code> has to make sure that the server is not simultaneously serving in multiple processes or Ray workers, and that there is only one single authoritative source of truth for all subprocesses to connect to. Subprocesses connect to the store via a small <code>LightningStoreClient</code> bundled within <code>LightningStoreServer</code>.</p> <p>Note</p> <p>The birds' eye view illustrates how adapters, proxies, and stores interact when the algorithm spawns additional workers. Use that diagram as a checklist when introducing new distributed components.</p>"},{"location":"tutorials/parallelize/#parallelizing-lightningstore","title":"Parallelizing <code>LightningStore</code>","text":"<p>By default, Agent-lightning persists rollouts and spans in an in-memory store. <code>Trainer.fit</code> spins it up automatically, or you can launch it yourself via the <code>agl store</code> command. <code>InMemoryLightningStore</code> keeps all state inside the current process, which makes local iteration fast but introduces two production constraints:</p> <ol> <li>Spans are evicted once the process crosses its memory cap, so long runs risk data loss unless the host has abundant RAM.</li> <li>Although the store is well optimized via asynchronous programming, the store lives in a single process and remains bound by the GIL, preventing it from saturating multi-core machines.</li> </ol> <p>General note for all server-client stores</p> <p>If your algorithm and runners communicate through HTTP protocol (which should be the default for 99% of the cases), you need to ensure the file limit is sufficiently large to avoid the \"Too many open files\" error. You can set the file limit by running the following command:</p> <pre><code>ulimit -n 100000\n</code></pre> <p>For resilient runs, switch to a persistent backend such as <code>MongoLightningStore</code>, which writes data to MongoDB instead of local RAM. Agent-lightning relies on pymongo to interact with MongoDB, which can be installed via:</p> <pre><code>pip install agentlightning[mongo]\n</code></pre> <p>To use the MongoDB store, you need to pass the MongoDB URI to the store constructor. The URI should be in the format of <code>mongodb://&lt;host&gt;:&lt;port&gt;/&lt;database&gt;?replicaSet=&lt;replicaSet&gt;</code>.</p> <pre><code>from agentlightning.store.mongo import MongoLightningStore\n\ntrainer = agl.Trainer(\n    algorithm=algorithm,\n    store=MongoLightningStore(mongo_uri=\"mongodb://localhost:27017/?replicaSet=rs0\"),\n)\n</code></pre> <p>Setting up MongoDB</p> <p>MongoDB is a popular document-oriented database. Before running Agent-lightning with <code>MongoLightningStore</code>, make sure that you've already had a MongoDB instance running. Setting up can be conveniently done via Docker Compose via compose.mongo.yml. Unless targeting serious production use, we recommend creating the data folders and setting them to <code>777</code> permission to avoid permission issues.</p> <pre><code>mkdir -p data/mongo-host\nchmod 777 data/mongo-host\ndocker compose -f compose.mongo.yml up -d\n</code></pre> <p>Alternatively, you can also install MongoDB manually following the official documentation. If you installed MongoDB manually, an important note is that you need to ensure that the MongoDB instance has enabled replica set feature, since Agent-lightning uses the transactional operations internally. The simplest approach is to use the following script (executed in the MongoDB shell) to initialize the replica set:</p> <pre><code>rs.initiate({\n  _id: \"rs0\",\n  members: [{ _id: 0, host: \"localhost:27017\" }],\n});\n</code></pre> <p>To scale out further, launch the store server via <code>agl store --backend mongo</code> (see Debugging with External Store). The CLI accepts <code>--n-workers</code>, which starts the server under <code>gunicorn</code> with multiple worker processes so concurrent runners can push and pull at higher throughput. This option applies only to persistent backends; an in-memory store, on the other hand, cannot be sharded across workers because its state lives inside one process.</p> <p>Note</p> <p>The <code>--n-workers</code> here is the number of worker processes for the store server, NOT related to the number of rollout runners.</p>"},{"location":"tutorials/parallelize/#increasing-throughput-of-llm-proxy","title":"Increasing Throughput of LLM Proxy","text":"<p>Agent-lightning includes an optional <code>LLMProxy</code> that wraps LiteLLM to provide a unified OpenAI-compatible endpoint for your agents. When rollout throughput increases, the proxy can become a bottleneck. You can scale it out using the same pattern as the store server.</p> <p>To increase proxy throughput, pass <code>num_workers</code> when constructing the proxy:</p> <pre><code>import agentlightning as agl\n\nproxy = agl.LLMProxy(\n    port=4000,\n    launch_mode=\"mp\",  # multiprocessing mode\n    num_workers=4,  # four gunicorn workers handle concurrent requests\n)\n</code></pre> <p>You can also configure the proxy through <code>Trainer</code>:</p> <pre><code>trainer = agl.Trainer(\n    algorithm=algorithm,\n    n_runners=8,  # The runners here is the rollout runners, not related to LLM proxy replicas\n    llm_proxy={\"port\": 4000, \"num_workers\": 4},  # launch mode is actually mp by default\n)\n</code></pre> <p>When <code>num_workers &gt; 1</code>, the launcher starts gunicorn with the specified number of worker processes. Each worker runs its own event loop, allowing the proxy to handle many concurrent LLM requests without being blocked by Python's GIL.</p> <p>Tip</p> <p>When using <code>mp</code> launch mode, <code>LLMProxy</code> will start the server in a separate process. To make sure the proxy is still accessing the same store as the main process, you need to set the store to be zero-copy compatible, which means, either the store is a native zero-copy store like <code>MongoLightningStore</code> or the store is wrapped via <code>LightningStoreServer</code> or <code>LightningStoreClient</code>.</p> <p>Shared Server Infrastructure</p> <p>Both <code>LightningStoreServer</code> and <code>LLMProxy</code> rely on a common utility called <code>PythonServerLauncherArgs</code>. This dataclass captures the settings needed to launch a FastAPI application:</p> <pre><code>from agentlightning.utils import PythonServerLauncherArgs\n\nargs = PythonServerLauncherArgs(\n    port=8000,\n    host=\"0.0.0.0\",\n    n_workers=4,          # spawn 4 gunicorn workers\n    launch_mode=\"thread\", # or \"mp\" for multiprocessing, \"asyncio\" for in-loop\n)\n</code></pre> <p>Under the hood, <code>PythonServerLauncher</code> reads these arguments and chooses between uvicorn (single worker) and gunicorn (multiple workers) automatically.</p>"},{"location":"tutorials/traces/","title":"Working with Traces","text":"<p>Tracing is the secret capability that lets Agent-lightning train almost any agent without rewriting its core logic. The idea was born in observability tooling inside LLMOps workflows and, in Agent-lightning, evolved into a first-class primitive inside the learning loop. Beyond helping you understand what happened inside a rollout, traces provide reward spans and other learning signals that power reinforcement learning and fine-tuning algorithms.</p> <p></p> <p>Agent-lightning stores every recorded operation as a <code>Span</code> inside a <code>LightningStore</code>. The naming comes from OpenTelemetry spans, shown in the screenshot above. A span can represent an LLM call, a tool invocation, a graph edge, an explicit reward emission, or an arbitrary Python code block. Spans form a tree where parent spans describe higher-level steps and children record the detailed work. The sections below walk through how spans are produced and how to interpret them once they reach the store.</p>"},{"location":"tutorials/traces/#writing-spans","title":"Writing Spans","text":"<p>Most <code>Runner</code> implementations wire a <code>Tracer</code> into the agent\u2019s lifecycle. The tracer is responsible for installing instrumentation, buffering OpenTelemetry spans, and committing them to the <code>LightningStore</code>. When a runner executes a rollout, it allocates a store-backed tracing context:</p> <pre><code>async with tracer.trace_context(\n    name=\"my-rollout\",\n    store=store,\n    rollout_id=rollout.rollout_id,\n    attempt_id=attempt.attempt_id,\n):\n    await run_agent_logic()\n</code></pre> <p>The context manager then requests sequence numbers from the store, converts OpenTelemetry spans into <code>Span</code> objects, and persists them in the middle or at the end of the attempt, depending on the tracer implementation. Agent-lightning ships two tracers out of the box; both rely on OpenTelemetry Traces and ignore metrics or logs.</p> <p>What's instrumentation?</p> <p>In simple terms, instrumentation means adding \"patches\" or hooks inside your code so you can observe what it\u2019s doing while it runs. Think of it like putting flight recorders in an airplane \u2014 instrumentation records key actions, inputs, outputs, and timings without changing how the code behaves. In Agent-lightning tracers, this instrumentation automatically creates spans (small, structured records of work) that show what each part of an agent did, how long it took, and how different steps connect together.</p>"},{"location":"tutorials/traces/#agentops-tracer","title":"AgentOps Tracer","text":"<p><code>AgentOpsTracer</code> will be the default tracer when <code>Trainer</code> is used but no tracer is explicitly specified. It bootstraps the AgentOps SDK locally, installs the supplied instrumentation hooks (LangChain, LangGraph, LiteLLM, FastAPI, and others) provided by the AgentOps Python SDK, and forwards everything through a local OpenTelemetry <code>TracerProvider</code>. <code>AgentOpsTracer</code> never calls the hosted AgentOps service; instead, it attaches a <code>LightningSpanProcessor</code> implemented by the Agent-lightning team so that spans are captured and shipped straight into the store.</p> <p>Because it shares the AgentOps instrumentation surface, any framework supported by AgentOps automatically gains tracing in Agent-lightning. We layer additional hooks on top of AgentOps to capture features that the SDK misses today:</p> <ol> <li>Certain providers emit extra metadata \u2014 for example, token IDs returned by vLLM \u2014 that are not recorded by the stock SDK. We augment those spans with the missing payloads.</li> <li>AgentOps constructs parent-child relationships on a best-effort basis, but mixed instrumentation (for example, OpenAI Agent SDK alongside direct OpenAI Chat Completion calls) can leave segments disconnected. Our implementation (actually implemented in the <code>TracerTraceToTriplet</code> adapter) repairs those relationships when the hierarchy can be inferred from rollout context.</li> <li>Some versions of downstream frameworks simply do not emit spans for critical events (LangGraph node entrances are a common example). The tracer installs lightweight shims so those spans appear consistently.</li> </ol> <p>If a vendor integration behaves unexpectedly, users are encouraged to combine the tracer with Hooks to inspect the raw spans or diagnostics, and/or implement a specialized tracer for the framework in question.</p>"},{"location":"tutorials/traces/#opentelemetry-tracer","title":"OpenTelemetry Tracer","text":"<p><code>OtelTracer</code> is a minimal implementation that initializes a vanilla <code>TracerProvider</code> and gives you direct control over span creation through the standard <code>opentelemetry.trace</code> API. Use it when you already have explicit instrumentation in your agent, when the AgentOps SDK does not support your framework, or when you want to emit custom spans from business logic.</p> <p>Note</p> <p>Microsoft Agent Framework is a typical example with built-in OpenTelemetry support. Once you set <code>OBSERVABILITY_SETTINGS.enable_otel = True</code>, the framework will automatically emit OpenTelemetry spans, and <code>OtelTracer</code> will be able to capture them. No extra instrumentation is needed.</p> <p>Inside your agent you can call <code>opentelemetry.trace.get_trace_provider().get_tracer(\"my-agent\")</code> and use that tracer to create spans exactly as you would in any OpenTelemetry application. The Lightning span processor attached by <code>OtelTracer</code> guarantees that every span is sequenced, converted, and written to the store. The same applies for emitted rewards (<code>emit_reward</code>) and other emitter signals, which are just a special case of manually-created spans.</p>"},{"location":"tutorials/traces/#weave-tracer-experimental","title":"Weave Tracer (Experimental)","text":"<p><code>WeaveTracer</code> is an experimental tracer that integrates with the Weave Python SDK. Use it as a substitute for <code>AgentOpsTracer</code> when the AgentOps SDK does not fit your environment.</p> <p>The Weave SDK instruments LLM calls and agent libraries directly. Unlike <code>AgentOpsTracer</code>, Weave does not rely on OpenTelemetry to export spans; it routes everything through a dedicated Weave Trace Server. Agent-lightning implements a custom Weave Trace Server so every call captured by the Weave SDK can be persisted to the <code>LightningStore</code>.</p> <p>Warning</p> <p><code>WeaveTracer</code> remains experimental and has not been tested as thoroughly as <code>AgentOpsTracer</code>. It may conflict with libraries that ship OpenTelemetry instrumentation by default (for example, LiteLLM-based LLM proxies). Use the tracer with caution and report any issues to the Agent-lightning team.</p>"},{"location":"tutorials/traces/#llm-proxy","title":"LLM Proxy","text":"<p>Sometimes the runner can\u2019t observe the agent directly \u2014 because it\u2019s in another language or running remotely. <code>LLMProxy</code> bridges that gap by instrumenting the server side of LLM calls. It wraps LiteLLM and adds middleware that accepts prefixed routes like <code>/rollout/{rid}/attempt/{aid}/v1/chat/completions</code>. Before forwarding, the middleware rewrites the path to <code>/v1/chat/completions</code>, fetches a monotonic <code>sequence_id</code> from the <code>LightningStore</code>, injects <code>x-rollout-id</code>, <code>x-attempt-id</code>, and <code>x-sequence-id</code> into the request headers, and then forwards the request to the backend LLM endpoint.</p> <p>LiteLLM produces OpenTelemetry spans for the request/response. A custom <code>LightningSpanExporter</code> reads the rollout/attempt/sequence identifiers from the recorded request headers and persists each span to the store. Because the <code>sequence_id</code> is allocated at the start of the request, traces stay in strict order even across machines with skewed clocks or asynchronous responses.</p> <pre><code>sequenceDiagram\n    participant Agent\n    participant Proxy as LLM Proxy\n    participant Backend as LLM Backend\n    participant Store as LightningStore\n\n    Agent-&gt;&gt;Proxy: POST /rollout/{rid}/attempt/{aid}/v1/chat/completions\n    Proxy-&gt;&gt;Store: get_next_span_sequence_id(rid, aid)\n    Store--&gt;&gt;Proxy: sequence_id\n    Proxy-&gt;&gt;Backend: Forward /v1/chat/completions&lt;br&gt;(headers: rid, aid, sid)\n    Backend--&gt;&gt;Proxy: Response (tokens, usage, token_ids)\n    Proxy-&gt;&gt;Store: Export OTEL spans (rid, aid, sequence_id)\n    Proxy--&gt;&gt;Agent: OpenAI-compatible response</code></pre> <p><code>LLMProxy</code> actually provides more functionalities than just the middleware for tracing. Read Serving LLM for more details.</p> <p></p> <p>Distributed Tracing</p> <p>Agent-lightning enforces deterministic span ordering by assigning a monotonic <code>sequence_id</code> to every span within an attempt. Before calling <code>LightningStore.add_span</code> or <code>LightningStore.add_otel_span</code>, tracers are expected to call <code>LightningStore.get_next_span_sequence_id</code> to get the next sequence id. This removes clock skew and merges spans produced on different machines or threads. If you implement a custom tracer or exporter, make sure you do this (or respect the one provided in headers by components such as <code>LLMProxy</code>); otherwise, adapters will struggle to properly reconstruct the execution tree.</p>"},{"location":"tutorials/traces/#custom-tracer","title":"Custom Tracer","text":"<p>If none of the built-in tracers fit your environment, the first option to consider is to return the spans directly from your agent implementation. If that's not possible, or you want to support multiple agents in a unified effort, you can implement your own tracer by subclassing <code>Tracer</code>.</p> <p>Custom tracers must implement at least <code>trace_context</code>. The <code>trace_context</code> coroutine should install or activate whatever instrumentation you need, then yield a span processor that ultimately adds spans to the store. You can reuse the <code>LightningSpanProcessor</code> if you produce OpenTelemetry <code>ReadableSpan</code> objects, or call <code>LightningStore.add_span</code> directly if you generate <code>Span</code> instances yourself.</p> <p>Advanced tracers often run auxiliary services (for example, starting a telemetry daemon or attaching to a container runtime) inside <code>init_worker</code> and tear them down in <code>teardown_worker</code>. The <code>ParallelWorkerBase</code> lifecycle that <code>Tracer</code> inherits from ensures those hooks are executed in every runner subprocess.</p>"},{"location":"tutorials/traces/#reading-traces","title":"Reading Traces","text":"<p>Generally, there are two approaches to reading traces. When you only need a quick look, <code>Tracer.get_last_trace</code> returns the raw OpenTelemetry spans captured most recently. For historical analysis, use the <code>LightningStore.query_spans</code> API, which yields normalized <code>Span</code> objects keyed by rollout ID and attempt ID. Combine those queries with <code>LightningStore.query_rollouts</code> to align spans with rollout status, retries, and timing information.</p> <p>Spans arrive asynchronously, originate from different processes, and form hierarchies rather than simple lists. The attributes of each span are tedious and unfriendly to human readers. This combination makes raw traces time-consuming to inspect, especially when you only care about specific signals such as rewards, LLM prompts, responses, or tool outputs. Understanding how the store exposes traces and how adapters reshape them will save hours when debugging or training.</p> <p>Why traces can be difficult to read?</p> <p>The trace tree for a single rollout typically mixes multiple abstraction layers: a planner span may contain several LLM spans, each of which contains tool execution spans that can themselves trigger nested agent invocations. There are also instrumentations at different levels. For example, when a request delegates to another library (e.g., from LangChain to OpenAI), two libraries might emit spans for the same request. At the top level, there could be concurrently running agents that may flush spans slightly out of order. Sorting by <code>sequence_id</code> restores the chronological view, but interpreting the tree requires additional context about parent-child relationships and rollout metadata.</p>"},{"location":"tutorials/traces/#adapter","title":"Adapter","text":"<p>Adapters transform lists of spans into higher-level data structures that training algorithms can consume directly. Agent-lightning provides several adapters out of the box:</p> <ul> <li><code>TracerTraceToTriplet</code> converts spans into <code>(prompt, response, reward)</code> triplets, which power reinforcement-learning algorithms such as VERL and connect trace data to gradient updates.</li> <li><code>TraceToMessages</code> rewrites spans into OpenAI chat message JSON suitable for supervised fine-tuning or evaluation harnesses.</li> <li><code>LlmProxyTraceToTriplet</code> mirrors <code>TracerTraceToTriplet</code> but understands spans emitted by LLMProxy. It is experimental and might be merged with <code>TracerTraceToTriplet</code> in the future.</li> </ul> <p>Adapters are regular Python callable instances, so you can plug them into <code>Trainer</code> via the <code>adapter</code> argument, or call them manually during exploration. When used in <code>Trainer</code>, adapters are bundled into the <code>Algorithm</code> before the algorithm runs, through the <code>Algorithm.set_adapter</code> method.</p> <p>You can also customize an <code>Adapter</code> by extending the implementations above or subclassing the base class. If you need a bespoke format, subclass <code>TraceAdapter</code> (for store spans) or <code>OtelTraceAdapter</code> (for raw OpenTelemetry spans) and implement <code>adapt</code> (these two classes can usually share the same implementation).</p>"},{"location":"tutorials/traces/#reading-rewards","title":"Reading Rewards","text":"<p>Rewards are recorded as dedicated spans named <code>agentlightning.annotation</code>. Emitting a reward through <code>emit_reward</code> or <code>emit_annotation</code> ensures the value is stored in the span\u2019s <code>attributes</code>. To audit rewards, fetch spans from the store and use the helper utilities in <code>agentlightning.emitter</code>:</p> <pre><code>from agentlightning.emitter import find_final_reward\n\nspans = await store.query_spans(rollout_id)\nreward = find_final_reward(spans)\nprint(f\"Final reward: {reward}\")\n</code></pre> <p><code>find_reward_spans</code> returns every reward span so you can visualize intermediate shaping signals, while <code>find_final_reward</code> extracts the last non-null reward per attempt. While these helpers are convenient, they may not help you fully understand the chronological or hierarchical relationships between reward spans and other spans. Using an <code>Adapter</code> \u2014 especially the same one used in the algorithm you\u2019re working with \u2014 remains the recommended way to inspect your generated spans.</p>"},{"location":"tutorials/write-agents/","title":"Writing Agents","text":"<p>This tutorial will focus on the heart of the system: the agent itself, guiding you through the different ways to define an agent's logic in Agent-lightning.</p> <p>The basic requirements for any agent are:</p> <ol> <li>It must accept a single task as input.</li> <li>It must accept a set of tunable resources (like a PromptTemplate or LLM).</li> <li>It must emit trace span data so that algorithms can understand its behavior and learn from it. The simplest way to do this is by returning a final reward.</li> </ol> <p>In practice, please also bear in mind that tasks, resources, and spans have extra requirements, in order to make it trainable within Agent-lightning:</p> <ol> <li>You will need a training dataset containing a set of tasks, of the same type that your agent expects as input.</li> <li>The tunable resources are related to the algorithm. For example, the APO algorithm we've seen tunes a PromptTemplate. Other algorithms might tune model weights or other configurations.</li> <li>The type of spans an algorithm can use varies. Almost all algorithms support a single, final reward span at the end of a rollout. However, not all algorithms support rewards emitted mid-rollout, let alone other kinds of spans like exceptions or log messages.</li> </ol> <p>This tutorial will show you how to write an agent that can handle various tasks and resources and emit all kinds of spans. However, you should understand that agents and algorithms are often co-designed. Supporting new types of resources or spans in an algorithm is often much more complex than just adding them to an agent.</p>"},{"location":"tutorials/write-agents/#rollout-decorator","title":"<code>@rollout</code> Decorator","text":"<p>The simplest way to create an agent is by writing a standard Python function and marking it with the @rollout decorator. This approach is perfect for agents with straightforward logic that doesn't require complex state management.</p> <p>Agent-lightning automatically inspects your function's signature and injects the required resources. For example, if your function has a parameter named <code>prompt_template</code>, Agent-lightning will find the PromptTemplate resource for the current rollout and pass it in.</p> <p>Let's revisit the <code>room_selector</code> agent from the first tutorial:</p> <pre><code>from typing import TypedDict\nfrom agentlightning import PromptTemplate, rollout\n\n# Define a data structure for the task input\nclass RoomSelectionTask(TypedDict):\n    # ... fields for the task ...\n    pass\n\n@rollout\ndef room_selector(task: RoomSelectionTask, prompt_template: PromptTemplate) -&gt; float:\n    # 1. Use the injected prompt_template to format the input for the LLM\n    prompt = prompt_template.format(**task)\n\n    # 2. Execute the agent's logic (e.g., call an LLM, use tools)\n    # ...\n\n    # 3. Grade the final choice to get a reward\n    reward = room_selection_grader(final_message, task[\"expected_choice\"])\n\n    # 4. Return the final reward as a float\n    return reward\n</code></pre> <p>When you train this agent, the dataset is expected to be a list of <code>RoomSelectionTask</code> objects:</p> <pre><code>from agentlightning import Dataset, Trainer\n\ndataset: Dataset[RoomSelectionTask] = [\n    RoomSelectionTask(date=\"2025-10-15\", time=\"10:00\", duration_min=60, attendees=10),\n    RoomSelectionTask(date=\"2025-10-16\", time=\"10:00\", duration_min=60, attendees=10),\n]\n\nTrainer().fit(agent=room_selector, train_dataset=dataset)\n</code></pre> <p>Behind the scenes, the <code>@rollout</code> decorator wraps your function in a <code>FunctionalLitAgent</code> object, which is a subclass of LitAgent introduced below, making it compatible with the Trainer and Runner. It supports parameters like <code>task</code>, <code>prompt_template</code>, <code>llm</code>, and <code>rollout</code>, giving you flexible access to the execution context.</p> <p>Here is another example with more advanced usage with <code>llm</code> and <code>rollout</code> as parameters. The <code>llm</code> parameter gives you an OpenAI-compatible LLM endpoint to interact with, which can be tuned under the hood by algorithms. The <code>rollout</code> parameter gives you the full Rollout object, which contains the rollout ID, rollout mode (training or validation), etc.</p> <pre><code>from openai import OpenAI\nfrom agentlightning import LLM, Rollout\n\nclass FlightBookingTask(TypedDict):\n    request: str\n    expected_booking: dict\n\n@rollout\ndef flight_assistant(task: FlightBookingTask, llm: LLM, rollout: Rollout) -&gt; float:\n    print(f\"Rollout ID: {rollout.rollout_id}\")\n    print(f\"Rollout Mode: {rollout.mode}\")\n\n    # Use the tuned LLM resource to create an OpenAI client\n    client = OpenAI(\n        # This endpoint could be a proxy to a proxy to a proxy ...\n        # It could be different every time `flight_assistant` is called\n        # But it should be OpenAI-API compatible\n        base_url=llm.endpoint,\n\n        # Use a dummy key if not provided\n        # Usually this does not matter because the training LLM is often not guarded by an API key\n        # But you can use `or os.environ[\"OPENAI_API_KEY\"]` to make the function compatible with 3rd-party LLMs\n        api_key=llm.api_key or \"dummy-key\",\n    )\n\n    # Make an API call with the specified model\n    response = client.chat.completions.create(\n        model=llm.model,\n        messages=[{\"role\": \"user\", \"content\": task[\"request\"]}],\n    )\n    # Whether the API supports features like streaming, tool calls, etc. depends on\n    # the endpoint that algorithms are serving to you.\n    final_message = response.choices[0].message.content\n\n    # Grade the result and return a reward\n    reward = grade_flight_booking(final_message, task[\"expected_booking\"])\n    return reward\n</code></pre>"},{"location":"tutorials/write-agents/#return-values-from-agents","title":"Return Values from Agents","text":"<p>The value your agent function returns (i.e., the return value of the function decorated by <code>@rollout</code>) is crucial, as it's the primary way to report the outcome of a rollout. Agent-lightning supports several return types to accommodate different scenarios, from simple rewards to detailed, custom traces.</p> <ul> <li> <p><code>float</code>: This is the simplest and most common return type. The <code>float</code> is treated as the final reward for the entire rollout. Agent-lightning automatically creates a final reward span based on this value.</p> </li> <li> <p><code>None</code>: Returning <code>None</code> tells the runner that trace collection is being handled entirely by the Tracer through auto-instrumentation (e.g., via AgentOps). In this case, the runner will simply retrieve the spans that the tracer has already captured.</p> </li> </ul> <p>Emitting the Final Reward</p> <p>When returning <code>None</code>, you must still ensure a final reward is logged. You can do this by using the <code>emit_reward</code> function (covered in the Use Emitters documentation). Wrapping your reward calculation function with the <code>@reward</code> decorator is NOT the recommended approach any more.</p> <ul> <li><code>list[ReadableSpan]</code>, <code>list[SpanCoreFields]</code>, or <code>list[Span]</code>: For advanced use cases, you can manually construct and return a complete list of all spans for the rollout. This gives you full control over the trace data. You can return either a list of OpenTelemetry <code>ReadableSpan</code> objects or Agent-lightning's native <code>Span</code> objects.</li> </ul> <p>For most users, returning a <code>float</code> for simple agents or returning <code>None</code> and using the emitter for more complex ones are the recommended approaches.</p>"},{"location":"tutorials/write-agents/#class-based-agents","title":"Class-based Agents","text":"<p>For more complex agents that require state, helper methods, or distinct logic for training versus validation, you can create a class that inherits from <code>LitAgent</code>. This object-oriented approach provides more structure and control over the agent's lifecycle.</p> <p>To create a class-based agent, you subclass agentlightning.LitAgent and implement its <code>rollout</code> method.</p> <p></p> <p>Here's how the <code>room_selector</code> could be implemented as a class. The rollout method has a slightly different signature than the function-based agent, mainly in how it handles the resources. Putting it simply, algorithms do not just send a PromptTemplate to the agents, they instead send NamedResources, which is a mapping from resource key to Resource. This design is to allow for more advanced features like multi-resource tuning.</p> <p>With <code>@rollout</code> decorator, the resource with correctly matched type will be automatically injected into the rollout method. However, when you use a class-based agent, you need to manually access the resource from the <code>resources</code> dictionary. Built-in algorithms listed their resource key naming conventions here.</p> <pre><code>import agentlightning as agl\n\nclass RoomSelectorAgent(agl.LitAgent[RoomSelectionTask]):\n    def rollout(self, task: RoomSelectionTask, resources: agl.NamedResources, rollout: agl.Rollout) -&gt; float:\n        # 1. Access the prompt_template from the resources dictionary\n        prompt_template = resources[\"prompt_template\"]\n\n        # 2. Execute the agent's logic\n        prompt = prompt_template.format(**task)\n        # ...\n\n        # 3. Grade the final choice\n        reward = room_selection_grader(final_message, task[\"expected_choice\"])\n\n        # 4. Return the final reward\n        return reward\n\n# To use it with the trainer:\n# agent = RoomSelectorAgent()\n# trainer.fit(agent=agent, ...)\n</code></pre> <p>The <code>LitAgent</code> class provides several methods you can override for more fine-grained control:</p> <ul> <li><code>rollout()</code>: The primary method for the agent's logic. It's called for both training and validation by default.</li> <li><code>training_rollout()</code> / <code>validation_rollout()</code>: Implement these if you need different behavior during training (e.g., with exploration) and validation (e.g., with deterministic choices).</li> <li><code>rollout_async()</code> / <code>training_rollout_async()</code> / <code>validation_rollout_async()</code>: Implement the asynchronous versions of these methods if your agent uses <code>asyncio</code>.</li> </ul> <p>Note</p> <p>Rollout is always executed in an asynchronous context no matter whether the agent is asynchronous or synchronous. If your synchronous agent contains some <code>asyncio.run()</code> calls, it might raise an error that there is already an event loop running. To avoid blocking the event loop, it's recommended to offload the inner async operations to a separate thread. Here is a sample code:</p> <pre><code>import asyncio\nimport queue\nimport threading\n\ndef run_sync_ephemeral(coro) -&gt; Any:\n    \"\"\"\n    Run an async coroutine from sync code.\n    - If no loop in this thread: use asyncio.run() directly.\n    - If already in an event loop: spawn a worker thread that calls asyncio.run()\n    (which creates and closes a brand-new event loop per call).\n    \"\"\"\n    try:\n        asyncio.get_running_loop()\n    except RuntimeError:\n        # No running loop in this thread; safe to use asyncio.run\n        return asyncio.run(coro)\n\n    # Already in a running loop -&gt; execute in a worker thread\n    q = queue.Queue[Any]()\n\n    def worker():\n        try:\n            result = asyncio.run(coro)  # creates &amp; closes its own loop\n            q.put((True, result))\n        except BaseException as e:\n            q.put((False, e))\n\n    t = threading.Thread(target=worker, daemon=True)\n    t.start()\n    ok, payload = q.get()\n    t.join()\n    if ok:\n        return payload\n    raise payload\n</code></pre>"}]}